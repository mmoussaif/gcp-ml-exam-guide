<div align="center">

# ğŸ—ï¸ System Design Essentials

**The complete reference for building scalable, reliable distributed systems**

</div>

---

> ğŸ“– **Related:** For ML and GenAI system design (LLM serving, RAG, agents, MLOps), see **[ML & GenAI System Design Guide](./system-design-genai.md)**

---

## ğŸ“‘ Table of Contents

1. [Core Concepts](#1-core-concepts) â€” ACID, CAP theorem, RSM, concurrency patterns
2. [Cloud Computing & Security](#2-cloud-computing--security) â€” IAM, encryption, VPC, 3-tier architecture
3. [Networking & VPC](#3-networking--vpc) â€” DNS resolution, TCP/IP model, proxies
4. [Key Components](#4-key-components) â€” Load balancers, API gateway, rate limiting
5. [Databases](#5-databases) â€” SQL vs NoSQL, replication, sharding strategies
6. [Caching](#6-caching) â€” Write strategies, Redis vs Memcached, invalidation
7. [Message Queues & Pub/Sub](#7-message-queues--pubsub) â€” Kafka architecture, DLQ patterns
8. [Storage](#8-storage) â€” Block/File/Object storage, CDN optimization
9. [Scalability Patterns](#9-scalability-patterns) â€” Horizontal vs vertical, microservices, DR
10. [Distributed System Patterns](#10-distributed-system-patterns) â€” Consistent hashing, quorum, leader election
11. [Capacity Estimation](#11-capacity-estimation) â€” Latencies, QPS formulas, back-of-envelope math
12. [Design Examples](#12-design-examples) â€” URL shortener, chat, notifications
13. [Quick Reference](#13-quick-reference) â€” Checklist, mindset, failure analysis, trade-offs
14. [Glossary](#14-glossary) â€” 100+ acronyms explained in plain English

---

## 1. Core Concepts

**The foundational principles:** Every distributed system must grapple with these core trade-offs. ACID ensures data integrity. CAP forces you to choose between consistency and availability. Understanding these isn't optionalâ€”they dictate your architecture.

### ğŸ” ACID Properties

**ACID** (Atomicity, Consistency, Isolation, Durability) ensures data integrity in database transactionsâ€”critical for financial systems, inventory, or any scenario where partial updates cause inconsistent states.

| Property | Description | Example |
|----------|-------------|---------|
| **Atomicity** | All-or-nothing transactions | Bank transfer: debit fails â†’ credit rolls back |
| **Consistency** | Data always valid per rules | Balances can't go negative |
| **Isolation** | Concurrent txns don't interfere | Two users booking last seat â†’ one succeeds |
| **Durability** | Committed = permanent | Survives crashes via write-ahead logging |

### âš–ï¸ CAP Theorem

The CAP theorem states distributed systems can only guarantee **two of three** properties: **Consistency**, **Availability**, **Partition Tolerance**. Since network partitions are inevitable, you're choosing between C and A.

**The trade-off**: During a network split, you either accept writes (stay available, sacrifice consistency) or reject writes (stay consistent, sacrifice availability).

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CP (Consistency + Partition)  â”‚  AP (Availability + Partition)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Reject requests during split  â”‚  Accept requests, may be stale   â”‚
â”‚  HBase, MongoDB, Redis         â”‚  Cassandra, DynamoDB, CouchDB    â”‚
â”‚  Use: Financial, inventory     â”‚  Use: Social feeds, analytics    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ“Š Reliability, Scalability, Maintainability

These three qualities define production success:

| Quality | Definition | Key Techniques |
|---------|------------|----------------|
| **Reliability** | Functions correctly despite faults | Redundancy, monitoring, Chaos Monkey, graceful degradation |
| **Scalability** | Handles growth in users/data/load | Vertical (bigger) or horizontal (more machines) scaling |
| **Maintainability** | Easy to operate, understand, evolve | Good docs, simple architecture, modular design |

### ğŸ”„ Concurrency Control

When multiple transactions access shared data simultaneously, we need mechanisms to ensure correctness. Without proper concurrency control, you could end up with lost updates, dirty reads, or phantom reads.

**Two-Phase Commit (2PC)** is a distributed transaction protocol that ensures all participants in a transaction either commit or abort together. It works in two phases:

1. **Prepare Phase**: The coordinator asks all participants "Can you commit?" Each participant prepares the transaction (acquires locks, writes to log) and votes YES or NO.
2. **Commit Phase**: If all vote YES, the coordinator tells everyone to commit. If any vote NO, everyone aborts.

The limitation of 2PC is that it's blockingâ€”if the coordinator fails after sending PREPARE but before sending COMMIT, participants are stuck waiting. This is why many modern systems prefer eventual consistency patterns.

```mermaid
sequenceDiagram
    participant C as Coordinator
    participant P1 as Participant 1
    participant P2 as Participant 2

    Note over C,P2: Phase 1: Prepare
    C->>P1: PREPARE
    C->>P2: PREPARE
    P1-->>C: VOTE YES
    P2-->>C: VOTE YES

    Note over C,P2: Phase 2: Commit
    C->>P1: COMMIT
    C->>P2: COMMIT
    P1-->>C: ACK
    P2-->>C: ACK
```

**SAGA Pattern** is an alternative for long-running distributed transactions. Instead of locking resources across multiple services, a saga breaks the transaction into a sequence of local transactions, each with a compensating action that can undo its effects.

For example, in an e-commerce order:
- T1: Reserve inventory â†’ C1: Release inventory
- T2: Charge payment â†’ C2: Refund payment  
- T3: Ship order â†’ C3: Cancel shipment

If T3 fails, the saga executes C2, then C1, rolling back the entire business transaction without distributed locks.

```mermaid
flowchart LR
    subgraph Success["Happy Path"]
        T1[T1: Reserve<br/>Inventory] --> T2[T2: Charge<br/>Payment]
        T2 --> T3[T3: Ship<br/>Order]
        T3 --> T4[T4: SUCCESS]
    end

    subgraph Rollback["On T3 Failure"]
        T3 -.->|failure| C2[C2: Refund<br/>Payment]
        C2 -.-> C1[C1: Release<br/>Inventory]
    end
```

> **Saga vs 2PC:** Saga doesn't hold locksâ€”each step commits locally. If step N fails, compensating transactions (C1...C(N-1)) undo previous work. Better for long-running distributed transactions.

---

## 2. Cloud Computing & Security

### â˜ï¸ Cloud Computing Overview

On-demand IT resources with pay-as-you-go pricing. Three main categories:

| Category | Services | Examples |
|----------|----------|----------|
| **Compute** | VMs, containers, serverless | EC2, ECS/EKS, Lambda |
| **Storage** | Object, block, file | S3, EBS, EFS |
| **Database** | Relational, NoSQL, cache | RDS, DynamoDB, ElastiCache |

All services communicate through **VPC** (Virtual Private Cloud)â€”your isolated network with custom IP ranges, subnets, and routing.

### ğŸ›¡ï¸ Security Fundamentals (Defense in Depth)

Layer multiple security controlsâ€”if one fails, others protect your assets:

| Pillar | What It Does | Key Components |
|--------|--------------|----------------|
| **IAM** | Controls who can do what | Users, Groups, Roles, Policies, MFA |
| **Encryption** | Protects data at rest & transit | KMS (at rest), TLS/SSL (transit), HSM |
| **Network** | Controls traffic flow | VPC, Subnets, Security Groups, NACLs |

**Principle**: Least privilegeâ€”grant only minimum access needed for each task.

### ğŸ›ï¸ 3-Tier Application Security

A typical web application has three tiers, each with different security requirements. The key insight is that each tier should only communicate with its adjacent layersâ€”the internet talks to the web tier, the web tier talks to the app tier, and the app tier talks to the database. No direct internet access to your database!

**Web Tier** sits in a public subnet and handles incoming HTTP/HTTPS traffic. Its security group allows inbound traffic on ports 80 and 443 from anywhere, but outbound traffic only goes to the application tier. All traffic should use TLS/SSL encryption.

**Application Tier** resides in a private subnet with no direct internet access. It accepts traffic only from the web tier's security group and can initiate connections to the database tier. Administrative access (SSH/RDP) should go through a bastion host or VPN, never directly from the internet.

**Database Tier** is the most protected layer, also in a private subnet. It accepts connections only from the application tier on the database port (e.g., 3306 for MySQL). Enable encryption at rest using KMS and encryption in transit using SSL certificates. Never expose database ports to the internet.

```mermaid
flowchart TB
    subgraph Internet["ğŸŒ Internet"]
        Users[Users]
    end

    subgraph Public["Public Subnet"]
        FW[ğŸ”¥ Firewall/WAF]
        WEB[ğŸ–¥ï¸ Web Tier<br/>EC2 - Ports 80/443<br/>TLS/SSL]
    end

    subgraph Private1["Private Subnet 1"]
        APP[âš™ï¸ App Tier<br/>EC2 - From Web only<br/>SSH via Bastion]
    end

    subgraph Private2["Private Subnet 2"]
        DB[(ğŸ—„ï¸ Database Tier<br/>RDS - From App only<br/>Encrypted at rest)]
    end

    Users --> FW
    FW --> WEB
    WEB --> APP
    APP --> DB
```

| Tier | Subnet | Security Group Rules |
|------|--------|---------------------|
| **Web** | Public | Inbound: 80/443 from Internet |
| **App** | Private | Inbound: From Web tier only, SSH via bastion |
| **DB** | Private | Inbound: DB port from App tier only |

### ğŸ”’ Security Groups vs NACLs

Both Security Groups and Network ACLs (NACLs) filter traffic, but they operate at different levels and have important behavioral differences.

**Security Groups** are your first line of defense, operating at the instance level. They're **stateful**, meaning if you allow inbound traffic, the response is automatically allowed outboundâ€”you don't need separate rules for request and response. Security Groups only have ALLOW rules; anything not explicitly allowed is denied.

**NACLs** operate at the subnet level and serve as a second layer of defense. They're **stateless**, so you must explicitly allow both inbound requests AND outbound responses. NACLs support both ALLOW and DENY rules, processed in numerical orderâ€”the first matching rule wins.

**When to use each**: Use Security Groups for most filtering needs since they're easier to manage. Use NACLs when you need to explicitly deny specific IP addresses or ranges, or when you want subnet-wide rules that apply regardless of instance security groups.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SECURITY GROUPS              vs           NACLs         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Instance Level   â”‚              â”‚   Subnet Level    â”‚      â”‚
â”‚  â”‚  (1st defense)    â”‚              â”‚   (2nd defense)   â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                 â”‚
â”‚  â€¢ STATEFUL                         â€¢ STATELESS                â”‚
â”‚    (return traffic                    (must explicitly         â”‚
â”‚     auto-allowed)                      allow return)           â”‚
â”‚                                                                 â”‚
â”‚  â€¢ Allow rules only                 â€¢ Allow AND Deny rules     â”‚
â”‚                                                                 â”‚
â”‚  â€¢ All rules evaluated              â€¢ Rules processed in       â”‚
â”‚                                       order (numbered)          â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                       VPC                                â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚   â”‚
â”‚  â”‚  â”‚              SUBNET (NACL)                      â”‚     â”‚   â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚     â”‚   â”‚
â”‚  â”‚  â”‚  â”‚   EC2 (SG)   â”‚    â”‚   EC2 (SG)   â”‚          â”‚     â”‚   â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚     â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. Networking & VPC

### ğŸŒ VPC Architecture

A Virtual Private Cloud (VPC) is your isolated network within the cloud. Think of it as your own private data center, but without the physical hardware to manage. When you create a VPC, you define a CIDR block (e.g., 172.31.0.0/16) that determines the IP address range for all resources within it.

**Subnets** divide your VPC into smaller network segments, each residing in a single Availability Zone. Public subnets have routes to an Internet Gateway, allowing resources to communicate with the internet. Private subnets don't have direct internet accessâ€”resources here can only reach the internet through a NAT Gateway (for outbound-only traffic).

**Route Tables** determine where network traffic is directed. Each subnet associates with a route table that contains rules like "send 0.0.0.0/0 (all internet traffic) to the Internet Gateway" or "send traffic to other subnets via the local router."

**Internet Gateway** enables communication between your VPC and the internet. It's horizontally scaled, redundant, and highly available by default.

**NAT Gateway** allows private subnet resources to initiate outbound internet connections (e.g., to download software updates) while preventing unsolicited inbound connections. The internet can only respond to requestsâ€”it cannot initiate connections to your private resources.

```mermaid
flowchart TB
    subgraph Region["AWS Region"]
        subgraph VPC["VPC (172.31.0.0/16)"]
            subgraph AZA["Availability Zone A"]
                subgraph PubA["Public Subnet â€¢ 172.31.1.0/24"]
                    EC2A[EC2]
                    NATA[NAT GW]
                end
                subgraph PrvA["Private Subnet â€¢ 172.31.2.0/24"]
                    AppA[EC2]
                    RDSA[(RDS)]
                end
            end
            subgraph AZB["Availability Zone B"]
                subgraph PubB["Public Subnet â€¢ 172.31.3.0/24"]
                    EC2B[EC2]
                    NATB[NAT GW]
                end
                subgraph PrvB["Private Subnet â€¢ 172.31.4.0/24"]
                    AppB[EC2]
                    RDSB[(RDS)]
                end
            end
            RT[Route Table]
        end
        IGW[Internet Gateway]
    end
    Internet[ğŸŒ Internet]

    PubA & PubB --> RT
    PrvA & PrvB --> RT
    RT --> IGW
    IGW --> Internet
```

> **Key insight:** Public subnets route to Internet Gateway. Private subnets route through NAT Gateway for outbound-only internet access. Database in private subnet = no direct internet exposure.

### ğŸ” DNS Resolution Flow

**DNS** (Domain Name System) translates domain names to IP addresses. When you type a URL into your browser, a chain of queries performs that translation. This process typically takes milliseconds but involves multiple servers across the internet.

**Step 1-2**: Your browser first checks its cache, then asks your configured DNS resolver (often your ISP's server or a public resolver like 8.8.8.8). If the resolver doesn't have the answer cached, it begins a recursive lookup.

**Step 3**: The resolver queries a root server. Root servers don't know specific domains, but they know which servers handle top-level domains like .com, .org, or .io. There are only 13 root server addresses (though many physical servers behind them).

**Step 4**: The resolver queries the TLD server for .com (or whatever the domain's TLD is). The TLD server responds with the authoritative nameservers for the specific domain.

**Step 5-6**: Finally, the resolver queries the authoritative nameserver for example.com, which returns the actual IP address. This answer is cached at various levels with TTL (time-to-live) values.

**Step 7**: Your browser connects to the web server at the returned IP address.

```mermaid
sequenceDiagram
    participant C as Client
    participant R as DNS Resolver
    participant Root as Root Server
    participant TLD as TLD Server (.com)
    participant Auth as Authoritative NS
    participant Web as Web Server

    C->>R: 1. www.example.com?
    R->>Root: 2. Query root
    Root-->>R: 3. Go to .com TLD
    R->>TLD: 4. Query .com TLD
    TLD-->>R: 5. Go to example.com NS
    R->>Auth: 6. Query authoritative
    Auth-->>R: 7. IP: 93.184.216.34
    R-->>C: 8. Return IP
    C->>Web: 9. Connect to 93.184.216.34
```

### ğŸ“¡ TCP/IP Model

The TCP/IP model describes how data travels across networks in layers, with each layer handling specific responsibilities. Understanding this model helps you troubleshoot network issues and design systems that communicate efficiently.

**Physical Layer** deals with the actual hardwareâ€”cables, network interface cards, and the electrical or optical signals that carry data.

**Data Link Layer** handles communication between directly connected nodes. Ethernet is the most common protocol here. Switches operate at this layer, forwarding frames based on MAC addresses.

**Network Layer** routes packets across different networks. IP (Internet Protocol) operates here, and routers make forwarding decisions based on IP addresses. This is where logical addressing happens.

**Transport Layer** ensures reliable (**TCP**, Transmission Control Protocol) or fast (**UDP**, User Datagram Protocol) delivery between applications. TCP breaks data into segments, handles acknowledgments, retransmissions, and flow control. UDP is simpler and faster but doesn't guarantee deliveryâ€”perfect for real-time applications like video calls or gaming where a dropped packet matters less than latency.

**Application Layer** is where HTTP, FTP, SMTP, and other protocols live. This is the interface between network communication and your application code.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     TCP/IP MODEL                                â”‚
â”‚                                                                 â”‚
â”‚  Layer 5  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚           â”‚              APPLICATION                         â”‚  â”‚
â”‚           â”‚         HTTP, FTP, SMTP, DNS                     â”‚  â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                              â”‚                                  â”‚
â”‚  Layer 4  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚           â”‚               TRANSPORT                          â”‚  â”‚
â”‚           â”‚           TCP (reliable)                         â”‚  â”‚
â”‚           â”‚           UDP (fast)                             â”‚  â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                              â”‚                                  â”‚
â”‚  Layer 3  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚           â”‚                NETWORK                           â”‚  â”‚
â”‚           â”‚            IP, Routers                           â”‚  â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                              â”‚                                  â”‚
â”‚  Layer 2  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚           â”‚               DATA LINK                          â”‚  â”‚
â”‚           â”‚          Ethernet, Switches                      â”‚  â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                              â”‚                                  â”‚
â”‚  Layer 1  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚           â”‚               PHYSICAL                           â”‚  â”‚
â”‚           â”‚         Cables, NICs, Hubs                       â”‚  â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  TCP vs UDP:                                                   â”‚
â”‚  â€¢ TCP: Connection-oriented, reliable (file transfer, web)     â”‚
â”‚  â€¢ UDP: Connectionless, fast (streaming, gaming, VoIP)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”€ Proxies

| Type | Acts For | Use Cases | Examples |
|------|----------|-----------|----------|
| **Forward Proxy** | Clients | Mask IP, content filtering, caching, geo-bypass | Corporate proxy |
| **Reverse Proxy** | Servers | Load balancing, SSL termination, caching, WAF | Nginx, HAProxy, ALB |

```
Client â†’ Forward Proxy â†’ Internet â†’ Reverse Proxy â†’ Server
         (client-side)               (server-side)
```

---

## 4. Key Components

### âš–ï¸ Load Balancer with Auto Scaling

Load balancers distribute incoming traffic across multiple servers, preventing any single server from becoming overwhelmed and enabling horizontal scaling. Combined with auto-scaling, your application can automatically adjust capacity based on demand.

**How it works**: Clients connect to the load balancer's address (often via a DNS name like api.example.com). The load balancer monitors the health of backend servers and routes requests only to healthy instances. If a server fails health checks, traffic automatically shifts to remaining healthy servers.

**Auto Scaling Groups** manage the lifecycle of your servers. You define minimum (always running), maximum (cost cap), and desired capacity (normal state). Scaling policies adjust capacity based on metrics like CPU utilization, request count, or custom CloudWatch metrics. For predictable traffic patterns (like a daily sales event), scheduled scaling can pre-warm capacity before the spike hits.

**Key tip for high-traffic events**: Don't rely solely on reactive scaling. Pre-warm your load balancer and use scheduled scaling to have instances ready before the traffic arrives. The startup time for new instances includes launching the VM, running bootstrap scripts, and warming application cachesâ€”often several minutes.

```mermaid
flowchart TB
    Internet[ğŸŒ Internet]
    DNS[Route 53<br/>DNS]
    LB[âš–ï¸ Load Balancer<br/>ALB/NLB]

    subgraph ASG["Auto Scaling Group"]
        EC1[EC2 #1]
        EC2[EC2 #2]
        EC3[EC2 #3]
    end

    subgraph Policies["Scaling Policies"]
        P1["Min: 2 | Max: 10"]
        P2["Scale out: CPU > 70%"]
        P3["Scale in: CPU < 30%"]
    end

    Internet --> DNS
    DNS --> LB
    LB --> EC1 & EC2 & EC3
    ASG -.-> Policies
```

| Policy Type | Trigger | Action |
|-------------|---------|--------|
| **Target Tracking** | CPU > 70% | Add instances |
| **Step Scaling** | CPU > 90% | Add 2 instances |
| **Scheduled** | 9am weekdays | Pre-warm to 5 instances |

### ğŸ”„ Load Balancer Types

The choice between Layer 4 and Layer 7 load balancers depends on your requirements for performance versus intelligence.

**Layer 4 (Network Load Balancer)** operates at the transport layer, making routing decisions based only on IP addresses and TCP/UDP ports. It's extremely fastâ€”millions of requests per second with ultra-low latencyâ€”because it doesn't inspect packet contents. Use NLB when you need raw performance, non-HTTP protocols (gaming, IoT), or need to preserve client IPs.

**Layer 7 (Application Load Balancer)** operates at the HTTP/HTTPS layer, understanding the content of requests. It can route based on URL paths (/api/* to API servers, /images/* to media servers), HTTP headers, cookies, or even query strings. It handles SSL termination, freeing your servers from encryption overhead. Use ALB when you need content-based routing, sticky sessions, or WebSocket support.

**Load balancing algorithms** determine how traffic is distributed:
- **Round Robin**: Requests rotate through servers sequentially. Simple but doesn't account for server capacity differences.
- **Least Connections**: Sends traffic to the server with fewest active connections. Better for varying request durations.
- **IP Hash**: Uses client IP to determine the server, ensuring a client always hits the same server (sticky sessions without cookies).

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   LOAD BALANCER TYPES                           â”‚
â”‚                                                                 â”‚
â”‚   LAYER 4 (Transport)          LAYER 7 (Application)           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚   â”‚ Network Load    â”‚          â”‚ Application     â”‚             â”‚
â”‚   â”‚   Balancer      â”‚          â”‚ Load Balancer   â”‚             â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚            â”‚                            â”‚                       â”‚
â”‚   â€¢ TCP/UDP routing            â€¢ HTTP/HTTPS routing            â”‚
â”‚   â€¢ IP + Port based            â€¢ URL, Headers, Cookies         â”‚
â”‚   â€¢ Very fast (millions RPS)   â€¢ Content-based routing         â”‚
â”‚   â€¢ No inspection              â€¢ TLS termination               â”‚
â”‚   â€¢ Preserves client IP        â€¢ Rate limiting                 â”‚
â”‚                                                                 â”‚
â”‚   Use when:                    Use when:                       â”‚
â”‚   â€¢ Need raw performance       â€¢ Need smart routing            â”‚
â”‚   â€¢ Non-HTTP protocols         â€¢ Path-based routing            â”‚
â”‚   â€¢ Gaming, IoT, streaming     â€¢ Microservices                 â”‚
â”‚                                                                 â”‚
â”‚   Algorithms:                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚ Round Robin â”‚ Least Conn  â”‚ IP Hash (Sticky Sessions)   â”‚  â”‚
â”‚   â”‚     â†“       â”‚     â†“       â”‚            â†“                â”‚  â”‚
â”‚   â”‚  1â†’2â†’3â†’1    â”‚  Route to   â”‚  Same client â†’ Same server  â”‚  â”‚
â”‚   â”‚             â”‚  least busy â”‚                             â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸšª API Gateway Pattern

An API Gateway serves as the single entry point for all client requests to your backend services. Instead of clients knowing about and connecting to multiple services directly, they connect to one gateway that handles routing, security, and cross-cutting concerns.

**Why use an API Gateway?**

Without a gateway, every microservice must implement authentication, rate limiting, logging, and CORS handling independently. This leads to code duplication, inconsistent security, and complex client code that must track multiple service endpoints.

**Key responsibilities:**

- **Authentication/Authorization**: Verify JWT tokens or API keys once at the gateway rather than in every service
- **Rate Limiting**: Protect services from abuse by limiting requests per client
- **Request Routing**: Direct `/users/*` to the User Service, `/orders/*` to the Order Service
- **Protocol Translation**: Accept REST from clients, communicate via gRPC to internal services
- **Request/Response Transformation**: Add headers, modify payloads, aggregate responses from multiple services
- **Logging and Monitoring**: Central place to capture metrics and traces for all API traffic

**Trade-offs**: The gateway can become a bottleneck and single point of failure if not properly scaled. It also adds latency to every request (typically 1-10ms). Design for horizontal scaling and deploy across multiple availability zones.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      API GATEWAY                                â”‚
â”‚                                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚   â”‚ Mobile â”‚  â”‚  Web   â”‚  â”‚  IoT   â”‚                           â”‚
â”‚   â”‚  App   â”‚  â”‚ Client â”‚  â”‚ Device â”‚                           â”‚
â”‚   â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                           â”‚
â”‚       â”‚           â”‚           â”‚                                 â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â”‚
â”‚                   â”‚                                             â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
â”‚           â”‚  API GATEWAY  â”‚                                     â”‚
â”‚           â”‚               â”‚                                     â”‚
â”‚           â”‚ â€¢ Auth/AuthZ  â”‚  â† Verify tokens once              â”‚
â”‚           â”‚ â€¢ Rate Limit  â”‚  â† Protect services                â”‚
â”‚           â”‚ â€¢ SSL Term    â”‚  â† Offload encryption              â”‚
â”‚           â”‚ â€¢ Routing     â”‚  â† Direct to services              â”‚
â”‚           â”‚ â€¢ Logging     â”‚  â† Central observability           â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚                   â”‚                                             â”‚
â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”‚
â”‚       â”‚           â”‚           â”‚                                 â”‚
â”‚       â–¼           â–¼           â–¼                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚   â”‚ User  â”‚   â”‚ Order â”‚   â”‚Productâ”‚                            â”‚
â”‚   â”‚Serviceâ”‚   â”‚Serviceâ”‚   â”‚Serviceâ”‚                            â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸš¦ Rate Limiting Algorithms

Rate limiting protects your services from being overwhelmed by too many requestsâ€”whether from a misbehaving client, a DDoS attack, or simply unexpected viral traffic. Different algorithms offer different trade-offs between burst handling, fairness, and implementation complexity.

**Token Bucket** is the most common algorithm, allowing controlled bursts. Imagine a bucket that holds tokens, with new tokens added at a fixed rate. Each request consumes a token. If the bucket is empty, the request is rejected (or queued). The bucket capacity determines maximum burst size, while the refill rate determines sustained throughput.

*Example*: A bucket with 100 tokens refilled at 10 tokens/second allows bursts of 100 requests but sustains only 10 RPS.

**Leaky Bucket** enforces a constant output rate regardless of input burstiness. Requests enter the bucket, but they "leak" out at a fixed rate. If the bucket overflows, excess requests are discarded. This is ideal when your downstream services need predictable, smooth traffic.

**Fixed Window Counter** divides time into fixed windows (e.g., 1-minute intervals) and counts requests per window. Simple to implement but has a boundary problem: a user could send 100 requests at 0:59 and another 100 at 1:00, effectively getting 200 requests in 2 seconds while respecting the 100/minute limit.

**Sliding Window** solves the boundary problem by looking at a rolling time window. It can be implemented as a log of timestamps (accurate but memory-heavy) or as a weighted average of current and previous windows (efficient approximation).

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   RATE LIMITING ALGORITHMS                      â”‚
â”‚                                                                 â”‚
â”‚  TOKEN BUCKET                    LEAKY BUCKET                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ Tokens added at â”‚            â”‚ Requests enter  â”‚            â”‚
â”‚  â”‚   fixed rate    â”‚            â”‚   at any rate   â”‚            â”‚
â”‚  â”‚       â†“         â”‚            â”‚       â†“         â”‚            â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”     â”‚            â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”     â”‚            â”‚
â”‚  â”‚   â”‚ â— â— â— â”‚     â”‚            â”‚   â”‚ â— â— â— â”‚     â”‚            â”‚
â”‚  â”‚   â”‚ â— â—   â”‚ cap â”‚            â”‚   â”‚ â— â—   â”‚     â”‚            â”‚
â”‚  â”‚   â”‚ â—     â”‚     â”‚            â”‚   â”‚ â—     â”‚     â”‚            â”‚
â”‚  â”‚   â””â”€â”€â”€â”¬â”€â”€â”€â”˜     â”‚            â”‚   â””â”€â”€â”€â”¬â”€â”€â”€â”˜     â”‚            â”‚
â”‚  â”‚       â”‚         â”‚            â”‚       â”‚leak     â”‚            â”‚
â”‚  â”‚       â–¼         â”‚            â”‚       â–¼         â”‚            â”‚
â”‚  â”‚   Requests      â”‚            â”‚   Constant      â”‚            â”‚
â”‚  â”‚   (burst OK)    â”‚            â”‚   output rate   â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚  Allows bursts up to            Smooths traffic to             â”‚
â”‚  bucket capacity                constant rate                  â”‚
â”‚                                                                 â”‚
â”‚  FIXED WINDOW                    SLIDING WINDOW                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚                 â”‚            â”‚                 â”‚            â”‚
â”‚  â”‚  |â–ˆâ–ˆâ–ˆâ–ˆ|â–ˆâ–ˆâ–ˆâ–ˆ|    â”‚            â”‚    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     â”‚            â”‚
â”‚  â”‚  |    |    |    â”‚            â”‚   â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’  â”‚            â”‚
â”‚  â”‚  t0   t1   t2   â”‚            â”‚  Rolling window â”‚            â”‚
â”‚  â”‚                 â”‚            â”‚                 â”‚            â”‚
â”‚  â”‚ Reset at window â”‚            â”‚ Smoother, no    â”‚            â”‚
â”‚  â”‚ boundary        â”‚            â”‚ boundary issue  â”‚            â”‚
â”‚  â”‚ (boundary spike â”‚            â”‚                 â”‚            â”‚
â”‚  â”‚  problem!)      â”‚            â”‚                 â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5. Databases

### ğŸ“Š SQL vs NoSQL

| Aspect | SQL (Relational) | NoSQL (Non-relational) |
|--------|------------------|------------------------|
| **Schema** | Fixed, predefined | Flexible, evolving |
| **Consistency** | ACID (strong) | BASE (eventual) |
| **Queries** | Complex JOINs | Simple key-based |
| **Scaling** | Vertical (traditionally) | Horizontal |
| **Examples** | MySQL, PostgreSQL | Redis, MongoDB, Cassandra |

**Choose SQL when**: Transactions critical, complex queries, relational data, data integrity paramount.
**Choose NoSQL when**: Massive scale, flexible schema, simple access patterns, high write throughput.

**Note**: Lines are blurringâ€”PostgreSQL has JSON support, Spanner offers SQL at scale. Don't choose NoSQL just because "it scales."

### ğŸ—ƒï¸ NoSQL Types

NoSQL isn't a single technologyâ€”it's a family of databases optimized for different data models and access patterns.

**Key-Value Stores** (Redis, DynamoDB) are the simplest: store a value with a key, retrieve it by key. Extremely fast for simple lookups. Redis extends this with data structures (lists, sets, sorted sets) making it great for caching, sessions, leaderboards, and real-time analytics.

**Document Databases** (MongoDB, Firestore) store semi-structured documents, typically JSON. Unlike relational tables, each document can have different fields. Ideal for content management, user profiles, or any domain where entities have varying attributes. They support querying on nested fields, not just keys.

**Graph Databases** (Neo4j, Amazon Neptune) model data as nodes and edges, optimized for traversing relationships. When you need to answer questions like "find friends of friends who also like jazz," graph databases outperform relational JOINs by orders of magnitude. Use for social networks, recommendation engines, fraud detection, and knowledge graphs.

**Columnar/Wide-Column Stores** (Cassandra, HBase, BigTable) organize data by columns rather than rows. When your query only needs 3 columns from a table with 100 columns, columnar storage reads only those 3 columns. This makes them excellent for analytics and aggregations over large datasets. They also excel at time-series data where you frequently query recent data.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      NoSQL TYPES                                â”‚
â”‚                                                                 â”‚
â”‚  KEY-VALUE              DOCUMENT              GRAPH             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚key â†’ valueâ”‚         â”‚{          â”‚         â”‚  (A)â”€â”€(B) â”‚     â”‚
â”‚  â”‚key â†’ valueâ”‚         â”‚  "name":  â”‚         â”‚   â”‚ \  â”‚  â”‚     â”‚
â”‚  â”‚key â†’ valueâ”‚         â”‚  "age":   â”‚         â”‚  (C)â”€â”€(D) â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  "tags":[]â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                        â”‚}          â”‚                            â”‚
â”‚  Redis, DynamoDB       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         Neo4j              â”‚
â”‚                        MongoDB               Amazon Neptune     â”‚
â”‚  Use: Cache,           Firestore                                â”‚
â”‚  sessions,             Use: CMS,             Use: Social nets,  â”‚
â”‚  leaderboards          catalogs,             recommendations,   â”‚
â”‚                        user profiles         fraud detection    â”‚
â”‚                                                                 â”‚
â”‚  COLUMNAR (Wide-Column)                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚ Row Key â”‚ Col1  â”‚ Col2  â”‚ Col3  â”‚ ... â”‚                    â”‚
â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”‚                    â”‚
â”‚  â”‚ user1   â”‚ name  â”‚ email â”‚       â”‚     â”‚  Sparse: rows can  â”‚
â”‚  â”‚ user2   â”‚ name  â”‚       â”‚ phone â”‚     â”‚  have diff columns â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚  Cassandra, HBase, BigTable                                     â”‚
â”‚  Use: Analytics, time-series, IoT data, audit logs             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ“‹ Database Replication

Replication copies data across multiple servers for availability, durability, and read scalability. If one server fails, others continue serving requests. The key decision is how to handle writes.

**Master-Replica (Primary-Secondary)** is the most common pattern. One server (master) handles all writes and replicates changes to multiple replicas that handle reads. This scales read capacity linearlyâ€”add more replicas for more read throughput. The trade-off is replication lag: replicas may serve slightly stale data.

*Replication methods*:
- **Synchronous**: Master waits for replica acknowledgment before confirming write. Guarantees no data loss but adds latency.
- **Asynchronous**: Master confirms immediately, replicates in background. Faster writes but potential data loss if master fails before replication.

**Multi-AZ Deployment** protects against entire data center failures. Your primary database runs in Availability Zone A with a synchronous standby in Zone B. All writes go to primary; the standby continuously replays changes. If the primary fails, AWS automatically promotes the standbyâ€”typically within 60-120 seconds. Your application connects via a DNS endpoint that automatically points to the current primary.

*This is different from read replicas*: Multi-AZ is for high availability (automatic failover), while read replicas are for read scaling (manual promotion if needed).

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 DATABASE REPLICATION                            â”‚
â”‚                                                                 â”‚
â”‚   MASTER-REPLICA (Read Replicas)                                â”‚
â”‚                                                                 â”‚
â”‚              Writes                                             â”‚
â”‚                 â”‚                                               â”‚
â”‚                 â–¼                                               â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚           â”‚ MASTER  â”‚  â† All writes go here                    â”‚
â”‚           â”‚  (RW)   â”‚                                           â”‚
â”‚           â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                                           â”‚
â”‚                â”‚ Replication (sync or async)                    â”‚
â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
â”‚       â”‚        â”‚        â”‚                                       â”‚
â”‚       â–¼        â–¼        â–¼                                       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
â”‚   â”‚Replicaâ”‚â”‚Replicaâ”‚â”‚Replicaâ”‚  â† Handles reads                 â”‚
â”‚   â”‚ (R)   â”‚â”‚ (R)   â”‚â”‚ (R)   â”‚                                  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
â”‚       â–²        â–²        â–²                                       â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
â”‚                â”‚                                                â”‚
â”‚              Reads                                              â”‚
â”‚                                                                 â”‚
â”‚   MULTI-AZ DEPLOYMENT (High Availability)                       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚   â”‚       AZ-A      â”‚     â”‚       AZ-B      â”‚                  â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚     â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚                  â”‚
â”‚   â”‚   â”‚ PRIMARY â”‚   â”‚â”€â”€â”€â”€â–ºâ”‚   â”‚ STANDBY â”‚   â”‚                  â”‚
â”‚   â”‚   â”‚   (RW)  â”‚   â”‚sync â”‚   â”‚  (Hot)  â”‚   â”‚                  â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚     â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚                  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                                 â”‚
â”‚   Auto-failover: If primary fails, standby promoted            â”‚
â”‚   automatically in 60-120 seconds                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### âš¡ Database Sharding

When a single database server can't handle your data volume or write throughput, sharding (horizontal partitioning) distributes data across multiple database servers. Each shard holds a subset of the data.

**How sharding works**: A shard key (like user_id) determines which shard stores each record. When your application needs to read or write data, it calculates the shard from the key and connects to the appropriate database.

**Sharding strategies**:

- **Hash-based**: Apply a hash function to the key, modulo the number of shards. Distributes data evenly but makes range queries difficult (querying users with IDs 1-1000 might hit all shards).
  
- **Range-based**: Assign ranges to shards (A-H on shard 1, I-P on shard 2). Enables efficient range queries but can create hotspots if data isn't uniformly distributed.
  
- **Directory-based**: A lookup service maps keys to shards. Most flexible but adds a potential bottleneck and single point of failure.

**The challenge**: Cross-shard queries are expensive. If you need to JOIN data across shards, you're doing multiple queries and combining results in your application. Design your shard key to keep related data together. For example, in a multi-tenant SaaS app, shard by tenant_id so all queries within a tenant hit one shard.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATABASE SHARDING                            â”‚
â”‚                                                                 â”‚
â”‚   Application makes request for user_id = 12345                 â”‚
â”‚                    â”‚                                            â”‚
â”‚                    â–¼                                            â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚           â”‚  Shard Router   â”‚  â† Determines target shard       â”‚
â”‚           â”‚  hash(12345) %3 â”‚                                   â”‚
â”‚           â”‚     = 0         â”‚                                   â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚                    â”‚                                            â”‚
â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚       â”‚            â”‚            â”‚                               â”‚
â”‚       â–¼            â–¼            â–¼                               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚   â”‚Shard 0 â”‚  â”‚Shard 1 â”‚  â”‚Shard 2 â”‚                           â”‚
â”‚   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚                           â”‚
â”‚   â”‚hash=0  â”‚  â”‚hash=1  â”‚  â”‚hash=2  â”‚                           â”‚
â”‚   â”‚Users:  â”‚  â”‚Users:  â”‚  â”‚Users:  â”‚                           â”‚
â”‚   â”‚3,6,9...â”‚  â”‚1,4,7...â”‚  â”‚2,5,8...â”‚                           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                                                                 â”‚
â”‚   Sharding Strategies:                                          â”‚
â”‚   â€¢ Hash-based: Even distribution, poor range queries          â”‚
â”‚   â€¢ Range-based: Good ranges, risk of hotspots                 â”‚
â”‚   â€¢ Directory-based: Flexible, extra lookup step               â”‚
â”‚                                                                 â”‚
â”‚   Challenge: Cross-shard queries need aggregation in app       â”‚
â”‚   Solution: Choose shard key to keep related data together     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 6. Caching

### ğŸï¸ Cache Architecture

A cache is a high-speed data storage layer that stores a subset of data so future requests can be served faster than querying the primary data store. Caches are typically stored in memory (RAM), which is orders of magnitude faster than disk-based databases.

**Why cache?** Consider an e-commerce product page. Without caching, every page view queries the database for product details, reviews, and recommendations. At scale, this overwhelms the database. With caching, the first request populates the cache; subsequent requests are served from memory in microseconds rather than milliseconds.

**Cache-aside (Lazy Loading)** is the most common pattern:
1. Application checks cache for data
2. **Cache hit**: Return cached data immediately
3. **Cache miss**: Query database, store result in cache, return to client

The application manages both the cache and database, giving full control over what gets cached and when. The downside is that the first request for any data always hits the database, and you must handle cache invalidation when data changes.

**Measuring cache effectiveness**: The cache hit ratio (percentage of requests served from cache) directly impacts performance.

#### Effective Access Time (EAT)

$$
\text{EAT} = (h \times t_c) + ((1-h) \times t_d)
$$

> **Terms:** h = cache hit ratio (0-1), $t_c$ = cache access time, $t_d$ = database access time, $(1-h)$ = miss ratio

**ğŸ“ Example:**
```
95% hit rate, 1ms cache, 100ms database:
EAT = (0.95 Ã— 1) + (0.05 Ã— 100) = 5.95ms
Without cache: 100ms â†’ With cache: ~6ms (17Ã— faster!)
```

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CACHING ARCHITECTURE                         â”‚
â”‚                                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚   â”‚Clientâ”‚â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  Application  â”‚â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ Database â”‚        â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”˜        â”‚    Server     â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜              â–²              â”‚
â”‚                           â”‚                      â”‚              â”‚
â”‚                           â–¼                      â”‚              â”‚
â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚              â”‚
â”‚                   â”‚     CACHE     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                   â”‚  (Redis/MC)   â”‚     Cache Miss              â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                                                                 â”‚
â”‚   CACHE-ASIDE PATTERN (Lazy Loading)                            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚  1. App checks cache for key                             â”‚  â”‚
â”‚   â”‚  2. If HIT  â†’ return cached data (fast path)            â”‚  â”‚
â”‚   â”‚  3. If MISS â†’ query DB â†’ write to cache â†’ return        â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚   Cache Performance Formula:                                    â”‚
â”‚   EAT = (Hit% Ã— HitTime) + (Miss% Ã— MissTime)                  â”‚
â”‚                                                                 â”‚
â”‚   Example: 95% hit rate, 1ms hit, 100ms miss                   â”‚
â”‚   EAT = (0.95 Ã— 1) + (0.05 Ã— 100) = 5.95ms                    â”‚
â”‚   Without cache: 100ms â†’ With cache: ~6ms (17x faster!)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ“ Caching Strategies

Different write strategies offer trade-offs between consistency, performance, and complexity.

**Write-Through** writes data to both cache and database synchronously. The write isn't considered complete until both succeed. This ensures cache and database are always consistent but adds latency to write operations since you're waiting for two writes.

*Use when*: Consistency is critical and write frequency is low (user profiles, configuration).

**Write-Back (Write-Behind)** writes to cache immediately and returns to the client. The cache asynchronously persists to the database later (often batched for efficiency). This provides excellent write performance but risks data lossâ€”if the cache fails before persisting, updates are lost.

*Use when*: Write performance is critical and some data loss is acceptable (gaming leaderboards, analytics counters).

**Write-Around** bypasses the cache on writes, writing directly to the database. The cache is only populated on reads (cache-aside). This prevents the cache from being filled with data that might never be read, but the first read after a write always hits the database.

*Use when*: Data is written frequently but read rarely (logs, audit trails).

**Cache Invalidation** is the hardest problem in caching. When database data changes, the cache must be updated or invalidated. Strategies include:
- **TTL (Time-To-Live)**: Data expires after a set time. Simple but may serve stale data.
- **Active Invalidation**: Application explicitly deletes/updates cache when data changes.
- **Event-Driven**: Database changes trigger cache invalidation via messaging.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   CACHING STRATEGIES                            â”‚
â”‚                                                                 â”‚
â”‚  WRITE-THROUGH               WRITE-BACK (LAZY)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚   Write to DB   â”‚        â”‚  Write to Cache â”‚                â”‚
â”‚  â”‚       AND       â”‚        â”‚      ONLY       â”‚                â”‚
â”‚  â”‚  Cache together â”‚        â”‚  (Async to DB)  â”‚                â”‚
â”‚  â”‚                 â”‚        â”‚                 â”‚                â”‚
â”‚  â”‚  App â†’ Cache    â”‚        â”‚  App â†’ Cache    â”‚                â”‚
â”‚  â”‚   â†“      â†“      â”‚        â”‚         â†“       â”‚                â”‚
â”‚  â”‚  DB â†â”€â”€â”€â”€â”˜      â”‚        â”‚        DB       â”‚                â”‚
â”‚  â”‚  (synchronous)  â”‚        â”‚    (batched)    â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚  âœ“ Always consistent        âœ“ Fast writes                      â”‚
â”‚  âœ— Higher write latency     âœ— Risk of data loss               â”‚
â”‚                                                                 â”‚
â”‚  WRITE-AROUND                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚  â”‚  Write to DB    â”‚                                           â”‚
â”‚  â”‚  BYPASS Cache   â”‚                                           â”‚
â”‚  â”‚                 â”‚                                           â”‚
â”‚  â”‚  App â”€â”€â”€â”€â”€â”€â–º DB â”‚                                           â”‚
â”‚  â”‚       Cache     â”‚  Cache only populated on reads            â”‚
â”‚  â”‚   (not updated) â”‚                                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚  âœ“ No cache pollution from write-heavy, read-rare data        â”‚
â”‚  âœ— First read always misses                                   â”‚
â”‚                                                                 â”‚
â”‚  CACHE INVALIDATION (the hard problem):                        â”‚
â”‚  â€¢ TTL: Simple but may serve stale data                        â”‚
â”‚  â€¢ Active: App deletes cache on DB update                      â”‚
â”‚  â€¢ Events: DB changes â†’ message â†’ cache invalidation           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”´ Redis vs Memcached

| Feature | Redis | Memcached |
|---------|-------|-----------|
| **Data Types** | Strings, Lists, Sets, Hashes, Sorted Sets | Strings only |
| **Persistence** | Yes (RDB + AOF) | No (volatile) |
| **Replication** | Built-in | No |
| **Threading** | Single (100K+ ops/s) | Multi-threaded |
| **Use Cases** | Sessions, queues, leaderboards, pub/sub | Simple caching, large objects |

**Rule of thumb**: Start with Redis unless you specifically need multi-threaded caching of simple strings.

---

## 7. Message Queues & Pub/Sub

### ğŸ“¬ Message Queue Architecture

Message queues decouple components by allowing asynchronous communication. Instead of Service A calling Service B directly (synchronous), Service A puts a message in a queue and continues its work. Service B processes the message when ready.

**Why use queues?**
- **Decoupling**: Services don't need to know about each other
- **Buffering**: Handle traffic spikes by queuing excess requests
- **Reliability**: Messages persist until processed, surviving service restarts
- **Scalability**: Add more consumers to process messages faster

**Queue vs Pub/Sub** are fundamentally different patterns:

In a **Queue**, each message is delivered to exactly one consumer. When multiple consumers listen to the same queue, they compete for messagesâ€”each message goes to only one of them. This is perfect for task distribution (process these 1000 images).

In **Pub/Sub**, each message is delivered to all subscribers of a topic. When a user posts a tweet, multiple services need to know: the notification service, the analytics service, the timeline service. Each gets its own copy of the message.

**Dead Letter Queue (DLQ)** handles messages that repeatedly fail processing. After N retries, the message moves to the DLQ for manual inspection rather than blocking the queue or being lost. Common causes: malformed data, bugs in consumer code, or downstream service failures.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   MESSAGE QUEUE                                 â”‚
â”‚                                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚ Producer â”‚â”€â”€â”€â”€â”€â–ºâ”‚     QUEUE       â”‚â”€â”€â”€â”€â”€â–ºâ”‚ Consumer â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚  â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”  â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                     â”‚  â”‚ 1 â”‚ 2 â”‚ 3 â”‚  â”‚                        â”‚
â”‚                     â”‚  â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜  â”‚                        â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                                                 â”‚
â”‚   Point-to-Point: Each message goes to ONE consumer            â”‚
â”‚   Use: Task distribution, work queues, job processing          â”‚
â”‚                                                                 â”‚
â”‚                   PUB/SUB (Publish-Subscribe)                   â”‚
â”‚                                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚Publisher â”‚â”€â”€â”€â”€â”€â–ºâ”‚     TOPIC       â”‚â”€â”€â”€â”€â”€â–ºâ”‚Subscriberâ”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚    (Broker)     â”‚â”€â”€â”   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                      â”‚
â”‚                                          â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚                                          â””â”€â”€â–ºâ”‚Subscriberâ”‚      â”‚
â”‚                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                 â”‚
â”‚   Broadcast: Each message goes to ALL subscribers              â”‚
â”‚   Use: Event notification, real-time updates, fan-out          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”¶ Kafka Architecture

Apache Kafka is a distributed streaming platform designed for high-throughput, fault-tolerant message handling. Unlike traditional queues, Kafka persists messages to disk and allows consumers to "replay" historical messages.

**Core concepts**:

**Topics** are categories for messages (think: database tables). A topic is split into **partitions** distributed across brokers. Each partition is an ordered, immutable sequence of messages.

**Partitions** enable parallelism. If a topic has 10 partitions, you can have up to 10 consumers reading in parallel. Messages within a partition maintain order, but there's no global ordering across partitions.

**Consumer Groups** enable both pub/sub and queue semantics. Consumers in the same group share partitions (queue behaviorâ€”each message to one consumer). Different consumer groups each receive all messages (pub/sub behavior).

**Offsets** track consumer progress. Each message has an offset (position in partition). Consumers commit offsets to mark progress. If a consumer restarts, it resumes from the last committed offsetâ€”no messages lost.

**Why Kafka over RabbitMQ?**
- Kafka: High throughput, message replay, stream processing. Better for analytics, logs, event sourcing.
- RabbitMQ: Lower latency, complex routing, traditional messaging. Better for task queues, RPC patterns.

```mermaid
flowchart LR
    subgraph Producers
        P1[Producer 1]
        P2[Producer 2]
        P3[Producer 3]
    end

    subgraph Kafka["Kafka Topic"]
        subgraph Part0["Partition 0"]
            M0[0â”‚1â”‚2â”‚3â”‚4]
        end
        subgraph Part1["Partition 1"]
            M1[0â”‚1â”‚2â”‚3]
        end
        subgraph Part2["Partition 2"]
            M2[0â”‚1â”‚2]
        end
    end

    subgraph CG["Consumer Group"]
        C1[Consumer 1]
        C2[Consumer 2]
        C3[Consumer 3]
    end

    P1 & P2 & P3 --> Kafka
    Part0 --> C1
    Part1 --> C2
    Part2 --> C3
```

| Feature | Description |
|---------|-------------|
| **Ordering** | Guaranteed WITHIN partition, not across |
| **Persistence** | Messages stored on disk, replayable |
| **Throughput** | Millions of messages/sec |
| **Consumer Groups** | Share partitions (queue) or broadcast (pub/sub) |

> **Kafka vs RabbitMQ:** Kafka = log-based, replayable, high throughput (events, analytics). RabbitMQ = traditional queue, lower latency (tasks, RPC).

### ğŸ’€ Dead Letter Queue

When message processing fails repeatedly, you need a strategy to prevent one "poison pill" message from blocking your entire queue. Dead Letter Queues (DLQs) solve this by moving problematic messages aside for investigation.

**How it works**: Configure a retry policy (e.g., 3 attempts with exponential backoff). If a message fails all retries, the queue automatically moves it to the DLQ. Your main queue continues processing other messages while engineers investigate the DLQ contents.

**What causes DLQ messages?**
- **Malformed data**: A message with unexpected format that causes parsing errors
- **Business logic failures**: Data that violates constraints (e.g., negative quantities)
- **Downstream failures**: A dependent service is down (though this might warrant infinite retries)
- **Bugs**: Consumer code has a bug handling certain edge cases

**Best practices**:
- Monitor DLQ depthâ€”it should be empty or near-empty
- Set up alerts when messages enter the DLQ
- Include debugging metadata (original timestamp, failure reason, retry count)
- Build tooling to replay DLQ messages after fixing issues

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DEAD LETTER QUEUE (DLQ)                       â”‚
â”‚                                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚   â”‚ Producer â”‚â”€â”€â”€â”€â”€â–ºâ”‚ Main Queue  â”‚â”€â”€â”€â”€â”€â–ºâ”‚ Consumer â”‚          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                            â”‚                   â”‚                â”‚
â”‚                            â”‚    Retry 1:  âœ—   â”‚                â”‚
â”‚                            â”‚    Retry 2:  âœ—   â”‚                â”‚
â”‚                            â”‚    Retry 3:  âœ—   â”‚                â”‚
â”‚                            â”‚                   â”‚                â”‚
â”‚                            â–¼                   â”‚                â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚                â”‚
â”‚                     â”‚  Dead Letter â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                     â”‚    Queue     â”‚  After max retries         â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                            â”‚                                    â”‚
â”‚                            â–¼                                    â”‚
â”‚                    Manual inspection                            â”‚
â”‚                    Fix issue â†’ Replay                           â”‚
â”‚                                                                 â”‚
â”‚   DLQ messages caused by:                                       â”‚
â”‚   â€¢ Malformed data (parsing errors)                            â”‚
â”‚   â€¢ Business rule violations                                   â”‚
â”‚   â€¢ Consumer bugs on edge cases                                â”‚
â”‚   â€¢ Downstream service failures (maybe infinite retry instead) â”‚
â”‚                                                                 â”‚
â”‚   Best practices:                                               â”‚
â”‚   â€¢ Monitor DLQ depth (should be ~0)                           â”‚
â”‚   â€¢ Alert on new DLQ messages                                  â”‚
â”‚   â€¢ Include debugging metadata                                 â”‚
â”‚   â€¢ Build replay tooling                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 8. Storage

### ğŸ’¾ Storage Types Comparison

| Type | How It Works | Characteristics | Use Cases |
|------|--------------|-----------------|-----------|
| **Block (EBS)** | Raw blocks, mount as volume | Single EC2, lowest latency, highest IOPS | Databases, OS boot volumes |
| **File (EFS/FSx)** | NFS/SMB file system | Shared across instances, auto-scales | CMS, shared data, containers |
| **Object (S3)** | HTTP API, flat namespace | Unlimited capacity, 11 9s durability | Media, backups, data lakes |

**Key insight**: Block for performance, File for sharing, Object for scale. S3 is NOT for databases (no POSIX, higher latency).

### âš¡ Storage Performance Comparison

| Metric | Block (EBS) | File (EFS) | Object (S3) |
|--------|-------------|------------|-------------|
| **Latency** | <1ms | 1-10ms | 100-200ms |
| **Max IOPS** | 256,000 | Scales | N/A |
| **Throughput** | 4,000 MB/s | Scales | Unlimited (parallel) |
| **Access** | Single EC2 | Multiple | HTTP API |
| **Scope** | AZ | Regional | Regional |
| **Cost** | $$ | $$$ | $ |

**Choose**: EBS for databases (latency), EFS for shared files (concurrency), S3 for archives (cost/scale).

### ğŸŒ CDN Architecture

Caches content at edge locations worldwide, serving users from the nearest location.

```
Without CDN: User â†’ 150ms â†’ Origin â†’ 150ms â†’ User (300ms round-trip)
With CDN:    User â†’ 5ms â†’ Edge (cache hit!) â†’ User (5ms total)
```

| CDN Type | How It Works | Best For |
|----------|--------------|----------|
| **Pull** | Fetches on first request, then caches | Most use cases (simpler) |
| **Push** | You upload content proactively | Large files, predictable content |

**Benefits beyond caching**: DDoS protection, SSL termination, image optimization, geo-load balancing.
**AWS CloudFront**: Integrates with Shield (DDoS), WAF, Lambda@Edge.

---

## 9. Scalability Patterns

### ğŸ“ˆ Horizontal vs Vertical Scaling

| Approach | How | Pros | Cons |
|----------|-----|------|------|
| **Vertical (Up)** | Bigger servers | Simple, no code changes | Hardware limits, expensive, SPOF |
| **Horizontal (Out)** | More servers | No limit, fault tolerant | Stateless design, distributed complexity |

**Reality**: Use both. Vertical for databases (easier consistency), horizontal for stateless app servers.

### ğŸ§© Microservices Architecture

Small, independent services that communicate over the network. Each owns its data and can scale independently.

| Benefits | Challenges |
|----------|------------|
| Independent scaling | Network complexity (failures, timeouts) |
| Technology diversity | Data consistency (sagas, eventual) |
| Fault isolation | Operational overhead |
| Team autonomy | Debugging (need distributed tracing) |

**When to use**: Large teams, large apps, different scaling needs. NOT for startupsâ€”start with a monolith, extract services when needed.

```mermaid
flowchart TB
    Client[ğŸ“± Clients]

    subgraph Gateway["API Gateway"]
        GW["Auth â€¢ Rate Limit â€¢ Routing"]
    end

    subgraph Services["Microservices"]
        US[ğŸ‘¤ User Service<br/>Team A]
        OS[ğŸ“¦ Order Service<br/>Team B]
        PS[ğŸ·ï¸ Product Service<br/>Team C]
    end

    subgraph Databases["Databases"]
        UDB[(User DB)]
        ODB[(Order DB)]
        PDB[(Product DB)]
    end

    Client --> GW
    GW --> US & OS & PS
    US --> UDB
    OS --> ODB
    PS --> PDB
```

| Benefits | Challenges |
|----------|------------|
| âœ… Independent scaling | âŒ Network latency/failures |
| âœ… Technology flexibility | âŒ Data consistency (sagas) |
| âœ… Fault isolation | âŒ Operational complexity |
| âœ… Team autonomy | âŒ Debugging (need tracing) |

> **Start with a monolith.** Extract services when there's clear benefitâ€”don't start with microservices!

### ğŸ›¡ï¸ Disaster Recovery

Disaster Recovery (DR) plans for the worst: entire region failures, natural disasters, or catastrophic bugs. Your DR strategy depends on two key metrics:

**RTO (Recovery Time Objective)**: Maximum acceptable downtime. How long can your business survive without the system? E-commerce during Black Friday might need minutes; an internal reporting tool might tolerate hours.

**RPO (Recovery Point Objective)**: Maximum acceptable data loss. How much data can you afford to lose? A social media platform might accept losing the last hour of posts; a bank can't lose any transactions.

**DR Strategies** (from least to most expensive):

**Backup & Restore**: Regular backups to another region. Cheapest but slowest recovery (hours). RPO = time since last backup.

**Pilot Light**: Minimal core infrastructure running in DR region (databases replicating). On disaster, spin up remaining infrastructure. Recovery in tens of minutes.

**Warm Standby**: Scaled-down but functional environment in DR region. Traffic can be routed there immediately. Recovery in minutes.

**Hot Standby (Multi-Site)**: Full production capacity in both regions, traffic split between them. Instant failover, zero data loss. Most expensive but best RTO/RPO.

Route 53 (DNS) handles failover by directing traffic to the healthy region based on health checks.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  DISASTER RECOVERY                              â”‚
â”‚                                                                 â”‚
â”‚   RTO: Recovery Time Objective (max downtime you can tolerate) â”‚
â”‚   RPO: Recovery Point Objective (max data loss you can accept) â”‚
â”‚                                                                 â”‚
â”‚   PRIMARY REGION (us-east-1)     SECONDARY REGION (us-west-2)  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                         â”‚    â”‚                         â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”    â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚   â”‚  â”‚ EC2 â”‚    â”‚ EC2 â”‚    â”‚    â”‚  â”‚ EC2 â”‚    â”‚ EC2 â”‚    â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”¬â”€â”€â”˜    â””â”€â”€â”¬â”€â”€â”˜    â”‚    â”‚  â””â”€â”€â”¬â”€â”€â”˜    â””â”€â”€â”¬â”€â”€â”˜    â”‚   â”‚
â”‚   â”‚     â”‚          â”‚        â”‚    â”‚     â”‚          â”‚        â”‚   â”‚
â”‚   â”‚     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜        â”‚    â”‚     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜        â”‚   â”‚
â”‚   â”‚          â”‚              â”‚    â”‚          â”‚              â”‚   â”‚
â”‚   â”‚     â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”         â”‚    â”‚     â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”         â”‚   â”‚
â”‚   â”‚     â”‚   RDS   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â–ºâ”‚   RDS   â”‚         â”‚   â”‚
â”‚   â”‚     â”‚ Primary â”‚  Async  â”‚    â”‚     â”‚ Standby â”‚         â”‚   â”‚
â”‚   â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  Repli- â”‚    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚   â”‚
â”‚   â”‚                  cation â”‚    â”‚                         â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚              â”‚                              â”‚                   â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                             â”‚                                   â”‚
â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚                      â”‚  Route 53   â”‚  DNS health check         â”‚
â”‚                      â”‚ (Failover)  â”‚  auto-switches traffic    â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                                                                 â”‚
â”‚   Strategy      â”‚ RTO      â”‚ RPO      â”‚ Cost                   â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                â”‚
â”‚   Backup/Restoreâ”‚ Hours    â”‚ Hours    â”‚ $                      â”‚
â”‚   Pilot Light   â”‚ 10s mins â”‚ Minutes  â”‚ $$                     â”‚
â”‚   Warm Standby  â”‚ Minutes  â”‚ Minutes  â”‚ $$$                    â”‚
â”‚   Hot Standby   â”‚ Seconds  â”‚ Zero     â”‚ $$$$                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 10. Distributed System Patterns

### ğŸ”„ Consistent Hashing

Standard hashing (hash(key) % N) has a major problem: when you add or remove a node, almost all keys need to move. Consistent hashing solves this by mapping both keys and nodes onto a ring.

**How it works**: Imagine a ring representing the full hash space (0 to 2^32-1). Nodes are placed on the ring at positions determined by hashing their identifiers. Keys are also hashed to positions. Each key belongs to the first node encountered when moving clockwise from the key's position.

**Why it's better**: When a node joins or leaves, only keys between it and its predecessor need to moveâ€”roughly 1/N of the keys instead of nearly all of them. This minimizes disruption during scaling or failures.

**Virtual nodes** improve distribution. Instead of each physical node having one position, give it many "virtual" positions. This ensures more even distribution of keys, especially with heterogeneous node capacities.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  CONSISTENT HASHING                             â”‚
â”‚                                                                 â”‚
â”‚   Standard hashing: hash(key) % N                               â”‚
â”‚   Problem: If N changes, almost ALL keys move!                 â”‚
â”‚                                                                 â”‚
â”‚   Consistent hashing: Both keys and nodes on a ring            â”‚
â”‚                                                                 â”‚
â”‚                        0                                        â”‚
â”‚                        â”‚                                        â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚             /                     \                             â”‚
â”‚            /      Node A           \                            â”‚
â”‚           â”‚         â— (hash: 100)   â”‚                           â”‚
â”‚           â”‚    â˜… key1 (hash: 80)    â”‚  key1 â†’ clockwise â†’ A    â”‚
â”‚    270 â”€â”€â”€â”¤                         â”œâ”€â”€â”€ 90                     â”‚
â”‚           â”‚                         â”‚                           â”‚
â”‚           â”‚    â—                 â—  â”‚                           â”‚
â”‚           â”‚  Node C          Node B â”‚                           â”‚
â”‚            \  (240)           (180) /                           â”‚
â”‚             \                     /                             â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                        â”‚                                        â”‚
â”‚                       180                                       â”‚
â”‚                                                                 â”‚
â”‚   When Node B leaves: Only keys between C and B move to A      â”‚
â”‚   When Node D joins: Only keys in its range move to D          â”‚
â”‚                                                                 â”‚
â”‚   Virtual nodes: Each physical node â†’ multiple ring positions  â”‚
â”‚   Benefits: More even distribution, handle capacity differencesâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ—³ï¸ Quorum

Quorum-based systems balance consistency and availability by requiring a minimum number of nodes to participate in reads and writes. This ensures that reads and writes have at least one node in common.

#### Strong Consistency Condition

$$
W + R > N
$$

> **Terms:** N = total replicas, W = write quorum (nodes acknowledging write), R = read quorum (nodes responding to read)

> **ğŸ’¡ Intuition:** If W + R > N, every read will see at least one node that participated in the most recent writeâ€”guaranteeing strong consistency.

**Common configurations**:
- **W=N, R=1**: Strong consistency on writes, fast reads. But writes fail if any node is down.
- **W=1, R=N**: Fast writes, but reads are slow and must query all nodes.
- **W=majority, R=majority**: Balanced. For N=3, W=2, R=2 allows one node failure while maintaining consistency.

**Eventual consistency** (W=1, R=1): Fastest but no consistency guarantee. The write might go to node A while the read goes to node B, which hasn't replicated yet.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      QUORUM                                     â”‚
â”‚                                                                 â”‚
â”‚   N = Total replicas                                           â”‚
â”‚   W = Write quorum (nodes acknowledging write)                 â”‚
â”‚   R = Read quorum (nodes responding to read)                   â”‚
â”‚                                                                 â”‚
â”‚   RULE: W + R > N  â†’  Guarantees overlap (strong consistency)  â”‚
â”‚                                                                 â”‚
â”‚   Example: N=3, W=2, R=2  â†’  2+2=4 > 3 âœ“                       â”‚
â”‚                                                                 â”‚
â”‚   WRITE (W=2 must ACK)          READ (R=2 must respond)        â”‚
â”‚   â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”              â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”            â”‚
â”‚   â”‚ âœ“ â”‚ â”‚ âœ“ â”‚ â”‚   â”‚              â”‚ âœ“ â”‚ â”‚ âœ“ â”‚ â”‚   â”‚            â”‚
â”‚   â”‚N1 â”‚ â”‚N2 â”‚ â”‚N3 â”‚              â”‚N1 â”‚ â”‚N2 â”‚ â”‚N3 â”‚            â”‚
â”‚   â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜              â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜            â”‚
â”‚     â”‚     â”‚                        â”‚     â”‚                     â”‚
â”‚     â””â”€â”€â”¬â”€â”€â”˜                        â””â”€â”€â”¬â”€â”€â”˜                     â”‚
â”‚        â”‚                              â”‚                        â”‚
â”‚    W=2 âœ“                          R=2 âœ“                        â”‚
â”‚                                                                 â”‚
â”‚   N1 and N2 overlap â†’ Read sees latest write!                  â”‚
â”‚                                                                 â”‚
â”‚   Configurations:                                               â”‚
â”‚   â€¢ W=N, R=1: Strong write, one node down blocks writes       â”‚
â”‚   â€¢ W=1, R=N: Fast writes, must read all nodes                â”‚
â”‚   â€¢ W=majority, R=majority: Balanced (recommended)             â”‚
â”‚   â€¢ W=1, R=1: Eventual consistency (no guarantee)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ‘‘ Leader Election

Many distributed systems need a single "leader" to coordinate activitiesâ€”processing writes, assigning work, or making decisions. Leader election algorithms ensure exactly one leader is chosen, even as nodes fail and recover.

**Why it's hard**: In a distributed system, nodes can't see each other's state directly. Network partitions might make it appear a leader is dead when it's actually fine. Having two leaders (split-brain) can cause data corruption.

**Raft** is a popular consensus algorithm that's easier to understand than Paxos. Nodes are either leaders, followers, or candidates. Leaders send heartbeats; if followers don't hear from the leader, they become candidates and request votes. The candidate with majority votes becomes the new leader.

**Coordination services** like ZooKeeper and etcd implement leader election so you don't have to. They provide strongly consistent key-value storage with features like ephemeral nodes (disappear when client disconnects) and watches (notify when data changes). Use these rather than implementing consensus yourself!

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   LEADER ELECTION                               â”‚
â”‚                                                                 â”‚
â”‚   Why: Coordinate writes, assign work, make decisions          â”‚
â”‚   Challenge: Network partitions can cause "split brain"        â”‚
â”‚                                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚   â”‚Node A â”‚     â”‚Node B â”‚     â”‚Node C â”‚                        â”‚
â”‚   â”‚Followerâ”‚    â”‚Followerâ”‚    â”‚Followerâ”‚                       â”‚
â”‚   â””â”€â”€â”€â”¬â”€â”€â”€â”˜     â””â”€â”€â”€â”¬â”€â”€â”€â”˜     â””â”€â”€â”€â”¬â”€â”€â”€â”˜                        â”‚
â”‚       â”‚             â”‚             â”‚                             â”‚
â”‚       â”‚   Leader timeout!         â”‚                             â”‚
â”‚       â”‚   A becomes candidate     â”‚                             â”‚
â”‚       â”‚             â”‚             â”‚                             â”‚
â”‚       â”‚â”€â”€Request voteâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                             â”‚
â”‚       â”‚â—„â”€Vote yesâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                             â”‚
â”‚       â”‚â”€â”€Request voteâ”€â”€â”€â–ºâ”‚        â”‚                             â”‚
â”‚       â”‚â—„â”€Vote yesâ”€â”€â”€â”€â”€â”€â”€â”€â”‚        â”‚                             â”‚
â”‚       â”‚             â”‚             â”‚                             â”‚
â”‚       â”‚   Majority! A is leader   â”‚                             â”‚
â”‚       â”‚             â”‚             â”‚                             â”‚
â”‚       â–¼             â–¼             â–¼                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚   â”‚ LEADERâ”‚     â”‚Followerâ”‚    â”‚Followerâ”‚                        â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚       â”‚             â”‚             â”‚                             â”‚
â”‚       â”‚â”€â”€Heartbeatâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  Leader sends heartbeats    â”‚
â”‚       â”‚â”€â”€Heartbeatâ”€â”€â”€â–ºâ”‚          â”‚  Followers reset timeout    â”‚
â”‚                                                                 â”‚
â”‚   Don't implement yourself! Use:                               â”‚
â”‚   â€¢ ZooKeeper: Mature, battle-tested                           â”‚
â”‚   â€¢ etcd: Simpler, Kubernetes uses it                          â”‚
â”‚   â€¢ Consul: Also provides service discovery                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 11. Capacity Estimation

### â±ï¸ Key Latencies & QPS

| Operation | Latency | Typical QPS |
|-----------|---------|-------------|
| L1 Cache | 0.5 ns | - |
| L2 Cache | 7 ns | - |
| Main Memory | 100 ns | - |
| SSD Random Read | 150 Î¼s | - |
| Network Round Trip | 500 Î¼s | - |
| MySQL Query | 1-10 ms | ~1,000 |
| Key-Value (Redis) | <1 ms | ~10,000 |
| In-Memory Cache | <0.1 ms | ~100K-1M |

**Why KV is 10x faster than SQL**: O(1) hash lookup vs B-tree traversal, no query parsing, no JOINs.
**Key constant**: 86,400 seconds/day (24 Ã— 60 Ã— 60)â€”use for converting daily volume to QPS.

### ğŸ§® Estimation Formulas

Back-of-the-envelope calculations help you quickly assess whether a design is feasible. The goal isn't precisionâ€”it's understanding the order of magnitude.

#### QPS (Queries Per Second)

$$
\text{QPS}_{\text{avg}} = \frac{\text{DAU} \times R}{86400}
$$

$$
\text{QPS}_{\text{peak}} = \text{QPS}_{\text{avg}} \times M
$$

> **Terms:** DAU = daily active users, R = requests per user per day, 86400 = seconds per day, M = peak multiplier (typically 2-3Ã—)

**ğŸ“ Example:** 1M DAU, 5 requests/user/day
```
Average = (1,000,000 Ã— 5) / 86,400 = ~58 QPS
Peak = 58 Ã— 3 = ~174 QPS
```

---

#### Storage

$$
S_{\text{daily}} = N \times s
$$

$$
S_{\text{total}} = S_{\text{daily}} \times d \times r
$$

> **Terms:** N = records per day, s = record size (bytes), d = retention days, r = replication factor (typically 3Ã—)

**ğŸ“ Example:** 100M URLs/day, 500 bytes, 5 years, 3Ã— replication
```
Daily = 100M Ã— 500B = 50 GB
5 Years = 50GB Ã— 365 Ã— 5 Ã— 3 = ~275 TB
```

---

#### Bandwidth

$$
\text{BW} = \text{QPS} \times \bar{s}
$$

> **Terms:** BW = bandwidth, $\bar{s}$ = average request/response size

**ğŸ“ Example:** 100 QPS Ã— 10KB = 1 MB/s = 8 Mbps

---

#### Servers Needed

$$
N_{\text{servers}} = \frac{\text{QPS}_{\text{peak}}}{C} \times 1.5
$$

> **Terms:** C = capacity per server (QPS), 1.5 = buffer factor for headroom

**ğŸ“ Example:** 3,000 Peak QPS, 500 QPS/server = (3000/500) Ã— 1.5 = **9 servers**

---

**Common gotchas:**
- Don't forget replication factors
- Consider read vs write ratios (often 10:1 or 100:1)
- Peak traffic can be 10x+ average for spiky workloads
- Storage grows over timeâ€”estimate for retention period

---

## 12. Design Examples

### ğŸ”— URL Shortener

A URL shortener converts long URLs into short codes and redirects visitors to the original URL. Seems simple, but at scale it's a great example of key-value storage, encoding, and caching.

**Key design decisions**:

1. **Encoding**: Use **Base62** (alphanumeric encoding: characters a-z, A-Z, 0-9) for human-readable codes. With 7 characters, you get 62^7 = 3.5 trillion unique codesâ€”plenty for most use cases.

2. **ID Generation**: Use a distributed ID generator (like Snowflake) to create unique numeric IDs, then encode to Base62. Alternatively, generate random strings and check for collisions.

3. **Storage**: A key-value store (DynamoDB, Redis) is perfectâ€”you're doing simple lookups by short code. For durability, persist to disk with in-memory caching.

4. **Caching**: Most URL access follows power-law distributionâ€”a small percentage of URLs get most traffic. Cache popular URLs in Redis with **TTL** (time-to-live). Expect 80%+ cache hit rate.

5. **Redirection**: Use 301 (permanent) redirects if SEO matters, 302 (temporary) if you want to track clicks. Include analytics logging asynchronously.

```mermaid
flowchart LR
    Client[ğŸ‘¤ Client]
    LB[âš–ï¸ Load Balancer]
    API[ğŸ–¥ï¸ API Servers]
    Cache[(âš¡ Cache<br/>Redis)]
    DB[(ğŸ—„ï¸ Database<br/>Sharded)]

    Client --> LB --> API
    API --> Cache
    API --> DB
```

**Write Flow:**
1. Receive long URL: `https://example.com/very/long/path`
2. Generate unique ID (Snowflake or random)
3. Encode to Base62: `abc1234`
4. Store mapping: `abc1234 â†’ https://example.com/...`
5. Return: `https://short.ly/abc1234`

**Read Flow:**
1. Receive: `https://short.ly/abc1234`
2. Check cache (80%+ hit rate expected)
3. If miss, query database
4. 301/302 redirect to original URL

> **Encoding:** Base62 (a-z, A-Z, 0-9) â†’ 7 chars = 62â· = **3.5 trillion** unique URLs

### ğŸ’¬ Chat Application

A chat application requires real-time bidirectional communicationâ€”fundamentally different from request-response HTTP. Key challenges include maintaining persistent connections, delivering messages with low latency, and handling presence/typing indicators.

**WebSocket** is the foundation. Unlike HTTP where the client always initiates, WebSocket maintains a persistent connection allowing the server to push messages to clients instantly. Each connected client maintains a WebSocket to a Connection Manager.

**Connection Manager** tracks which users are connected to which servers. When User A sends a message to User B, the system must find B's server and route the message there. This is often implemented with Redis pub/subâ€”each server subscribes to channels for its connected users.

**Message persistence** uses a database optimized for time-series data (Cassandra works well). Messages are partitioned by conversation_id so all messages in a conversation are colocated. Support for message ordering, read receipts, and offline delivery adds complexity.

**Presence service** tracks online/offline status and typing indicators. These are ephemeralâ€”no need to persist, but they must propagate quickly. Often implemented with Redis with TTL-based expiration.

```mermaid
flowchart TB
    subgraph Clients
        UA[ğŸ‘¤ User A]
        UB[ğŸ‘¤ User B]
    end

    subgraph Backend["Backend Services"]
        CM[ğŸ”Œ Connection Manager<br/>WebSocket Sessions]
        MR[ğŸ“¨ Message Router<br/>Redis Pub/Sub]
    end

    subgraph Storage
        MSG[(ğŸ’¬ Messages DB<br/>Cassandra)]
        PRES[ğŸŸ¢ Presence Service<br/>Redis + TTL]
    end

    UA <-->|WebSocket| CM
    UB <-->|WebSocket| CM
    CM --> MR
    MR --> MSG
    MR --> PRES
```

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Connection** | WebSocket | Persistent, bidirectional real-time |
| **Routing** | Redis Pub/Sub | Cross-server message delivery |
| **Messages** | Cassandra | Partition by conversation_id |
| **Presence** | Redis + TTL | Online status, typing indicators |

### ğŸ”” Notification System

A notification system sends messages across multiple channels (push, SMS, email) to millions of users. Key challenges include high write volume, fanout to many recipients, and handling delivery failures gracefully.

**Key design decisions**:

1. **Decouple ingestion from delivery**: Accept notification requests quickly into a queue, process asynchronously. This prevents downstream failures from blocking the sender.

2. **Channel-specific workers**: Email, SMS, and push notifications have different rate limits, failure modes, and retry strategies. Use separate worker pools for each channel.

3. **Rate limiting**: Third-party providers (Twilio, SendGrid) have rate limits. Build rate limiting into your workers, not just at the API gateway.

4. **Delivery status tracking**: Store delivery status per notification per channel. Users need visibility into "was my notification delivered?"

5. **Template management**: Separate notification content from delivery logic. Support variables, localization, and A/B testing.

```mermaid
flowchart TB
    subgraph Ingestion
        TS[ğŸ¯ Trigger Service]
        NS[ğŸ“ Notification Service]
        TPL[ğŸ“‹ Templates +<br/>User Preferences]
    end

    Q[ğŸ“¬ Priority Queue<br/>Kafka/SQS]

    subgraph Workers["Channel Workers"]
        PW[ğŸ“± Push<br/>FCM/APNs]
        EW[ğŸ“§ Email<br/>SendGrid/SES]
        SW[ğŸ’¬ SMS<br/>Twilio]
        DLQ[ğŸ’€ DLQ]
    end

    DB[(ğŸ“Š Delivery Status DB)]

    TS --> NS
    NS --> TPL
    TPL --> Q
    Q --> PW & EW & SW
    Q -.->|failures| DLQ
    PW & EW & SW --> DB
```

| Design Decision | Why |
|-----------------|-----|
| **Async queue** | Decouple sender from delivery, handle spikes |
| **Channel workers** | Different rate limits, retry strategies |
| **User preferences** | Opt-out, channel priority, quiet hours |
| **Deduplication** | Prevent duplicate notifications |
| **Delivery tracking** | sent â†’ delivered â†’ opened |

**Scaling considerations**:
- **Write-heavy**: Most operations are writes (send notification), reads are status checks
- **Fanout pattern**: One event (new follower post) â†’ millions of notifications
- **Batch vs real-time**: Marketing emails can batch, security alerts must be real-time
- **Priority queues**: Separate queues for transactional (password reset) vs marketing

---

## 13. Quick Reference

### ğŸ”¢ Important Numbers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   IMPORTANT NUMBERS                             â”‚
â”‚                                                                 â”‚
â”‚   Time                           Bytes                          â”‚
â”‚   â”€â”€â”€â”€                           â”€â”€â”€â”€â”€                          â”‚
â”‚   1 million seconds â‰ˆ 11.5 days  1 KB  = 1,024 B               â”‚
â”‚   1 billion seconds â‰ˆ 31.7 years 1 MB  = 1,024 KB              â”‚
â”‚   Seconds per day   = 86,400     1 GB  = 1,024 MB              â”‚
â”‚   Seconds per year  â‰ˆ 31.5M      1 TB  = 1,024 GB              â”‚
â”‚                                  1 PB  = 1,024 TB              â”‚
â”‚                                                                 â”‚
â”‚   Network                                                       â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€                                                       â”‚
â”‚   1 Gbps = 125 MB/s                                            â”‚
â”‚   100 Mbps = 12.5 MB/s                                         â”‚
â”‚                                                                 â”‚
â”‚   Scale                                                         â”‚
â”‚   â”€â”€â”€â”€â”€                                                         â”‚
â”‚   Thousand = 10^3   (3 zeros)                                  â”‚
â”‚   Million  = 10^6   (6 zeros)                                  â”‚
â”‚   Billion  = 10^9   (9 zeros)                                  â”‚
â”‚   Trillion = 10^12  (12 zeros)                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### âœ… System Design Interview Checklist

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                SYSTEM DESIGN CHECKLIST                          â”‚
â”‚                                                                 â”‚
â”‚   1. CLARIFY REQUIREMENTS (5-10 min)                           â”‚
â”‚      â–¡ What are the core features? (functional)                â”‚
â”‚      â–¡ How many users? QPS? Data size? (scale)                 â”‚
â”‚      â–¡ What's the read/write ratio?                            â”‚
â”‚      â–¡ Latency requirements? (real-time vs batch)              â”‚
â”‚      â–¡ Availability target? (99.9%? 99.99%?)                   â”‚
â”‚                                                                 â”‚
â”‚   2. HIGH-LEVEL DESIGN (10-15 min)                             â”‚
â”‚      â–¡ Draw the major components                               â”‚
â”‚      â–¡ Show data flow for key operations                       â”‚
â”‚      â–¡ Identify APIs between components                        â”‚
â”‚                                                                 â”‚
â”‚   3. DEEP DIVE (15-20 min)                                     â”‚
â”‚      â–¡ Database schema design                                  â”‚
â”‚      â–¡ SQL vs NoSQL choice with reasoning                      â”‚
â”‚      â–¡ Caching strategy                                        â”‚
â”‚      â–¡ How to scale each component                             â”‚
â”‚                                                                 â”‚
â”‚   4. ADDRESS BOTTLENECKS (5-10 min)                            â”‚
â”‚      â–¡ Single points of failure?                               â”‚
â”‚      â–¡ What happens when X fails?                              â”‚
â”‚      â–¡ Hot spots or bottlenecks?                               â”‚
â”‚                                                                 â”‚
â”‚   5. WRAP UP (2-5 min)                                         â”‚
â”‚      â–¡ Summarize key design decisions                          â”‚
â”‚      â–¡ Acknowledge trade-offs made                             â”‚
â”‚      â–¡ Mention future improvements                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ§  Beyond Pattern Matching: The Interview Mindset

The gap between knowing patterns and actually designing systems that scale comes down to **reasoning about trade-offs in real time**, not memorizing reference architectures.

**The Pattern Trap**: Most candidates can draw a load balancer perfectly, but few can explain when horizontal scaling stops being the answer. Collecting architectural patterns like PokÃ©mon cardsâ€”Instagram's feed, Netflix's CDN, Twitter's fanoutâ€”isn't mastery. Mastery is understanding *why* each pattern exists and *when* it applies.

**Start With Numbers, Not Boxes**: Senior engineers don't start with architecture diagrams. They start with boring, unglamorous numbers:
- How many users?
- What's the read-to-write ratio?
- What's going to break first when this gets real traffic?

The boxes and arrows come later, after the math basically forces your hand.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             NUMBERS FIRST: KEY QUESTIONS                        â”‚
â”‚                                                                 â”‚
â”‚   BEFORE drawing any component, answer:                         â”‚
â”‚                                                                 â”‚
â”‚   â€¢ DAU (Daily Active Users): current vs. 1yr vs. 5yr?         â”‚
â”‚   â€¢ QPS (Queries Per Second): reads? writes? peak?             â”‚
â”‚   â€¢ Data size: per record? total? growth rate?                 â”‚
â”‚   â€¢ Latency: P50? P99? What's acceptable?                      â”‚
â”‚   â€¢ Read/Write ratio: 100:1? 1:1? Write-heavy?                 â”‚
â”‚                                                                 â”‚
â”‚   These numbers DICTATE your architecture choices.              â”‚
â”‚   10K QPS vs 10M QPS = completely different designs.           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Question Every Default Choice**: When you reach for consistent hashing, ask yourself: "Why consistent hashing here? What problem does it solve that a simple modulo wouldn't?" For a URL shortener with deterministic keys, do you actually need ring-based partitioning, or are you just pattern matching?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           CHALLENGE YOUR COMPONENT CHOICES                      â”‚
â”‚                                                                 â”‚
â”‚   For EVERY component you add, answer:                          â”‚
â”‚                                                                 â”‚
â”‚   â€¢ Why this specific component? (not "because tutorials")     â”‚
â”‚   â€¢ What metric proves it's necessary?                         â”‚
â”‚   â€¢ What new failure mode does it introduce?                   â”‚
â”‚   â€¢ Can the system work without it? If yes, don't add it.     â”‚
â”‚                                                                 â”‚
â”‚   Example: Cache                                                â”‚
â”‚   âœ— "Every system needs caching" (pattern matching)            â”‚
â”‚   âœ“ "Cache hit rate ~95% due to power-law access pattern,      â”‚
â”‚      reduces DB load from 50K to 2.5K QPS" (number-driven)     â”‚
â”‚                                                                 â”‚
â”‚   Counter-example: URL shortener with long-tail distribution   â”‚
â”‚   â†’ 40% cache hit rate â†’ caching adds latency + complexity     â”‚
â”‚   â†’ might be better to just scale the database                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Failure Mode Thinking**: The best interviewers don't ask "how would you design this"â€”they ask "what happens when":

- What happens when your primary database goes down mid-transaction?
- What happens when cache invalidation lags by 30 seconds during a viral spike?
- What happens when two datacenters split and both think they're the primary?

~73% of major outages involve state inconsistency during partial failuresâ€”the exact scenarios most candidates never rehearse.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FAILURE MODE EXERCISE                              â”‚
â”‚                                                                 â”‚
â”‚   For ANY component, ask three failure questions:               â”‚
â”‚                                                                 â”‚
â”‚   DATABASE                                                      â”‚
â”‚   â€¢ What if writes succeed but reads lag behind?               â”‚
â”‚   â€¢ What if the primary fails during a write?                  â”‚
â”‚   â€¢ How do you detect silent corruption?                       â”‚
â”‚                                                                 â”‚
â”‚   CACHE                                                         â”‚
â”‚   â€¢ What if eviction is faster than population?                â”‚
â”‚   â€¢ What if cache and DB disagree? Which wins?                 â”‚
â”‚   â€¢ What's your thundering herd strategy?                      â”‚
â”‚                                                                 â”‚
â”‚   LOAD BALANCER                                                 â”‚
â”‚   â€¢ What if health checks pass but service is deadlocked?      â”‚
â”‚   â€¢ What if one backend is slow but not failing?               â”‚
â”‚   â€¢ How do you handle sticky sessions during failover?         â”‚
â”‚                                                                 â”‚
â”‚   If you can't articulate what breaks and how you'd detect     â”‚
â”‚   it, you're probably not ready.                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Start Simple, Evolve With Constraints**: The best design isn't the most sophisticatedâ€”it's the simplest thing that could work, with complexity added only when measured constraints force it.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              EVOLUTION-DRIVEN DESIGN                            â”‚
â”‚                                                                 â”‚
â”‚   START WITH:                                                   â”‚
â”‚   â€¢ One database, one server, no cache                         â”‚
â”‚   â€¢ Vertical scaling first (it's simpler)                      â”‚
â”‚   â€¢ Monolith (until team/scale forces microservices)           â”‚
â”‚                                                                 â”‚
â”‚   ADD COMPLEXITY ONLY WHEN:                                     â”‚
â”‚   â€¢ A specific metric crosses a threshold you can NAME          â”‚
â”‚   â€¢ You can prove the simpler approach won't work              â”‚
â”‚   â€¢ The math forces your hand, not the pattern library         â”‚
â”‚                                                                 â”‚
â”‚   EVERY BOX YOU DRAW should solve a problem you've             â”‚
â”‚   already proven exists.                                        â”‚
â”‚                                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚   â”‚  Simple  â”‚â”€â”€â”€â”€â–ºâ”‚ Measure  â”‚â”€â”€â”€â”€â–ºâ”‚  Evolve  â”‚              â”‚
â”‚   â”‚  Design  â”‚     â”‚ Bottleneckâ”‚    â”‚  Targeted â”‚              â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚        â”‚                                   â”‚                    â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Repeat â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Red Flags That Signal Pattern Matching**:

| Red Flag | What It Reveals | Better Approach |
|----------|-----------------|-----------------|
| "We need consistent hashing" | Reaching for patterns before understanding the problem | "Our key distribution is X, so we need Y because..." |
| "Add Redis for caching" | Assuming caching always helps | "Our read/write ratio is X:1, hit rate would be ~Y%" |
| "Use Kafka for messaging" | Pattern matching on queue choice | "We need at-least-once delivery because... Kafka's log compaction helps with..." |
| "Shard the database" | Assuming write scaling is needed | "Current write QPS is X, single-node limit is Y, so we need Z shards" |
| "Add a load balancer" | Reflexive complexity | "We have N servers because each handles X QPS" |

**The Mindset Shift**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     PATTERN MATCHER                             â”‚
â”‚                                                                 â”‚
â”‚   â€¢ Starts with architecture diagrams                          â”‚
â”‚   â€¢ Adds components "because that's what you do"               â”‚
â”‚   â€¢ Can draw systems but can't explain trade-offs              â”‚
â”‚   â€¢ Freezes when constraints change                            â”‚
â”‚   â€¢ Knows WHAT to build, not WHY                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SYSTEM DESIGNER                              â”‚
â”‚                                                                 â”‚
â”‚   â€¢ Starts with numbers and constraints                        â”‚
â”‚   â€¢ Adds components when metrics force the decision            â”‚
â”‚   â€¢ Can defend every trade-off with data                       â”‚
â”‚   â€¢ Adapts when constraints shift                              â”‚
â”‚   â€¢ Knows WHY before deciding WHAT                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Practice Exercise**: Take any system design you've studied. Pick one component. Remove it. Can the system still work?
- If **yes**: You probably didn't need it.
- If **no**: What metric proves it's necessary?

That's how you build judgment instead of just pattern fluency.

### ğŸ’¥ Component Failure Analysis Framework

For every component in your design, systematically ask: **"What happens when X fails/degrades?"** This framework gives you the thinking model for each common component.

---

#### ğŸ—„ï¸ DATABASE

**Failure Modes:**

| Scenario | What Happens | Detection | Response |
|----------|--------------|-----------|----------|
| **Primary goes down** | Writes fail, reads may fail | Health checks, connection errors | Promote replica, redirect traffic |
| **Writes succeed, reads lag** | Users see stale data | Replication lag metrics | Read-your-writes pattern, sticky sessions |
| **Disk fills up** | Writes rejected, possible corruption | Disk usage alerts (>80%) | Auto-expand, archive old data, alert |
| **Connection pool exhausted** | New requests timeout | Pool metrics, error rates | Increase pool, add read replicas, queue requests |
| **Split-brain (multi-master)** | Data divergence, conflicts | Quorum failures, conflict detection | Fencing tokens, quorum writes, manual resolution |
| **Slow queries** | Cascading timeouts, resource starvation | P99 latency, slow query log | Query optimization, read replicas, caching |
| **Corruption** | Silent data loss | Checksums, backup validation | Point-in-time recovery, checksums on read |

**Design Questions:**
```
â€¢ What's the failover time? (seconds vs minutes matters)
â€¢ Can we tolerate stale reads? For how long?
â€¢ What's our backup/recovery RPO and RTO?
â€¢ How do we detect silent corruption?
```

---

#### âš¡ CACHE (Redis, Memcached)

**Failure Modes:**

| Scenario | What Happens | Detection | Response |
|----------|--------------|-----------|----------|
| **Cache miss storm** | All requests hit DB simultaneously | DB load spike, cache hit rate drop | Circuit breaker, request coalescing, staggered TTL |
| **Cache goes down entirely** | 100% traffic to DB | Health check failure | Graceful degradation, DB can handle (or reject overflow) |
| **Stale data after write** | Users see old data | Requires monitoring writes | Write-through, cache invalidation, short TTL |
| **Thundering herd on expiry** | Spike when popular key expires | Traffic patterns, expiry timing | Probabilistic early expiration, mutex/lock |
| **Memory pressure / eviction** | Important keys evicted | Eviction metrics, miss rate | Increase memory, tune eviction policy, tiered caching |
| **Hot key** | Single node overwhelmed | Per-key metrics, node CPU | Key replication, local caching, key splitting |
| **Network partition from app** | Timeouts, fallback to DB | Connection errors, latency | Fast timeout + fallback, don't block on cache |

**Design Questions:**
```
â€¢ What's the cache hit rate? Is caching worth it? (>80% usually needed)
â€¢ What's the invalidation strategy? (TTL, event-based, write-through)
â€¢ Can we survive total cache failure? (DB must handle peak if not)
â€¢ What's the consistency model? (eventual OK, or need strong?)
```

---

#### âš–ï¸ LOAD BALANCER

**Failure Modes:**

| Scenario | What Happens | Detection | Response |
|----------|--------------|-----------|----------|
| **LB itself fails** | Total outage | DNS/anycast failover, health checks | Redundant LBs, DNS failover, anycast |
| **Health checks pass but service deadlocked** | Traffic to broken servers | Application-level health, request latency | Deep health checks (not just TCP), liveness + readiness |
| **Uneven distribution** | Some servers overloaded | Per-server metrics, latency variance | Review algorithm (round-robin vs least-conn), consistent hashing |
| **Sticky session server dies** | User session lost | Session errors, re-auth rate | Session externalization (Redis), graceful degradation |
| **Slow backend not removed** | Cascading slowdowns | P99 latency, queue depth | Active health checks, circuit breaker, timeout-based ejection |
| **SSL termination overload** | Connection failures | CPU usage, connection errors | Dedicated SSL offload, hardware acceleration |

**Design Questions:**
```
â€¢ What's the failover mechanism for the LB itself?
â€¢ How quickly are unhealthy backends removed? (seconds matter)
â€¢ What's the health checkâ€”TCP, HTTP, or application-specific?
â€¢ How do we handle session affinity without coupling?
```

---

#### ğŸ“¬ MESSAGE QUEUE (Kafka, SQS, RabbitMQ)

**Failure Modes:**

| Scenario | What Happens | Detection | Response |
|----------|--------------|-----------|----------|
| **Consumer falls behind** | Queue backlog grows, memory pressure | Consumer lag metrics, queue depth | Scale consumers, increase batch size, alert on lag |
| **Duplicate messages** | Action performed twice | Idempotency key collisions | Idempotent consumers, deduplication layer |
| **Message loss** | Data lost, inconsistent state | End-to-end tracking, checksums | Acks, replication, dead-letter queue |
| **Poison message** | Consumer crashes repeatedly | Consumer crash rate, DLQ growth | Dead-letter queue, circuit breaker, manual review |
| **Broker failure** | Partition unavailable | Broker health, under-replicated partitions | Replication factor â‰¥3, automatic leader election |
| **Out of order processing** | Logic errors, race conditions | Sequence gaps, business logic errors | Partition by ordering key, sequence numbers |
| **Consumer rebalance storm** | Processing paused during rebalance | Rebalance events, processing gaps | Stable consumer groups, cooperative rebalancing |

**Design Questions:**
```
â€¢ What's our guarantee? (at-most-once, at-least-once, exactly-once)
â€¢ How do we handle duplicates? (idempotency keys)
â€¢ What happens to failed messages? (DLQ, retry, alert)
â€¢ What's acceptable consumer lag? (seconds, minutes, hours)
```

---

#### ğŸŒ API GATEWAY / SERVICE MESH

**Failure Modes:**

| Scenario | What Happens | Detection | Response |
|----------|--------------|-----------|----------|
| **Rate limit misconfigured** | Legitimate traffic rejected | 429 error rate, customer complaints | Gradual rollout, per-tenant limits, bypass for critical |
| **Gateway becomes bottleneck** | All services degraded | Gateway latency, CPU | Horizontal scaling, edge caching, bypass for internal |
| **Auth service down** | All authenticated requests fail | Auth error rate | Cache tokens, graceful degradation, retry with backoff |
| **Circuit breaker stuck open** | Service appears down when recovered | Circuit state, manual override | Time-based reset, health check override, monitoring |
| **Timeout too aggressive** | Slow but valid requests killed | Timeout errors vs actual failures | Adaptive timeouts, P99-based, separate read/write |
| **Cascading failure** | One slow service affects all | Cross-service latency correlation | Bulkheads, circuit breakers, async where possible |

**Design Questions:**
```
â€¢ What's the timeout strategy? (too short = false failures, too long = resource exhaustion)
â€¢ How do circuit breakers reset? (automatic vs manual)
â€¢ What's the rate limiting granularity? (per user, per IP, per tenant)
â€¢ What happens when auth is down? (fail open vs fail closed)
```

---

#### ğŸ’¾ DISTRIBUTED STORAGE (S3, GCS, HDFS)

**Failure Modes:**

| Scenario | What Happens | Detection | Response |
|----------|--------------|-----------|----------|
| **Region outage** | Data unavailable | Region health, availability metrics | Multi-region replication, cross-region failover |
| **Throttling** | Requests rejected | 503/429 errors, request metrics | Exponential backoff, request spreading, higher tier |
| **Eventual consistency bite** | Read after write returns old data | Application logic errors | Strong read (if available), read-your-writes pattern |
| **Large object timeout** | Upload/download fails | Transfer errors, partial uploads | Multipart upload, resumable transfers, chunking |
| **Cost explosion** | Unexpected bills | Cost monitoring, usage alerts | Lifecycle policies, tiered storage, cleanup jobs |
| **Accidental deletion** | Data loss | Audit logs, versioning | Versioning, soft delete, MFA delete, backups |

**Design Questions:**
```
â€¢ What's the consistency model? (strong vs eventual)
â€¢ What's the durability guarantee? (11 9s for S3, but region matters)
â€¢ How do we handle large files? (multipart, streaming)
â€¢ What's the access pattern? (random vs sequential affects storage choice)
```

---

#### ğŸ” SEARCH / ELASTICSEARCH

**Failure Modes:**

| Scenario | What Happens | Detection | Response |
|----------|--------------|-----------|----------|
| **Index lag** | Search returns stale results | Index freshness metrics | Near-real-time indexing, user expectations |
| **Query of death** | Cluster overwhelmed | CPU spike, timeout rate | Query validation, circuit breaker, query timeout |
| **Shard imbalance** | Hot spots, uneven performance | Per-shard metrics | Rebalancing, shard allocation awareness |
| **Split brain** | Data inconsistency | Cluster health, node count | Minimum master nodes, dedicated masters |
| **Memory pressure** | Slow queries, OOM | JVM heap, GC metrics | Heap tuning, circuit breakers, field data limits |
| **Mapping explosion** | Index bloat, performance degradation | Field count, mapping size | Strict mappings, dynamic template limits |

**Design Questions:**
```
â€¢ What's acceptable index lag? (real-time vs near-real-time vs batch)
â€¢ How do we handle query of death? (validation, timeouts, circuit breakers)
â€¢ What's the reindexing strategy? (zero-downtime, aliases)
â€¢ How do we scale? (more shards vs more nodes vs both)
```

---

#### ğŸ”— EXTERNAL API / THIRD-PARTY SERVICE

**Failure Modes:**

| Scenario | What Happens | Detection | Response |
|----------|--------------|-----------|----------|
| **Service down** | Dependent feature fails | Health checks, error rate | Circuit breaker, fallback, graceful degradation |
| **Rate limited** | Requests rejected | 429 errors | Backoff, request queuing, caching, higher tier |
| **Slow response** | Timeout, resource holding | Latency metrics | Aggressive timeouts, async, don't block critical path |
| **Breaking API change** | Integration fails | Schema validation, error patterns | Contract testing, versioned clients, monitoring |
| **Cost overrun** | Unexpected charges | Usage tracking, billing alerts | Caching, batching, usage caps |
| **Data quality issues** | Incorrect data in our system | Validation, anomaly detection | Input validation, sanity checks, alerts |

**Design Questions:**
```
â€¢ What's the SLA of the external service? (matches our needs?)
â€¢ What's the fallback when it's down? (cached data, degraded mode, error)
â€¢ How do we prevent our system from failing when theirs does?
â€¢ What's the cost model? (per request, per data volume)
```

---

#### ğŸ§  THE UNIVERSAL FAILURE CHECKLIST

For **ANY** component, ask these questions:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              UNIVERSAL COMPONENT FAILURE CHECKLIST              â”‚
â”‚                                                                 â”‚
â”‚   1. DETECTION                                                  â”‚
â”‚      â–¡ How do we know it's failing? (metrics, alerts)          â”‚
â”‚      â–¡ How do we know it's slow vs down? (latency vs errors)   â”‚
â”‚      â–¡ How do we detect silent failures? (data corruption)     â”‚
â”‚                                                                 â”‚
â”‚   2. IMPACT                                                     â”‚
â”‚      â–¡ What breaks if this fails? (blast radius)               â”‚
â”‚      â–¡ Can we operate in degraded mode? (graceful degradation) â”‚
â”‚      â–¡ What's the user-visible impact? (error vs slow vs none) â”‚
â”‚                                                                 â”‚
â”‚   3. RECOVERY                                                   â”‚
â”‚      â–¡ How long to detect? (MTTD)                              â”‚
â”‚      â–¡ How long to recover? (MTTR)                             â”‚
â”‚      â–¡ Is recovery automatic or manual?                        â”‚
â”‚      â–¡ What data is lost during recovery? (RPO)                â”‚
â”‚                                                                 â”‚
â”‚   4. PREVENTION                                                 â”‚
â”‚      â–¡ Redundancy: Is there a backup? (replicas, regions)      â”‚
â”‚      â–¡ Isolation: Does failure cascade? (bulkheads)            â”‚
â”‚      â–¡ Limits: Can it be overwhelmed? (rate limiting, quotas)  â”‚
â”‚                                                                 â”‚
â”‚   5. TESTING                                                    â”‚
â”‚      â–¡ Have we tested this failure? (chaos engineering)        â”‚
â”‚      â–¡ Do we have runbooks? (what to do when X fails)          â”‚
â”‚      â–¡ When was the last failover test?                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### ğŸ¯ INTERVIEW APPLICATION

When the interviewer asks **"What happens when X fails?"**, use this structure:

```
1. ACKNOWLEDGE the failure mode specifically
   "If the primary database fails mid-transaction..."

2. DETECT how we'd know
   "We'd detect this via connection errors and health check failures within 5 seconds..."

3. IMPACT what breaks
   "In-flight writes would fail, users would see errors for ~30 seconds..."

4. RESPOND what the system does automatically
   "The replica would be promoted automatically, connections would retry..."

5. RECOVER to normal state
   "After failover, we'd verify data consistency and clear any stuck transactions..."

6. PREVENT or mitigate in future
   "To minimize this, we use synchronous replication for zero data loss..."
```

**Example Answer:**
> "What happens when the cache fails?"
>
> "If Redis fails completely, we'd detect it within 1-2 seconds via connection timeouts. The impact is that all traffic immediately hits the databaseâ€”which is why we size the DB to handle peak load without cache (about 10K QPS in our case). The response is automatic: our cache client has a 100ms timeout and falls back to direct DB queries. We'd recover by restarting Redis and letting it warm up graduallyâ€”we use lazy population rather than pre-warming to avoid thundering herd. To prevent this being catastrophic, we run Redis in cluster mode with replicas, so single-node failure doesn't cause full cache loss."

### âš–ï¸ Trade-off Decision Matrix

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  TRADE-OFF MATRIX                               â”‚
â”‚                                                                 â”‚
â”‚   Decision          â”‚ Option A         â”‚ Option B              â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚   SQL vs NoSQL      â”‚ ACID, JOINs      â”‚ Scale, flexibility    â”‚
â”‚   Push vs Pull      â”‚ Low read latency â”‚ Lower write cost      â”‚
â”‚   Cache vs DB       â”‚ Speed            â”‚ Consistency           â”‚
â”‚   Sync vs Async     â”‚ Consistency      â”‚ Performance           â”‚
â”‚   Monolith vs Micro â”‚ Simplicity       â”‚ Scalability           â”‚
â”‚   Strong vs Event.  â”‚ Correctness      â”‚ Availability          â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚   TCP vs UDP        â”‚ Reliability      â”‚ Speed                 â”‚
â”‚   Direct vs VPN     â”‚ Performance      â”‚ Cost, simplicity      â”‚
â”‚   Block vs Object   â”‚ Low latency      â”‚ Scalability, cost     â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚   There's no universally "right" choiceâ€”it depends on your    â”‚
â”‚   specific requirements, constraints, and priorities.          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 14. Glossary

**Quick reference for all acronyms and terms used in this guide.** Organized by category for easy lookup.

### Core Concepts

| Acronym | Full Name | Plain English Definition |
|---------|-----------|--------------------------|
| **ACID** | Atomicity, Consistency, Isolation, Durability | Four properties that guarantee database transactions are processed reliably |
| **CAP** | Consistency, Availability, Partition Tolerance | Theorem stating distributed systems can only guarantee two of three properties |
| **BASE** | Basically Available, Soft state, Eventually consistent | Alternative to ACID for distributed systems prioritizing availability |
| **2PC** | Two-Phase Commit | Protocol ensuring all participants in a distributed transaction commit or abort together |
| **SAGA** | â€” | Pattern for long-running distributed transactions using compensating actions |
| **RSM** | Replicated State Machine | Approach to building fault-tolerant systems by replicating state across nodes |

### Cloud & Networking

| Acronym | Full Name | Plain English Definition |
|---------|-----------|--------------------------|
| **VPC** | Virtual Private Cloud | Your isolated network within the cloud with custom IP ranges and routing |
| **IAM** | Identity and Access Management | Controls who can do what in your cloud environment |
| **CIDR** | Classless Inter-Domain Routing | Notation for IP address ranges (e.g., 10.0.0.0/16) |
| **DNS** | Domain Name System | Translates domain names (example.com) to IP addresses |
| **TLD** | Top-Level Domain | The last part of a domain name (.com, .org, .io) |
| **NAT** | Network Address Translation | Allows private subnet resources to access the internet without exposing them |
| **IGW** | Internet Gateway | Enables communication between VPC and the internet |
| **SG** | Security Group | Instance-level firewall; stateful, allow-only rules |
| **NACL** | Network Access Control List | Subnet-level firewall; stateless, allow and deny rules |
| **TCP** | Transmission Control Protocol | Reliable, connection-oriented protocol for data transmission |
| **UDP** | User Datagram Protocol | Fast, connectionless protocol; no delivery guarantee |
| **IP** | Internet Protocol | Protocol for routing packets across networks using addresses |
| **HTTP/HTTPS** | Hypertext Transfer Protocol (Secure) | Protocol for web communication; HTTPS adds encryption |
| **TLS/SSL** | Transport Layer Security / Secure Sockets Layer | Protocols for encrypting data in transit |
| **WAF** | Web Application Firewall | Protects web apps by filtering malicious HTTP traffic |
| **VPN** | Virtual Private Network | Encrypted tunnel between networks over the internet |
| **AZ** | Availability Zone | Isolated data center within a cloud region |
| **MFA** | Multi-Factor Authentication | Requires multiple verification methods for access |

### Databases

| Acronym | Full Name | Plain English Definition |
|---------|-----------|--------------------------|
| **SQL** | Structured Query Language | Language for managing relational databases; also refers to relational DBs |
| **NoSQL** | Not Only SQL | Family of non-relational databases (key-value, document, graph, columnar) |
| **RDBMS** | Relational Database Management System | Database that stores data in tables with relationships (MySQL, PostgreSQL) |
| **IOPS** | Input/Output Operations Per Second | Measure of storage performance (reads + writes per second) |
| **RDS** | Relational Database Service | AWS managed relational database service |
| **KV** | Key-Value | Database type storing data as key-value pairs (Redis, DynamoDB) |
| **ORM** | Object-Relational Mapping | Converts between database records and programming objects |
| **MVCC** | Multi-Version Concurrency Control | Database technique allowing concurrent transactions without locking |

### Caching

| Acronym | Full Name | Plain English Definition |
|---------|-----------|--------------------------|
| **TTL** | Time To Live | How long cached data remains valid before expiring |
| **EAT** | Effective Access Time | Weighted average of cache hit and miss times |
| **LRU** | Least Recently Used | Eviction policy removing the least recently accessed items first |
| **LFU** | Least Frequently Used | Eviction policy removing the least often accessed items first |
| **CDN** | Content Delivery Network | Globally distributed cache serving content from edge locations |

### Messaging & Queues

| Acronym | Full Name | Plain English Definition |
|---------|-----------|--------------------------|
| **DLQ** | Dead Letter Queue | Queue for messages that repeatedly fail processing |
| **Pub/Sub** | Publish-Subscribe | Messaging pattern where publishers broadcast to multiple subscribers |
| **MQ** | Message Queue | System for asynchronous communication between services |
| **FIFO** | First In, First Out | Queue ordering where oldest messages are processed first |
| **ACK** | Acknowledgment | Confirmation that a message was received/processed |

### Load Balancing & Components

| Acronym | Full Name | Plain English Definition |
|---------|-----------|--------------------------|
| **LB** | Load Balancer | Distributes incoming traffic across multiple servers |
| **ALB** | Application Load Balancer | Layer 7 load balancer routing based on HTTP content |
| **NLB** | Network Load Balancer | Layer 4 load balancer routing based on IP/port; ultra-low latency |
| **L4** | Layer 4 (Transport) | Network layer dealing with TCP/UDP; IP addresses and ports |
| **L7** | Layer 7 (Application) | Network layer dealing with HTTP/HTTPS; URLs, headers, cookies |
| **RPS** | Requests Per Second | Measure of throughput; how many requests a system handles |
| **API** | Application Programming Interface | Contract defining how software components communicate |
| **REST** | Representational State Transfer | Architectural style for APIs using HTTP methods |
| **gRPC** | gRPC Remote Procedure Call | High-performance RPC framework using Protocol Buffers |
| **JWT** | JSON Web Token | Compact, self-contained token for secure information exchange |
| **CORS** | Cross-Origin Resource Sharing | Mechanism allowing web pages to request resources from other domains |

### Scalability & Reliability

| Acronym | Full Name | Plain English Definition |
|---------|-----------|--------------------------|
| **HA** | High Availability | System designed to minimize downtime; typically 99.9%+ uptime |
| **DR** | Disaster Recovery | Plans and systems for recovering from catastrophic failures |
| **RTO** | Recovery Time Objective | Maximum acceptable downtime after a failure |
| **RPO** | Recovery Point Objective | Maximum acceptable data loss (time since last backup) |
| **SPOF** | Single Point of Failure | Component whose failure causes entire system failure |
| **MTTR** | Mean Time To Recovery | Average time to restore service after failure |
| **MTTD** | Mean Time To Detect | Average time to detect a failure |
| **SLA** | Service Level Agreement | Contract specifying uptime and performance guarantees |
| **SLO** | Service Level Objective | Internal targets for service performance |
| **SLI** | Service Level Indicator | Metrics used to measure SLO compliance |

### Distributed Systems

| Acronym | Full Name | Plain English Definition |
|---------|-----------|--------------------------|
| **Raft** | â€” | Consensus algorithm for leader election and log replication |
| **Paxos** | â€” | Consensus algorithm for distributed agreement (more complex than Raft) |
| **ZK** | ZooKeeper | Distributed coordination service for leader election, config, locks |
| **etcd** | â€” | Distributed key-value store used by Kubernetes for coordination |
| **W/R/N** | Write quorum / Read quorum / Total replicas | Quorum parameters: W+R>N guarantees strong consistency |
| **KL** | Kullback-Leibler (divergence) | Measure of difference between probability distributions |

### Capacity & Performance

| Acronym | Full Name | Plain English Definition |
|---------|-----------|--------------------------|
| **QPS** | Queries Per Second | Rate of requests a system processes |
| **TPS** | Transactions Per Second | Rate of database transactions processed |
| **DAU** | Daily Active Users | Number of unique users per day |
| **MAU** | Monthly Active Users | Number of unique users per month |
| **P50/P95/P99** | Percentile latencies | 50th/95th/99th percentile response times |
| **TTFB** | Time To First Byte | Time from request to receiving first byte of response |

### Storage

| Acronym | Full Name | Plain English Definition |
|---------|-----------|--------------------------|
| **EBS** | Elastic Block Store | AWS block storage for EC2 instances |
| **EFS** | Elastic File System | AWS managed NFS file system |
| **S3** | Simple Storage Service | AWS object storage with unlimited capacity |
| **NFS** | Network File System | Protocol for sharing files over a network |
| **HDFS** | Hadoop Distributed File System | Distributed file system for big data processing |
| **SSD** | Solid State Drive | Fast storage using flash memory; no moving parts |
| **HDD** | Hard Disk Drive | Traditional storage using spinning magnetic disks |

### AWS Services

| Acronym | Full Name | Plain English Definition |
|---------|-----------|--------------------------|
| **EC2** | Elastic Compute Cloud | AWS virtual machine service |
| **ECS** | Elastic Container Service | AWS managed container orchestration |
| **EKS** | Elastic Kubernetes Service | AWS managed Kubernetes |
| **Lambda** | â€” | AWS serverless compute; runs code without managing servers |
| **SQS** | Simple Queue Service | AWS managed message queue |
| **SNS** | Simple Notification Service | AWS managed pub/sub messaging |
| **Route 53** | â€” | AWS DNS and traffic management service |
| **CloudFront** | â€” | AWS CDN for content delivery |
| **ElastiCache** | â€” | AWS managed Redis/Memcached |
| **DynamoDB** | â€” | AWS managed NoSQL key-value database |
| **KMS** | Key Management Service | AWS service for creating and managing encryption keys |
| **HSM** | Hardware Security Module | Physical device for secure key storage and cryptographic operations |

### Data Units

| Unit | Size | Context |
|------|------|---------|
| **KB** | 1,024 bytes | Small files, API responses |
| **MB** | 1,024 KB | Images, small documents |
| **GB** | 1,024 MB | Videos, databases |
| **TB** | 1,024 GB | Large databases, data warehouses |
| **PB** | 1,024 TB | Big data, enterprise storage |
| **Gbps** | Gigabits per second | Network bandwidth (divide by 8 for MB/s) |

---

<div align="center">

**System Design = Trade-offs + Scale + Failure Modes + Numbers**

*This guide provides foundational knowledge for building production systems. The best design is the simplest one that meets your needs.*

â­ Star this repo if it helped you!

*Last updated: February 2026*

</div>
