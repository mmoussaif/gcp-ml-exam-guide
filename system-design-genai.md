# ML & GenAI System Design Guide

A comprehensive guide to designing **ML (Machine Learning)** and **GenAI (Generative AI)** systems at scale, covering **LLM (Large Language Model)** serving, **RAG** (retrieval-augmented generation) systems, agentic AI, **MLOps** (ML operations) pipelines, and production considerations.

---

## Prerequisites

This guide focuses specifically on **ML and GenAI system design**. For foundational system design concepts (databases, caching, load balancing, networking, CAP theorem, etc.), see:

ðŸ“– **[System Design Essentials](./system-design-essentials.md)** - Core system design knowledge applicable to all distributed systems.

---

## Table of Contents

- [Introduction](#introduction)
- [GenAI System: Big Picture (Frontend to Backend)](#genai-system-big-picture-frontend-to-backend)
- [GenAI vs Traditional ML](#genai-vs-traditional-ml)
- [Using Models & Sampling Parameters](#using-models--sampling-parameters)
- [Google Generative AI Development Tools](#google-generative-ai-development-tools)
- [1. LLM Serving Architecture](#1-llm-serving-architecture-at-scale)
- [2. RAG Systems](#2-rag-retrieval-augmented-generation-system)
- [3. RAG vs Fine-Tuning](#3-rag-vs-fine-tuning-decision-framework)
- [4. Agentic AI Systems](#4-agentic-ai-systems)
- [5. LLM Evaluation & Quality](#5-llm-evaluation--quality)
- [6. GenAI Data Pipeline](#6-genai-data-pipeline-architecture)
- [7. Cost Optimization & Model Routing](#7-cost-optimization-for-genai-systems)
- [8. Scalability Patterns](#8-scalability-patterns-for-genai)
- [9. Monitoring & Observability](#9-monitoring--observability-for-genai)
- [10. Security & Guardrails](#10-security--guardrails)
- [11. Real-World Examples](#11-real-world-examples-applying-the-stack)
- [Resources](#resources)

---

## Introduction

Generative AI applications introduce unique challenges that differ significantly from traditional software systems:

- **Token-by-token generation**: Sequential decoding (unlike batch predictions)
- **Variable latency**: Generation time depends on output length
- **High memory requirements**: **KV cache** (key-value cache: stored attention keys and values in transformers) for attention mechanisms
- **Cost optimization**: Balance between latency and throughput
- **Hallucination management**: Ensuring factual accuracy
- **Agent orchestration**: Multi-step reasoning and tool use

This guide covers how to design, build, and operate GenAI systems at scale.

**Aha:** GenAI system design is different because you're optimizing for **non-determinism** (same prompt â†’ different outputs), **token economics** (cost and latency scale with length), and **orchestration** (models + retrieval + tools), not just throughput of identical requests.

---

## GenAI System: Big Picture (Frontend to Backend)

Before diving into components, here is the end-to-end shape of a GenAI system. The **request path** runs from frontend to backend; **supporting systems** (data pipelines, evaluation, monitoring, security) surround that path. Each numbered section later in this guide is a T-shaped deep dive on one layer or concern: broad role in this picture first, then detail.

**Request path (frontend â†’ backend):**

```
  User / Frontend
        â”‚
        â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  API Gateway    â”‚  Auth, rate limit, route
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Orchestration  â”‚  Agent, RAG, tools (sections 2, 4)
  â”‚  (Agent / RAG)  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  LLM(s)         â”‚  Inference, model routing (section 1)
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
  Response (â†’ user, or â†’ tools, then back into orchestration)
```

**Supporting systems (around the request path):**

| System | Role in the big picture | Deep dive |
|--------|--------------------------|-----------|
| **Evaluation** | "Did we build the right thing?" â€” quality, grounding, safety on a sample of requests | Â§5 Evaluation & Quality (metrics + eval *data* pipeline at scale) |
| **Training data pipeline** | "Where do fine-tuning examples come from?" â€” user interactions â†’ events â†’ lake â†’ training prep | Â§6 GenAI Data Pipeline |
| **Cost** | "How do we keep inference affordable?" â€” tokens, caching, model routing, quantization | Â§7 Cost Optimization |
| **Scale** | "How do we serve more load?" â€” horizontal scaling, model/pipeline parallelism, KV cache | Â§8 Scalability |
| **Monitoring** | "How do we observe the system?" â€” metrics, traces, drift | Â§9 Monitoring & Observability |
| **Security** | "How do we protect inputs, outputs, and access?" â€” guardrails, Model Armor, IAM | Â§10 Security & Guardrails |
| **Real-world examples** | "How do I build this with real tools?" â€” apply Â§1â€“Â§10 with LangChain, AWS, Google, open source | Â§11 Real-World Examples |

**Rationale in one line:** The **request path** (gateway â†’ orchestration â†’ LLM) is what users hit. **Evaluation** and **training data** are two different data flows: eval = "log predictions â†’ run quality metrics" (Â§5); training = "log interactions â†’ clean â†’ fine-tune" (Â§6). **Cost** (Â§7) is *spend per request*; **scale** (Â§8) is *throughput and capacity*. **Monitoring** (Â§9) and **security** (Â§10) are cross-cutting. **Examples** (Â§11) come last so you can apply everything with concrete stacks.

**Logical flow of this guide:** Big Picture â†’ foundations (GenAI vs ML, sampling, Google tools) â†’ **request path** (Â§1 Serving, Â§2 RAG, Â§3 RAG vs FT, Â§4 Agents) â†’ **evaluation** (Â§5: what to measure + eval data pipeline at scale; *consolidated* so "evaluation" is one place) â†’ **training data** (Â§6) â†’ **efficiency** (Â§7 Cost, Â§8 Scale) â†’ **operations** (Â§9 Monitoring, Â§10 Security) â†’ **Â§11 Real-World Examples** (apply Â§1â€“Â§10 with LangChain, AWS, Google, open source). Examples are last so every concept is already defined when you see concrete solutioning.

---

## GenAI vs Traditional ML

Understanding the fundamental differences between traditional ML systems and **GenAI** / **LLM (Large Language Model)** systems is crucial for making the right architectural decisions.

| Aspect         | Traditional ML       | GenAI/LLM                              |
| -------------- | -------------------- | -------------------------------------- |
| **Prediction** | Single forward pass  | Token-by-token generation              |
| **Latency**    | Fixed (milliseconds) | Variable (seconds to minutes)          |
| **Memory**     | Model weights        | Model + KV cache (grows with sequence) |
| **Batching**   | Static batches       | Dynamic/continuous batching            |
| **Cost**       | Per-request          | Per-token (input + output)             |
| **Control**    | Fixed weights        | Sampling parameters (temp, top-p, top-k) |

**Why these differences matter:**

- **Token-by-token generation** means you can't predict exact response timeâ€”a 10-token response is much faster than a 1000-token response.
- **KV cache growth** means memory requirements increase with context length, limiting how many concurrent requests you can serve.
- **Per-token pricing** means prompt engineering and response length directly impact costs.

**Aha:** Traditional ML is "one input â†’ one prediction." GenAI is "one prompt â†’ a stream of tokens, each depending on the last." That shifts bottlenecks from GPU compute to memory (KV cache), latency (time-to-first-token vs total time), and cost (every token billed).

---

## Using Models & Sampling Parameters

Generative AI agents are powered by models that act as the "brains" of the operation. While models are pre-trained, their behavior during inference can be customized using **sampling parameters**â€”the "knobs and dials" of the model.

### Common Sampling Parameters

**1. Temperature**

Controls the "creativity" or randomness of the output by rescaling logits before softmax.

- **High Temperature (T > 1)**: Flattens the distribution, making output more random, diverse, and unpredictable. Increases risk of incoherence.
- **Low Temperature (T < 1)**: Sharpens the distribution, making it more focused, deterministic, and repeatable.
- **Extreme (T â†’ 0)**: Collapses into greedy decoding (always picks the highest probability token).

*Use low temperature (0.1-0.3) for factual tasks, higher (0.7-1.0) for creative tasks.*

**Aha:** Temperature rescales logits before sampling. Low T makes the top token dominate (nearly deterministic); high T flattens the distribution so unlikely tokens get a real chance. You're tuning "how much to trust the model's confidence."

**2. Top-p (Nucleus Sampling)**

Selects the smallest set of tokens whose cumulative probability mass reaches threshold *p*.

- **High Top-p (0.9-1.0)**: Allows for more diversity by extending to lower probability tokens.
- **Low Top-p (0.1-0.5)**: Leads to more focused responses.
- **Adaptive**: Unlike Top-K, adapts to the distribution's shapeâ€”in confident contexts, the "nucleus" is small.

**Aha:** Top-p says "consider only tokens that together account for probability mass *p*." When the model is sure, that might be 2â€“3 tokens; when unsure, many more. So Top-p scales with confidence; Top-K does not.

**3. Top-K**

Restricts the model's choice to only the *k* most probable tokens at each step.

- Improves output stability by eliminating the "long tail" of extremely unlikely tokens.
- **Limitation**: Unlike Top-p, it is not adaptive to the distribution's shape.

**4. Maximum Length (Max New Tokens)**

Determines the maximum number of tokens to generate before stopping.

- Prevents runaway generation ("rambling") and controls compute costs.
- Models stop early if they hit an end-of-sequence (`<EOS>`) token.

**5. Repetition Penalty**

A factor (usually > 1.0) used to discount the probability of tokens that have already appeared in the output.

- Prevents the model from getting stuck in repetitive loops (e.g., "I'm not sure. I'm not sure.").

**6. Safety Settings**

Filters that block potentially harmful or inappropriate content (hate speech, harassment, etc.).

- Essential for enterprise-grade applications to ensure outputs align with safety policies.

### Accessing Parameters via APIs

Most generative AI models are accessed via **APIs**. The flow:

1. Your application sends a **Prompt** + **Sampling Parameters**
2. The API delivers these to the model
3. The model generates a response based on those specific parameters
4. The API returns the response to your application

---

## Google Generative AI Development Tools

Google provides two primary environments for experimenting with and deploying Gemini models:

| Attribute | Google AI Studio | Vertex AI Studio |
| :--- | :--- | :--- |
| **Focus** | Streamlined, easy-to-use interface for rapid prototyping | Comprehensive environment for building, training, and deploying ML models |
| **Target Users** | Beginners, hobbyists, initial project stages | Professionals, researchers, enterprise developers |
| **Access** | Standard Google Account login | Google Cloud Console (Enterprise account) |
| **Limitations** | Usage limits (**QPM** queries/min, **RPM** requests/min, **TPM** tokens/min); small-scale projects | Service charges based on usage; enterprise-grade quotas |
| **Advantages** | Simplified interface; easy to get started | Enterprise-grade security, compliance, flexible quotas |

**Key Takeaway**: Use **Google AI Studio** for fast, small-scale prototyping. Transition to **Vertex AI Studio** for large-scale, production-ready enterprise applications.

### Google's Generative AI APIs

Google's generative AI APIs offer pre-trained foundation models that can be fine-tuned for specific tasks:

- **Text Completion**: Generating long-form content or completing snippets
- **Multi-turn Chat**: Maintaining state across several turns of conversation
- **Code Generation**: Specialized models for writing and debugging code
- **Image Generation**: Using the Imagen API to create and customize images

---

## 1. LLM Serving Architecture at Scale

### Use Case: Design a Chatbot Service (like ChatGPT)

**Requirements:**
- Support 1M concurrent users
- Average response time < 2 seconds
- Handle 10,000 requests/second
- Support multiple models (GPT-4, Claude, Gemini)
- Cost-effective serving

**High-Level Design:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LLM SERVING ARCHITECTURE                     â”‚
â”‚                                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚ Clients  â”‚â”€â”€â”€â”€â–ºâ”‚ API Gateway â”‚â”€â”€â”€â”€â–ºâ”‚  Request Router   â”‚   â”‚
â”‚   â”‚ Web/API  â”‚     â”‚ Auth, Rate  â”‚     â”‚  Load Balancer    â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ Limiting    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚             â”‚
â”‚                                                  â”‚             â”‚
â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚                          â”‚                       â”‚         â”‚   â”‚
â”‚                          â–¼                       â–¼         â–¼   â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚                    â”‚   Cache   â”‚           â”‚  LLM Serving      â”‚â”‚
â”‚                    â”‚  (Redis)  â”‚           â”‚  Infrastructure   â”‚â”‚
â”‚                    â”‚           â”‚           â”‚                   â”‚â”‚
â”‚                    â”‚â€¢ Prompt   â”‚           â”‚ â€¢ Vertex AI       â”‚â”‚
â”‚                    â”‚  Cache    â”‚           â”‚ â€¢ SageMaker       â”‚â”‚
â”‚                    â”‚â€¢ Response â”‚           â”‚ â€¢ vLLM/TensorRT   â”‚â”‚
â”‚                    â”‚  Cache    â”‚           â”‚                   â”‚â”‚
â”‚                    â”‚â€¢ Semantic â”‚           â”‚ Continuous batch  â”‚â”‚
â”‚                    â”‚  Cache    â”‚           â”‚ KV cache mgmt     â”‚â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Design Decisions

**1. Model Serving Infrastructure**

| Option | Pros | Cons | Best For |
|--------|------|------|----------|
| **Managed (Vertex AI / SageMaker)** | Zero infra management, auto-scaling, built-in monitoring | Less optimization control, vendor lock-in, higher costs at scale | Startups, rapid prototyping, small ops teams |
| **Self-hosted (vLLM / TensorRT-LLM)** | Full control, better cost efficiency at scale, customizable | Requires ML infra expertise, GPU management complexity | High volume (millions/day), cost-sensitive |

**2. Continuous Batching**

**Problem**: Static batching wastes GPU when requests finish at different times.

**Why this happens**: LLM generation is sequential (token-by-token), so requests in a batch finish at different times. With static batching, the GPU waits for the slowest request before processing the next batch.

**Solution**: Dynamic batchingâ€”add new requests to batch as others complete.

```
Time 0: [Request A (100 tokens)]
Time 1: [Request A (50 tokens), Request B (100 tokens)] â† Added B
Time 2: [Request B (50 tokens), Request C (100 tokens)] â† A finished, added C
Time 3: [Request C (50 tokens), Request D (100 tokens)] â† B finished, added D
```

**Benefit**: 2-3x higher throughput because GPU utilization increases from ~40% to ~85%.

**Aha:** With static batching, one long answer blocks the whole batch. Continuous batching **refills** the batch as soon as any request completes, so the GPU rarely idles. The "aha" is: treat the batch as a **queue**, not a fixed group.

**3. KV Cache Management**

**What**: Store the **Key** and **Value** matrices produced by each attention head so they are not recomputed. In standard attention, the score matrix has shape `[batch, heads, sequence_length, sequence_length]`; each new token would require recomputing scores over all previous tokens.

**Why KV cache is needed**: Autoregressive decoding feeds all prior tokens into the next step. Without caching, every generation step recomputes keys and values for the entire prefix, giving O(nÂ²) work per token. Caching lets you compute K and V only for the new token and reuse the rest, reducing to O(n) per token. Reported speedups from KV caching are on the order of ~30â€“40% in standard implementations.

**How it works**: For each new token, compute and store its K and V; look up cached K/V for all previous positions when computing attention. Only the new tokenâ€™s key/value are written each step.

**Challenge**: Cache size grows linearly with sequence length (and with layers Ã— heads Ã— head_dim). For a 32-layer model with 768-dim embeddings, each token can use on the order of ~50KB of cache; a 2K-token sequence can need ~100MB of KV cache. Long contexts and many concurrent requests make this the main memory bottleneck.

**Solution â€” PagedAttention (vLLM)**: Inspired by OS virtual memory and paging. The KV cache is split into **fixed-size blocks** and stored in non-contiguous memory. That reduces fragmentation and allows sharing (e.g. shared prompt prefix across requests). vLLM reports near-zero wasted KV memory and roughly **2â€“4Ã— throughput** versus non-paged systems on long sequences and large models.

**5. Speculative Decoding**

**Problem**: Token-by-token autoregressive generation is slow because each new token requires a full forward pass of the large model.

**Solution**: A small **draft** model proposes several candidate tokens in a row. The **target** (large) model does a single forward pass over the whole candidate sequence and accepts tokens that match its predictions; the first mismatch stops the run and the rest are discarded. Accepted tokens advance the sequence without extra target-model steps. Typical reported speedups are **2â€“2.5Ã—**; variants (multiple draft models, tree-based decoding) can reach ~3â€“4Ã— or more at the cost of extra memory and complexity.

| Technique | Speedup | Trade-off |
|-----------|---------|-----------|
| **Standard Speculative** | 2â€“2.5Ã— (often up to ~3Ã—) | Needs a separate draft model |
| **Self-Speculative** | ~2.5Ã— | Uses smaller/quantized version of same model |
| **Tree-based** | Up to ~4â€“6Ã— | More memory and logic for tree search |

**Why it works**: The target model verifies **N** candidates in one forward pass (over a sequence of length N). That cost is similar to generating a single token, so you effectively get several tokens per large-model step when the draft is accurate. **Draft latency** (how fast the draft runs) usually matters more for end-to-end speedup than the draftâ€™s raw language quality.

**4. Caching Strategy**

| Strategy | Hit Rate | Latency | Best For |
|----------|----------|---------|----------|
| **Prompt caching** | High for system prompts | 2-5x speedup | Common prefixes, few-shot examples |
| **Response caching** | 10-30% | Instant | Identical requests |
| **Semantic caching** | 30-50% | +5-10ms overhead | Paraphrased queries |

---

## 2. RAG (Retrieval-Augmented Generation) System

### Use Case: Design a Document Q&A System

**Requirements:**
- Answer questions from 1M documents
- Support real-time queries (< 3 seconds)
- Handle 1,000 **QPS** (queries per second)
- Ensure factual accuracy (grounding)

**High-Level Design:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      RAG ARCHITECTURE                           â”‚
â”‚                                                                 â”‚
â”‚   INGESTION PIPELINE                                            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚Documents â”‚â”€â”€â–ºâ”‚ Chunking â”‚â”€â”€â–ºâ”‚Embedding â”‚â”€â”€â–ºâ”‚ Vector   â”‚   â”‚
â”‚   â”‚          â”‚   â”‚          â”‚   â”‚  Model   â”‚   â”‚   DB     â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚   QUERY PIPELINE                                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  Query   â”‚â”€â”€â–ºâ”‚ Embed    â”‚â”€â”€â–ºâ”‚Similarityâ”‚â”€â”€â–ºâ”‚ Top-K    â”‚   â”‚
â”‚   â”‚          â”‚   â”‚  Query   â”‚   â”‚  Search  â”‚   â”‚  Docs    â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                      â”‚         â”‚
â”‚                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚                                              â”‚   Reranker    â”‚ â”‚
â”‚                                              â”‚  (optional)   â”‚ â”‚
â”‚                                              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                      â”‚         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚   â”‚                        LLM                                â”‚ â”‚
â”‚   â”‚   Query + Retrieved Context â†’ Generated Answer           â”‚ â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Aha:** RAG doesn't cram everything into the model's weights. It keeps the LLM general and **fetches** relevant knowledge at query time. That gives you updatable knowledge, smaller models, and citationsâ€”but you must design retrieval and chunking well or the model "makes it up."

### Key Components

**1. Document Ingestion Pipeline**

| Service | Google Cloud | AWS |
|---------|--------------|-----|
| RAG Engine | Vertex AI RAG Engine | Bedrock Knowledge Bases |
| Vector Search | Vertex AI Vector Search | OpenSearch Serverless |
| Processing | Dataflow | Glue/EMR |

**2. Vector Database Options**

- **Managed**: Vertex AI Vector Search, Amazon OpenSearch
- **Self-hosted**: Pinecone, Weaviate, Qdrant, Milvus

**3. Embedding Models**

- **Google**: text-embedding-004 (Vertex AI)
- **AWS**: Amazon Titan Embeddings (Bedrock)
- **Open Source**: sentence-transformers, **BGE** (BAAI General Embeddings)â€”embedding models from BAAI (Beijing Academy of Artificial Intelligence), e.g. bge-base, BGE-M3 for multilingual

### Chunking Strategy Trade-offs

| Strategy | Pros | Cons | Best For |
|----------|------|------|----------|
| **Fixed-size (512 tokens)** | Simple, predictable | May split concepts | Uniform documents |
| **Semantic chunking** | Preserves coherence | Complex, variable sizes | Complex content |
| **Hybrid (fixed + overlap)** | Balanced | More storage | Most production systems |

**Why chunking matters**: LLMs have context windows. Documents often exceed this, so we must break them into chunks. Smaller chunks improve retrieval precisionâ€”a query about "Python loops" matches better to a 500-token chunk about loops than a 5000-token document about Python.

**Aha:** Chunk size is a **precision vs context** trade-off. Too small â†’ you retrieve the right idea but maybe miss surrounding explanation. Too large â†’ you get more context but dilute relevance. Overlap and semantic boundaries help keep "one concept per chunk."

### Retrieval Strategy Trade-offs

| Strategy | Latency | Semantic | Keywords | Best For |
|----------|---------|----------|----------|----------|
| **Dense (Vector)** | 10-50ms | âœ“ | âœ— | Conceptual queries |
| **Sparse (BM25)** | 1-5ms | âœ— | âœ“ | Exact matches |
| **Hybrid** | 15-60ms | âœ“ | âœ“ | Production (recommended) |

**BM25** = keyword-based ranking using term frequency and inverse document frequency; no embeddings, just lexical match.

**Why hybrid works**: Dense retrieval captures meaning ("iterate" â‰ˆ "loop"), sparse captures exact keywords ("Python"). Combining both via **RRF (Reciprocal Rank Fusion)** gives best results.

**Aha:** **Dense** = "these two *mean* the same thing" (embedding similarity). **Sparse** = "these two *contain* the same words" (e.g. BM25). Queries need both: "how do I loop in Python?" benefits from semantic match on "loop" and exact match on "Python." Hybrid + RRF merges the two rank lists without a single embedding doing everything.

### Reranking Trade-offs

**No Reranking**: Lower latency, simpler pipeline, but lower quality.

**Cross-Encoder Reranking**: Much higher accuracy because it processes query-document pairs together (sees interactions), but adds ~10ms per document.

**Best practice**: Retrieve K=20, rerank to top 5. The two-stage approach combines speed (bi-encoder retrieval) with accuracy (cross-encoder reranking).

**Aha:** **Bi-encoder** = query and doc are embedded *separately*; similarity is dot product. Fast (one pass each) but the model never sees "query + doc together." **Cross-encoder** = one forward pass with "[query] [doc]"; the model sees the *pair* and scores relevance directly. Slower, but much more accurate. So: retrieve broadly with bi-encoder, then rerank the top K with a cross-encoder.

### Advanced RAG Techniques

These techniques improve retrieval when plain â€œembed query â†’ topâ€‘k chunksâ€ is not enough: when answers span multiple hops, when queries vary in difficulty, or when user wording doesnâ€™t match document wording.

---

**1. Graph RAG**

**What it is:** You build a **knowledge graph** from your corpus (entities as nodes, relations as edges) and combine it with vector search. Retrieval can follow *links* (e.g. â€œthis person â†’ worked at â†’ this companyâ€) as well as semantic similarity.

**How it helps:** Many questions need **multi-hop** reasoning: â€œWho was the CEO of the company that acquired X?â€ requires (X â†’ acquired by â†’ company â†’ CEO â†’ person). Flat vector search often returns only one hop. Graph RAG retrieves **subgraphs** (e.g. k-hop neighborhoods) so the LLM sees not just similar text but explicit *whoâ€“whatâ€“where* structure.

**When to use:** Strong fit for domains rich in **entities and relations** (people, orgs, products, events) and questions that chain them. Overkill for unstructured long-form text with few named relations.

**Aha:** Vector search answers â€œwhat text is similar?â€ Graph RAG adds â€œhow are these things *connected*?â€ so the model can follow paths, not only similarity.

---

**2. Adaptive Retrieval**

**What it is:** Instead of always retrieving the same number of documents (e.g. k=10), you **change k per query**. Simple factoid questions get fewer docs; broad or multi-fact questions get more.

**How it helps:** With a **fixed k**, easy questions get unnecessary context (wasted tokens, more noise) and hard questions may get too few (missing evidence). Adaptive retrieval uses a small classifier, heuristics (e.g. query length, question type), or the **shape of similarity scores** (e.g. â€œbiggest dropâ€ between consecutive docs) to choose k. Some methods need no extra modelâ€”e.g. set k at the largest score gap in the ranked list.

**When to use:** When your traffic mixes **simple lookups** and **complex / multi-document** questions. Saves tokens and latency on easy queries and improves recall on hard ones.

**Aha:** One size doesnâ€™t fit all: â€œWhat is the capital of France?â€ needs 1â€“2 chunks; â€œCompare the economic policies of France and Germany in the 1980sâ€ needs many. Adaptive k tunes retrieval to each question.

---

**3. Query Decomposition**

**What it is:** Before retrieval, an LLM **splits** the user question into 2â€“5 **sub-questions** that are answered by different parts of the corpus. You run retrieval once per sub-question, then merge and deduplicate the chunks and pass that combined context to the final answer model.

**How it helps:** Questions like â€œHow does X differ from Y?â€ or â€œWhich of A, B, C had the highest Z?â€ donâ€™t match one passageâ€”they need **several**. One query embedding often misses some of them. Decomposing into â€œWhat is X?â€, â€œWhat is Y?â€, â€œHow do they differ?â€ (or â€œWhat is Z for A?â€, â€œWhat is Z for B?â€, â€¦) yields focused sub-queries and better coverage.

**When to use:** **Multi-part** or **comparison** questions, and whenever a single embedding tends to retrieve only one â€œsideâ€ of the answer. Adds latency (one LLM call to decompose, then multiple retrievals) but can significantly improve accuracy.

**Aha:** One query â†’ one vector â†’ one retrieval set often undersamples. Decomposing â€œHow does A differ from B?â€ into â€œWhat is A?â€ and â€œWhat is B?â€ (and optionally â€œHow do they differ?â€) pulls in the right evidence for each piece, then the model synthesizes.

---

**4. HyDE (Hypothetical Document Embeddings)**

**What it is:** You **donâ€™t** embed the user query directly. Instead, you ask an LLM: â€œWrite a short passage that would answer this question.â€ You get 1â€“5 such **hypothetical** passages, embed *those*, and (often) **average** their vectors. That single vector is used to search the real document index.

**How it helps:** Query and documents often use **different words** for the same idea (e.g. user: â€œloop,â€ docs: â€œiteration constructâ€). The query embedding can sit in a different region of the embedding space than the best-matching docs. Hypothetical answers â€œtranslateâ€ the question into **passage-like** text, so their embeddings sit closer to real relevant passages. Averaging smooths noise from any one generation.

**When to use:** When **vocabulary mismatch** hurts recall (e.g. lay users vs technical docs, or one language vs translated corpus) and when you can afford one extra LLM call before retrieval. Less useful when queries already look like document sentences.

**Aha:** Youâ€™re searching with â€œwhat an answer would look likeâ€ instead of â€œwhat the question looks like.â€ The hypothetical doc is in the same â€œlanguageâ€ as your corpus, so similarity search works better.

---

**Quick reference**

| Technique | Main idea | Best for |
|-----------|-----------|----------|
| **Graph RAG** | Vector search + graph structure (entities, relations); retrieve subgraphs for multi-hop | Entity-heavy domains, â€œwho/what/whereâ€ chains |
| **Adaptive Retrieval** | Vary number of retrieved docs (k) by query complexity | Mix of simple and complex questions |
| **Query Decomposition** | Split question into sub-questions; retrieve per sub-question; merge context | Multi-part, comparison, â€œA vs Bâ€ style questions |
| **HyDE** | Generate hypothetical answer(s), embed those, search with that vector | Vocabulary mismatch between user and corpus |

---

## 3. RAG vs Fine-Tuning Decision Framework

**Key insight:** This is not a binary choice. Think of it as a **spectrum of adaptation**: RAG and fine-tuning solve different problems and are often used **together**. The right question is not "RAG or fine-tuning?" but "What does the model lackâ€”**knowledge** or **behavior**?"

- **"The model doesn't *know* X"** â†’ Add knowledge via RAG (or long context, or caching).
- **"The model doesn't *behave* like Y"** â†’ Change behavior via fine-tuning (tone, format, schema, jargon).
- **"We need both fresh facts and consistent style"** â†’ Use both: RAG for what to say, fine-tuning for how to say it.

---

### When to Use RAG

**What RAG fixes:** Gaps in **knowledge** and **freshness**. The model is good at reasoning and language but hasn't seen your data (policies, tickets, docs, logs). RAG injects that at query time: you retrieve relevant chunks and put them in the prompt, so the model "reads" your corpus on demand.

**Use RAG when:** The model **lacks knowledge** about your domain (e.g. internal docs, product specs, support history). Your **data changes often** (e.g. daily reports, new releases, tickets)â€”RAG lets you update the index without retraining. You want to **reduce hallucinations** by **grounding** answers in retrieved text and to **cite sources** (chunk or doc IDs).

**RAG does *not* fix:** Tone, format, or jargon. If the base model is too informal or ignores your schema, RAG alone won't change thatâ€”you need behavior change (prompts or fine-tuning).

---

### When to Use Fine-Tuning

**What fine-tuning fixes:** **Behavior** and **style**. The model "knows" enough from pretraining, but its outputs don't match how you want it to answer: tone (formal vs casual), structure (e.g. JSON with fixed keys), or vocabulary (your domain terms). Fine-tuning adjusts the model's weights so it reliably produces that style.

**Use fine-tuning when:** You need a **specific tone or voice** (e.g. brand guidelines, compliance-friendly wording). You need **strict output format** (e.g. JSON, bullet lists, section headings)â€”fine-tuning helps the model adhere to schemas. The model **misuses or avoids domain jargon**; training on in-domain examples teaches it to use your terms correctly.

**Fine-tuning does *not* fix:** Missing or outdated facts. Weights are fixed until the next train run. For fast-changing knowledge, use RAG (or both).

---

### When to Use Both

**Use RAG + fine-tuning when** you need **accurate, up-to-date content** *and* **consistent presentation**: RAG supplies the **facts** (from docs, KB, logs); fine-tuning shapes **how** those facts are expressed (tone, format, terminology). Example: A support bot that answers from your knowledge base (RAG) but must always respond in a compliant, on-brand style (fine-tuned). Or a report generator that pulls from live data (RAG) and always outputs the same JSON schema (fine-tuned).

---

### Scenario Cheat Sheet

| Scenario | RAG | Fine-Tuning | Both |
|----------|:---:|:-----------:|:----:|
| Model lacks knowledge about your domain | âœ… | âŒ | |
| Data changes frequently (docs, tickets, metrics) | âœ… | âŒ | |
| Need specific tone, style, or brand voice | âŒ | âœ… | |
| Domain-specific jargon or terminology | âŒ | âœ… | |
| Reduce hallucinations by grounding in retrieved text | âœ… | | |
| Change output format or schema (e.g. JSON, sections) | âŒ | âœ… | |
| High accuracy *and* fresh data *and* consistent style | | | âœ… |

### Cost Comparison

Cost structure is different, not just "cheaper vs more expensive":

| Approach | Cost model | What you pay for | Example ballpark |
|----------|------------|------------------|------------------|
| **RAG** | **Per query** | Retrieval (embeddings, vector search) + LLM tokens (context + answer) | ~$0.01-0.05 per query; 1M queries/month â‰ˆ $10-50K |
| **Fine-tuning (e.g. LoRA)** | **One-time** | Training compute + data prep; then inference cost as usual | ~$500-2,000 for **LoRA** (Low-Rank Adaptation) on 7-70B model; amortizes over all future requests |
| **Full fine-tune** | **One-time, large** | Full training run on your data | $10K-100K+ depending on model size and data |

**How to think about it:** RAG cost grows with **usage** (every query pays). Fine-tuning cost is **upfront**; after that, marginal cost per request is similar to the base model (or lower if you use a smaller fine-tuned model). Break-even depends on volume: at very high QPS, RAG can exceed the amortized cost of a one-time fine-tune; at low QPS, RAG is often cheaper than investing in fine-tuning.

### Decision Flow

Start with the **cheapest, fastest** lever (prompts and few-shot examples). Only add RAG or fine-tuning when you've identified a clear gap: knowledge vs behavior.

```
Start with: System prompt + few-shot examples
        â”‚
        â–¼
Does the model lack KNOWLEDGE about your domain?
(e.g. your docs, products, policies, tickets)
        â”‚
    Yes â”€â”´â”€ No
        â”‚     â”‚
        â–¼     â–¼
   Add RAG   Does the model need BEHAVIOR change?
            (e.g. tone, format, schema, jargon)
                    â”‚
               Yes â”€â”´â”€ No
                    â”‚     â”‚
                    â–¼     â–¼
            Fine-tune   Done
```

You can **add RAG and then fine-tune** (or the reverse) if you need both knowledge and behavior. Many production systems use prompts + RAG + fine-tuning together.

---

### Best Practice

1. **Start simple:** Prompt engineering + a few examples. Ship and measure.
2. **Add RAG** when the main gap is "model doesn't know our content" or "content changes often."
3. **Add fine-tuning** when the main gap is "model doesn't answer in our tone/format/terms."
4. **Combine** when you need both correct, up-to-date content and consistent presentation.

**Aha:** RAG = **external memory** you can change without retraining (add docs, edit, delete). Fine-tuning = **internalized behavior** (tone, format, jargon) thatâ€™s fixed until the next train run. Use RAG when the world changes; use fine-tuning when you want the model itself to change how it answers.

---

## 4. Agentic AI Systems

### What Is an Agent? Why Do We Need One?

**Definition:** An **agent** is an LLM that **repeatedly** decides, acts, and observes until a task is done. It has access to **tools** (APIs, databases, search, code) and runs in a **loop**: perceive the current state â†’ decide the next step â†’ call a tool â†’ observe the result â†’ repeat. That loop is what makes it an agent, not "one prompt â†’ one answer."

**Why we need agents:** A single LLM call is stateless and one-shot. It can't look up live data, call your CRM, or run multi-step workflows. **RAG** adds retrieval at query time but still produces one answer from one retrieved contextâ€”no tool calls, no iterative refinement. **Agents** add the ability to *use the world*: query systems, run code, search, then decide what to do next from the results. So you need an agent when the task requires **multiple steps**, **live data** (orders, DB, APIs), or **decisions that depend on tool outputs** (e.g. "if order status is X, do Y").

**When to use agents vs. not:**

| Use an agent whenâ€¦ | Use a single call or RAG whenâ€¦ |
|-------------------|-------------------------------|
| The task needs **multiple tool calls** or steps (e.g. check order â†’ update CRM â†’ create ticket) | The task is **one question â†’ one answer** (e.g. "what is our return policy?") |
| The **next step depends on live results** (e.g. "if refund approved, thenâ€¦") | The pipeline is **fixed** (e.g. embed query â†’ retrieve â†’ generate) |
| You need **orchestration across systems** (APIs, DBs, search) | You only need **retrieval + generation** (RAG) or pure generation |
| Decisions are **context-sensitive** and hard to encode as rules | The flow is **deterministic** and easy to script |

**Aha:** Start with the simplest thing that works (single call, or RAG). Add an agent only when you need **loop + tools**â€”when the model must *use* external systems and *iterate* based on what it sees.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            SINGLE CALL / RAG vs AGENT                                        â”‚
â”‚                                                                              â”‚
â”‚   SINGLE CALL or RAG                    AGENT                               â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”€â”€â”€â”€â”€                               â”‚
â”‚   User â†’ Prompt (+ RAG?) â†’ LLM â†’ Answer  User â†’ Prompt â†’ LLM â†’ Thought       â”‚
â”‚   (one shot)                                  â”‚                              â”‚
â”‚                                         Tool call â†’ Observation â†’ (repeat)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Use Case: Design a Customer Support Agent

**Requirements:**
- Handle customer inquiries autonomously
- Access multiple tools (CRM, knowledge base, order system)
- Support multi-turn conversations
- Escalate to human when needed
- Handle 10,000 conversations/day

**Why an agent fits here:** Support often needs *multi-step* actions (look up order â†’ check policy â†’ create ticket or escalate) and *live data* (order status, account history). One LLM call or RAG-only can't do that; you need a loop + tools.

**High-Level Design:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AGENTIC AI ARCHITECTURE                      â”‚
â”‚                                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚   â”‚                  USER MESSAGE                             â”‚ â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                            â”‚                                    â”‚
â”‚                            â–¼                                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚   â”‚              AGENT ORCHESTRATOR (LLM)                     â”‚ â”‚
â”‚   â”‚                                                           â”‚ â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚ â”‚
â”‚   â”‚   â”‚  REASONING  â”‚â”€â–ºâ”‚   ACTING    â”‚â”€â–ºâ”‚ OBSERVATION â”‚     â”‚ â”‚
â”‚   â”‚   â”‚  (Analyze)  â”‚  â”‚(Tool call)  â”‚  â”‚  (Result)   â”‚     â”‚ â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â”‚ â”‚
â”‚   â”‚                            â–²                â”‚             â”‚ â”‚
â”‚   â”‚                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚ â”‚
â”‚   â”‚                         (Iterate until done)              â”‚ â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                            â”‚                                    â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚         â”‚                  â”‚                  â”‚                â”‚
â”‚         â–¼                  â–¼                  â–¼                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚   â”‚  Tool 1   â”‚      â”‚  Tool 2   â”‚      â”‚  Tool 3   â”‚         â”‚
â”‚   â”‚ Knowledge â”‚      â”‚  Order    â”‚      â”‚  Create   â”‚         â”‚
â”‚   â”‚   Base    â”‚      â”‚  Status   â”‚      â”‚  Ticket   â”‚         â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Aha:** An agent is an LLM in a **loop** with tools. The model doesnâ€™t just answer once; it *reasons â†’ acts (calls a tool) â†’ observes (gets result) â†’ reasons again* until it can respond. That turns the LLM into a controller over APIs, DBs, and searchâ€”so the "aha" is: the value is in the **loop + tools**, not in a bigger model.

### Agent Frameworks

Choose **no-code** (Vertex AI Agent Builder, Bedrock Agents) when you want to configure agents in a UI with minimal code. Choose **programmatic** (ADK, LangChain, LlamaIndex) when you need custom logic, complex workflows, or fine-grained control.

| Platform | Google Cloud | AWS | Open Source |
|----------|--------------|-----|-------------|
| No-code | Vertex AI Agent Builder | Bedrock Agents | - |
| Programmatic | Agent Development Kit (ADK) | AgentCore | LangChain, LlamaIndex, AutoGen |

---

### Tool Types

**Tools** are how the agent interacts with the world: APIs, DBs, search, code. The agent chooses *which* tool to call and *with what arguments*; the tool runs and returns a result, which the agent uses for the next step.

| Tool Type | Execution | Description | Best For |
|-----------|-----------|-------------|----------|
| **Extensions (APIs)** | Agent-side | Standardized bridges to external APIs | Multi-service access |
| **Function Calling** | Client-side | Model outputs function name + args; your app executes | Security, audit, human-in-loop |
| **Data Stores** | Agent-side | Connect to vector DBs, knowledge bases | RAG, real-time info |
| **Plugins** | Agent-side | Pre-built integrations (calendar, CRM) | Rapid capability addition |

**Aha:** **Function calling** (client-side) gives you control: the model outputs a tool name + args, and *your app* decides whether to run it. Use it when you need security, audit, or human-in-the-loop. **Agent-side** tools run automatically when the model requests themâ€”faster but less control.

---

### Agent Protocols: MCP and A2A

**MCP (Model Context Protocol)** and **A2A (Agent-to-Agent / Agent2Agent)** are open standards that define how agents get **tools and context** (MCP) and how **agents talk to other agents** (A2A). Both matter when you build multi-tool or multi-agent systems.

**MCP (Model Context Protocol)**

**MCP** is an open protocol (Anthropic, 2024) that standardizes how applications provide **tools and context** to LLMs. It acts as a universal connector: an LLM or agent connects to **MCP servers**, which expose tools, prompts, and resources (files, DBs, APIs) in a consistent way. So instead of each vendor defining its own tool format, you run or connect to MCP servers and the model gets a uniform interface.

| Aspect | Description |
|--------|-------------|
| **Purpose** | Standardize how models get tools, prompts, and resources from external systems |
| **Adoption** | Anthropic (Claude), OpenAI (Agents SDK), Microsoft (Agent Framework) |
| **Use cases** | AI-powered IDEs, custom workflows, connecting agents to Slack, Figma, databases, etc. |

**When it matters:** Use MCP when you want **portable tooling**â€”the same MCP server can back multiple agents or products. It also helps when you integrate many external systems (CRMs, docs, search) without writing custom glue per vendor.

**A2A (Agent-to-Agent / Agent2Agent Protocol)**

**A2A** is an open standard (Google, 2025) for **communication and collaboration between AI agents** built by different vendors and frameworks. It addresses interoperability: agents from different stacks (e.g. Vertex AI, LangChain, Salesforce) can discover each other, negotiate UX, and exchange tasks and state **without** sharing internal memory, resources, or tools.

| Aspect | Description |
|--------|-------------|
| **Purpose** | Enable agent-to-agent collaboration across vendors and frameworks |
| **Mechanisms** | **Agent Cards** (JSON metadata: identity, capabilities), capability discovery, task/state management, UX negotiation |
| **Transport** | JSON-RPC 2.0 over HTTP(S) |
| **Relationship to MCP** | A2A handles **agent â†” agent**; MCP handles **model â†” tools/context**. They complement each other. |

**When it matters:** Use A2A when you run **multi-agent** or **cross-vendor** workflows (e.g. your agent hands off to a partnerâ€™s agent, or you compose agents from different platforms). It gives you a shared protocol for discovery, tasks, and security instead of one-off integrations.

**Aha:** **MCP** = â€œhow does *this* agent get its tools and context?â€ **A2A** = â€œhow do *multiple* agents from different systems work together?â€ For a single agent with your own tools, MCP is the standard to consider. For agent-to-agent orchestration across products or vendors, A2A is the standard to consider.

---

### Reasoning Frameworks

**Chain-of-Thought (CoT):** The model generates **intermediate reasoning steps** ("think step-by-step") before the final answer. No tool useâ€”just internal logic. Use when you need interpretability or multi-step reasoning without external data.

**ReAct (Reason + Act):** Combines **reasoning** with **tool use** in a loop. Each turn is either a *Thought* (what to do next), an *Action* (tool name + args), or an *Observation* (tool result). The model keeps going until it can give a final answer.

| Phase | What Happens |
|-------|--------------|
| **1. Reasoning** | Agent analyzes task, selects tools |
| **2. Acting** | Agent executes selected tool |
| **3. Observation** | Agent receives tool output |
| **4. Repeat** | Agent reasons from the observation, then next Thought/Action or final answer |

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ReAct LOOP (example)                           â”‚
â”‚   User: "What's the status of order #123? Can I get a refund?"   â”‚
â”‚      Thought: I need to look up order #123 first.                 â”‚
â”‚      Action: get_order_status(order_id="123")                    â”‚
â”‚      Observation: { "status": "delivered", "date": "2024-01-15" }â”‚
â”‚      Thought: Delivered. User asked about refund. Check policy.   â”‚
â”‚      Action: search_knowledge_base(query="refund policy")         â”‚
â”‚      Observation: "Refunds within 30 days of delivery..."         â”‚
â”‚      Thought: I have enough. Compose answer.                      â”‚
â”‚      Answer: "Order #123 was delivered Jan 15. Our policy..."     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Aha:** ReAct makes the reasoning **visible** (Thought) and **grounded** (Action â†’ Observation). The model canâ€™t wander off; each step is either "I thinkâ€¦" or "I do X" followed by real tool output. That reduces hallucination in tool use because the next thought is conditioned on actual observations.

### Agent Design Patterns

**When to use which:** Start with **Single Agent** (one LLM + all tools). Add **Multi-Agent** or **Hierarchical** when one agent can't handle the diversity of tasks or when you want specialists (e.g. research vs writing vs coding) or clearer separation of concerns.

---

**1. Single Agent Pattern**

One LLM handles the entire conversation and has access to all tools. The model decides when to call which tool.

```
   User â”€â”€â–º LLM (orchestrator) â”€â”€â–º Tool A, Tool B, Tool C
              â–²         â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  (loop until done)
```

- âœ… Simple, low latency, easy to debug
- âŒ Limited capabilities, may struggle with very complex or diverse tasks
- *Best for*: Simple use cases, single domain (e.g. support bot with KB + CRM + ticketing)

---

**2. Multi-Agent Pattern**

Multiple specialized agents, each with its own tools. **There is no single "boss."** Agents can **hand off** to each other (e.g. Agent A finishes and passes to B), **work in parallel** (A, B, C run at once and someone aggregates), or **negotiate** who does what. Control and flow are **distributed**â€”each agent or a lightweight router decides the next step, not one central planner.

```
   User â”€â”€â–º [Agent A] â†â”€â”€â–º [Agent B] â†â”€â”€â–º [Agent C] â”€â”€â–º combined result
              â”‚               â”‚               â”‚
           Tools A        Tools B        Tools C
        (peer-to-peer handoffs or parallel, then aggregate)
```

- âœ… Specialists, parallel execution, modular, flexible routing
- âŒ Coordination logic lives in handoffs/aggregation; can be harder to reason about
- *Best for*: Domains where agents **collaborate as peers** (e.g. research agent + writing agent + fact-check agent that hand off or run in parallel; no one agent "owns" the plan)

---

**3. Hierarchical Pattern (Supervisor/Manager)**

**One supervisor** agent receives the user request, **owns the plan**, and **delegates** to specialist agents. Specialists do the work and **report back only to the supervisor**; they do **not** talk to each other. The supervisor decides the next step, assigns it, waits for the result, then repeats or synthesizes the final answer. Control and flow are **centralized** in the supervisor.

```
   User â”€â”€â–º Supervisor (LLM) â”€â”€â–º "Do step 1" â”€â”€â–º Specialist A â”€â”€â–º result â”€â”€â–º Supervisor
                    â”‚
                    â”œâ”€â”€â–º "Do step 2" â”€â”€â–º Specialist B â”€â”€â–º result â”€â”€â–º Supervisor
                    â”‚
                    â””â”€â”€â–º synthesize â”€â”€â–º Answer
```

- âœ… Clear ownership of the plan, easier to debug and reason about, scalable workflow
- âŒ Supervisor is a bottleneck; more latency than flat handoffs when steps are independent
- *Best for*: Workflows with a **fixed or predictable sequence** (e.g. research â†’ draft â†’ review â†’ publish) where one "conductor" should own the plan

---

**Multi-Agent vs Hierarchical: Clear distinction**

| Aspect | Multi-Agent | Hierarchical |
|--------|-------------|--------------|
| **Who decides the plan?** | Distributed: agents hand off, or a router chooses; no single owner | **One supervisor** owns the plan and assigns steps |
| **Who do specialists talk to?** | Each other (handoffs) or an aggregator; flow is peer-to-peer or fan-out | **Only the supervisor**; specialists do not talk to each other |
| **Control shape** | **Flat** or **peer-to-peer**: many agents, shared or emergent coordination | **Tree**: one node (supervisor) at the top, specialists as children |
| **Flow** | Emergent (handoffs, parallel, negotiate) | **Top-down**: Supervisor â†’ assign step â†’ Specialist â†’ result â†’ Supervisor |
| **When to use** | You want **peers** that hand off or run in parallel and someone (or the group) aggregates | You want **one conductor** that plans and delegates in sequence or in a clear DAG |

**Aha:** **Multi-agent** = "several agents, no single boss; they hand off or run in parallel." **Hierarchical** = "one boss (supervisor) that assigns tasks to specialists and gets results back; specialists donâ€™t talk to each other." Use multi-agent when control should be shared or emergent; use hierarchical when one agent should own the plan and delegate.

---

**4. Additional Patterns**

Beyond single-, multi-, and hierarchical agents, three common *orchestration shapes* show up in production: stages in a fixed order, independent experts run in parallel, and adversarial roles that argue before a judge. Use these when the task has a natural flow (sequence), benefits from multiple viewpoints (fan-out), or must be stress-tested (debate).

---

**1. Sequential Pipeline**

**What it is:** A fixed chain of steps, A â†’ B â†’ C. Each stage consumes the prior stage's output and produces input for the next. No parallelism within the pipeline; order is part of the design (e.g. outline before draft, draft before edit).

**How it works:** One agent or model run handles each step. Outputs are passed as context or artifacts to the next. Handoffs are explicit (e.g. "outline," "draft," "edited_draft"). Failures or rewinds usually mean restarting from the failing step or the beginning, depending on your design.

**When to use:** **Content creation** (outline â†’ draft â†’ edit), **ETL-style** flows (extract â†’ transform â†’ load), or any process where step N truly depends on step Nâˆ’1 and there's no benefit from running steps in parallel.

```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚Outline  â”‚ â”€â”€â–¶ â”‚ Draft   â”‚ â”€â”€â–¶ â”‚ Edit    â”‚ â”€â”€â–¶ output
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       A               B               C
```

---

**2. Parallel Fan-out**

**What it is:** One query (or task) is sent to **multiple agents or tools** at once; each runs independently. A separate **aggregator** (or router) collects their outputs and merges them into one answer or decision.

**How it works:** Fan-out: duplicate the request to A, B, C (and optionally more). No agent waits on another during the parallel phase. Aggregate: combine results via another LLM call (e.g. "synthesize these three analyses") or a rule (e.g. majority vote, weighted average). Latency is dominated by the slowest branch plus aggregation, not the sum of all branches.

**When to use:** **Research** or **multi-perspective analysis** (e.g. legal, market, technical views in parallel), **ensemble** answers (e.g. multiple retrieval strategies or models), or whenever you want **diversity** then **reconciliation** in one round.

```
       Query
          â”‚
    â”Œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”
    â–¼     â–¼     â–¼
  â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”
  â”‚ A â”‚ â”‚ B â”‚ â”‚ C â”‚   (parallel)
  â””â”€â”¬â”€â”˜ â””â”€â”¬â”€â”˜ â””â”€â”¬â”€â”˜
    â””â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”˜
          â–¼
     Aggregate â”€â”€â–¶ final answer
```

---

**3. Debate / Adversarial**

**What it is:** Two (or more) **adversarial roles** argue opposite sides (e.g. Pro vs Con, attacker vs defender). A **judge** (or meta-agent) reads the debate and produces the final decision or output. The goal is to surface objections and reduce overconfidence.

**How it works:** Pro and Con (or Red / Blue) each get the same task and constraints; they may see each other's replies in one or more rounds. The judge receives the full transcript and possibly the original query, then outputs the chosen stance, a synthesis, or a "no decision" with reasons. You can cap rounds (e.g. 1â€“2) to control cost and latency.

**When to use:** **High-stakes decisions** (e.g. approvals, audits, policy), **red teaming** (stress-test an idea or policy before release), or when you want the system to **explicitly consider counterarguments** instead of one-shot answers.

```
  â”Œâ”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”
  â”‚ Pro â”‚ â”€â”€â”€â”€ argue â”€â”€â”€â”€â”€â”€â”€â–¶â”‚Judgeâ”‚
  â””â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”¬â”€â”€â”˜
       â–²                        â”‚
       â””â”€â”€ argue â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”Œâ”€â”€â”€â”€â”€â”
  â”‚ Con â”‚
  â””â”€â”€â”€â”€â”€â”˜
```

---

**Quick reference**

| Pattern | Architecture | Use Case |
|---------|--------------|----------|
| **Sequential Pipeline** | A â†’ B â†’ C (fixed order) | Content creation (outline â†’ draft â†’ edit), ETL-style flows |
| **Parallel Fan-out** | Query â†’ [A, B, C] â†’ Aggregate | Research, multi-perspective analysis, ensembles |
| **Debate/Adversarial** | Pro vs Con â†’ Judge | High-stakes decisions, red teaming, counterargument stress-test |

**Aha:** Single agent = one brain, many tools. Multi-agent = many brains, each with its own tools; you need handoffs. Hierarchical = one brain that delegates; specialists don't talk to each other directly.

### Context Engineering

**The Problem**: As agents run longer, context (chat history, tool outputs, documents) **explodes**. Simply using larger context windows is not a scaling strategy.

**Aha:** More context isnâ€™t always better. Models often **underuse** the middle of long prompts ("lost in the middle"). So putting the most important instructions or retrieval at the **start and end** of the context, and keeping working context small and focused, improves both quality and cost. Tiered context (working / session / memory / artifacts) is how you scale *usage* of context without scaling *size* of every call.

**The Three-Way Pressure on Context:**

| Pressure | Problem |
|----------|---------|
| **Cost & latency spirals** | Cost and time-to-first-token grow with context size |
| **Signal degradation** | Irrelevant logs distract the model ("lost in the middle") |
| **Physical limits** | RAG results and traces eventually overflow even largest windows |

**The Solution: Tiered Context Model**

Keep **working context** (the prompt for this turn) small and focused. Push durable state into **Session** (conversation log), **Memory** (searchable, cross-session), and **Artifacts** (large files by reference, not pasted). Put the most important instructions and retrieval at the **start and end** of the prompt.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TIERED CONTEXT                                 â”‚
â”‚   WORKING (this turn)   Session (this convo)   Memory (long-term) â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚ System + key â”‚      â”‚ Chat history      â”‚  â”‚ Searchable   â”‚  â”‚
â”‚   â”‚ docs + query â”‚      â”‚ + tool I/O        â”‚  â”‚ facts, prefs â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚   ARTIFACTS: Large files addressed by name, not pasted           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Layer | Purpose | Lifecycle |
|-------|---------|-----------|
| **Working Context** | Immediate prompt for this call | Ephemeral |
| **Session** | Durable log of events | Per-conversation |
| **Memory** | Long-lived searchable knowledge | Cross-session |
| **Artifacts** | Large files | Addressed by name, not pasted |

**Multi-Agent Context Scoping:** When one agent delegates to another, control what the sub-agent sees. **Agents as Tools** = sub-agent gets only the instructions and inputs you pass. **Agent Transfer** = sub-agent gets a configurable view over Session (e.g. last N turns).

| Pattern | Description |
|---------|-------------|
| **Agents as Tools** | Sub-agent sees only specific instructions and inputs |
| **Agent Transfer** | Sub-agent inherits a configurable view over Session |

---

## 5. LLM Evaluation & Quality

**What "knowledge quality" means here:** For LLM and RAG systems, quality is **groundedness** (is the answer supported by the context?), **relevance** (does it address the question?), and **retrieval quality** (did we fetch the right chunks?). You rarely have gold labels for every request, so evaluation mixes **reference-free** automated metrics (e.g. faithfulness, relevancy) with **sampled human review** to calibrate and catch edge cases. This section is tool-first: each concept is tied to frameworks you can run today.

---

### Evaluation Frameworks & Metrics

**RAGAS** (Python: `pip install ragas`) is the de facto open-source choice for **reference-free** RAG evaluation. You pass a dataset of `(user_input, retrieved_contexts, response)` plus optional `reference`; RAGAS runs LLM-as-judge and embedding-based metrics and returns scores. Used by LangChain, LlamaIndex, and LangSmith integrations.

| Metric | What It Measures | How (in RAGAS) | Tool |
|--------|------------------|----------------|------|
| **Faithfulness** | Is response grounded in context? | LLM extracts claims â†’ checks each against retrieved docs | `ragas.metrics.Faithfulness` |
| **Answer Relevancy** | Does answer address the question? | Inverse of LLM-generated â€œcounterfactualâ€ questions needed to recover answer | `ragas.metrics.AnswerRelevancy` |
| **Context Precision** | Are relevant docs ranked above noise? | Ground-truth relevant items ranked high â†’ higher score | `ragas.metrics.ContextPrecision` (needs ground truth) |
| **Context Recall** | Did we retrieve what we need? | Overlap between answer-supporting context and retrieved context; or vs. reference | `ragas.metrics.ContextRecall` / `LLMContextRecall` |

**Practical RAGAS workflow:** Build a list of dicts with `user_input`, `retrieved_contexts`, `response`, and optionally `reference`. Load into `EvaluationDataset.from_list(dataset)`, then call `evaluate(dataset=..., metrics=[Faithfulness(), AnswerRelevancy(), ...], llm=evaluator_llm)`. Use a **different** LLM for evaluation than for generation to reduce self-consistency bias. See [RAGAS docs](https://docs.ragas.io/en/stable/getstarted/rag_eval/).

**Other tools:**

- **LangSmith** (LangChain): Predefined RAG evaluators (correctness, relevance, groundedness), dataset runs, human annotation queues, and online feedback. Use `client.run_evaluator` or the LangSmith UI to run evals on logged runs. Strong when your stack is already LangChain.
- **Giskard** (Python: `pip install giskard`): RAG Evaluation Toolkit (RAGET)â€”testset generation, knowledge-baseâ€“aware tests, and scalar metrics. Good for â€œtest-suiteâ€ style regression and CI.
- **Arize Phoenix** (Python: `pip install arize-phoenix`): Open-source LLM tracing + evals. Phoenix Evals include **hallucination**, relevance, toxicity; they run over OpenTelemetry traces. Use for production monitoring and â€œeval on sampled traffic.â€
- **Braintrust** (Python: `braintrust`): `Eval()` / `EvalAsync()` over datasets; you define **scorers** (functions that score outputs). Fits custom logic and proprietary benchmarks.
- **TruLens**: Focus on â€œRAG triadâ€ (context relevance, grounding, relevance) with minimal config; integrates with LlamaIndex and other frameworks.

---

### Hallucination Detection: Approaches & Tools

| Approach | What It Does | Accuracy | Latency | Tools / How |
|----------|--------------|----------|---------|-------------|
| **Self-consistency** | Sample N answers, check agreement | Moderate | High (NÃ— calls) | Custom loop or Braintrust/Phoenix over multiple runs |
| **NLI / cross-encoder** | Entailment model: premise = context, hypothesis = claim | High | +50â€“100 ms | Sentence-transformers NLI, or Phoenix â€œgroundednessâ€â€“style evals |
| **LLM-as-Judge** | â€œIs this claim supported by the context?â€ | High | +100â€“200 ms | **RAGAS** `Faithfulness`, **LangSmith** groundedness, **Phoenix** hallucination template, **Braintrust** custom scorer |
| **Specialized faithfulness models** | Fine-tuned â€œfaithfulness vs. hallucinationâ€ judge | Highest | ~+50 ms | **Vectara FaithJudge** ([GitHub](https://github.com/vectara/FaithJudge)): benchmark + model for RAG QA/summarization; use when you need max agreement with human judgment |

**Practical tip:** In production, run **fast** checks inline (format, length, toxicity if you have a small classifier), and push **faithfulness / hallucination** to async jobs on a sample (e.g. 5â€“10%) using RAGAS or Phoenix so cost and latency stay bounded.

---

### How to Run Evaluation in Practice

1. **Offline / batch (before release or in CI)**  
   - **Data:** List of `(query, retrieved_contexts, response)` or `(query, response)`; optional `reference` for reference-based metrics.  
   - **Run:** RAGAS `evaluate()` on a dataset; or LangSmith â€œevaluate datasetâ€; or Braintrust `Eval(dataset, scorers=...)`.  
   - **Use:** Regressions, A/B on prompts or retrievers, and calibration of thresholds.

2. **Online / production (sampled)**  
   - **Data:** Log requests and responses (and retrieved contexts if RAG) to **LangSmith**, **Phoenix**, or your own store.  
   - **Run:** Periodic jobs (e.g. cron or queue) that pull a sample (e.g. 5%), run RAGAS or Phoenix evals, and write scores to a dashboard or alerting.  
   - **Use:** Drift detection, â€œdid we build the right thing?â€ in the wild.

3. **Human loop**  
   - **Data:** Subset of production or offline examples (e.g. 100â€“500) with labels (good/bad, error type, etc.).  
   - **Tools:** **LangSmith** annotation queue, Label Studio, or internal tooling.  
   - **Use:** Calibrate automated metrics (â€œat what faithfulness score do humans usually approve?â€), build training data for task-specific judges, and categorize failure modes.

**Aha:** You donâ€™t need gold labels for every request. **Reference-free** metrics (RAGAS faithfulness, answer relevancy, Phoenix hallucination) answer â€œis this grounded?â€ and â€œdoes this match the question?â€ without human annotations. Use them on a sample in production, then a **small human-labeled set** to set thresholds and sanity-check.

---

### Production Evaluation Strategy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 EVALUATION PIPELINE                             â”‚
â”‚                                                                 â”‚
â”‚   Request â†’ LLM Response                                        â”‚
â”‚                â”‚                                                â”‚
â”‚                â”œâ”€â”€â–º Real-time checks (< 50ms budget)             â”‚
â”‚                â”‚    â€¢ Toxicity (e.g. Perspective API, small      â”‚
â”‚                â”‚      classifier, or rule-based filters)         â”‚
â”‚                â”‚    â€¢ Format validation (schema, length)         â”‚
â”‚                â”‚    â€¢ Length limits                             â”‚
â”‚                â”‚    Tools: in-process code, light model or API  â”‚
â”‚                â”‚                                                â”‚
â”‚                â”œâ”€â”€â–º Async evaluation (sampled, e.g. 5â€“10%)      â”‚
â”‚                â”‚    â€¢ Faithfulness / grounding â†’ RAGAS, Phoenix  â”‚
â”‚                â”‚    â€¢ Hallucination â†’ Phoenix evals, FaithJudge â”‚
â”‚                â”‚    â€¢ Task-specific metrics â†’ Braintrust, custom â”‚
â”‚                â”‚    Tools: RAGAS, Phoenix, LangSmith, Braintrust  â”‚
â”‚                â”‚                                                â”‚
â”‚                â””â”€â”€â–º Human evaluation (subset of async or batch) â”‚
â”‚                     â€¢ Quality ratings, error taxonomy            â”‚
â”‚                     â€¢ Calibrate automated score thresholds       â”‚
â”‚                     Tools: LangSmith annotation, Label Studio   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Insight:** Not every request gets every metric. Use **tiered evaluation**â€”cheap checks inline, expensive ones (RAGAS, hallucination, custom scorers) on a **sample** and/or async, so latency and cost stay under control.

---

### Tools Quick Reference

| Tool | What It Does | When to Use |
|------|----------------|-------------|
| **RAGAS** | Reference-free RAG metrics (faithfulness, relevancy, context precision/recall) | Batch RAG evals, CI, offline benchmarks; Python-first |
| **LangSmith** | Evaluators, datasets, runs, human annotation | LangChain-based apps; need UI + queues + feedback |
| **Phoenix** | Tracing + evals (hallucination, relevance, toxicity) over OTLP | Production monitoring, eval-on-sampled-traffic |
| **Giskard** | RAG test suite, testset generation, scalar metrics | Regression and â€œtest suiteâ€ style RAG evaluation |
| **Braintrust** | Custom scorers, `Eval`/`EvalAsync`, experiments | Proprietary benchmarks, custom logic, experiments |
| **FaithJudge** (Vectara) | Faithfulness/hallucination benchmark + model | High-stakes RAG; max agreement with human judgment |

---

### Evaluation data pipeline at scale

The metrics and tools above assume you have prediction data to evaluate. At scale, you need a **data pipeline**: predictions flow from the LLM â†’ event stream â†’ stream processor â†’ evaluation/metrics layer and time-series store â†’ dashboards and alerting. This is the *evaluation* pipeline (log predictions, run quality/safety/cost metrics); the *training* pipeline (user interactions â†’ fine-tuning data) is Â§6.

**Use case: Production LLM evaluation system**

**Requirements:** Evaluate model performance continuously; track 100+ metrics (accuracy, latency, cost, safety); process 1M predictions/day; alert on degradation; support A/B testing.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  EVAL DATA PIPELINE (at scale)                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚    LLM       â”‚â”€â”€â”€â”€â–ºâ”‚ Event Stream â”‚â”€â”€â”€â”€â–ºâ”‚   Stream     â”‚   â”‚
â”‚   â”‚ Predictions  â”‚     â”‚ Pub/Sub or   â”‚     â”‚ Processor    â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ Kinesis      â”‚     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚           â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚                    â–¼                               â–¼       â–¼    â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚              â”‚ Evaluationâ”‚                   â”‚  Time-Series   â”‚ â”‚
â”‚              â”‚ (RAGAS,   â”‚                   â”‚  DB â†’ Dashboardsâ”‚
â”‚              â”‚ Phoenixâ€¦) â”‚                   â”‚  Alerting, A/B â”‚ â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Sampling:** Full (100%) = complete visibility but costly; sampled (e.g. 10%) = cheaper, may miss rare errors; **smart (100% errors + sample successes)** = recommendedâ€”capture all failures, sample successes for stats.

**Frequency:** Real-time for latency/errors (user-facing); batch (hourly/daily) for quality/cost (expensive metrics); **hybrid** for most production.

**What to track:** Quality (task accuracy, ROUGE/BLEU, human eval), latency (P50/P95/P99), cost (tokens, model tier), safety (toxicity, jailbreak, bias).

---

## 6. GenAI Data Pipeline Architecture

**In the big picture** (see [GenAI System: Big Picture](#genai-system-big-picture-frontend-to-backend)), this is the **training-data pipeline**: the path from "users interacted with the system" to "we have clean, formatted examples for fine-tuning." It is *distinct* from the evaluation pipeline (Â§5), which moves *prediction* data into metrics and alerts. Here we focus on **collecting user interactions** (prompts, responses, feedback), processing them at scale, and producing training-ready datasets.

**T-shaped summary:** User interactions â†’ event stream (Pub/Sub, Kinesis) â†’ stream processor (Dataflow, etc.) â†’ data lake and optionally feature store â†’ training data prep (filter, dedupe, validate, format for fine-tuning). Deep dive below.

---

### Use Case: Design a Training Data Pipeline for Fine-Tuning

**Requirements:**
- Collect user interactions (prompts, responses, feedback)
- Process 10M examples/day
- Clean and prepare data for fine-tuning
- Support continuous data collection

**High-Level Design:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  TRAINING DATA PIPELINE                         â”‚
â”‚                                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚    User      â”‚â”€â”€â”€â”€â–ºâ”‚    Event     â”‚â”€â”€â”€â”€â–ºâ”‚    Data      â”‚   â”‚
â”‚   â”‚ Interactions â”‚     â”‚  Collection  â”‚     â”‚  Processing  â”‚   â”‚
â”‚   â”‚              â”‚     â”‚  Pub/Sub     â”‚     â”‚  Dataflow    â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                     â”‚           â”‚
â”‚                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤           â”‚
â”‚                        â”‚                            â”‚           â”‚
â”‚                        â–¼                            â–¼           â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚                  â”‚ Data Lake â”‚              â”‚ Feature Store â”‚   â”‚
â”‚                  â”‚   (GCS)   â”‚              â”‚               â”‚   â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                        â”‚                                        â”‚
â”‚                        â–¼                                        â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚                  â”‚          Training Data Prep               â”‚ â”‚
â”‚                  â”‚  Filter, dedupe, validate, format         â”‚ â”‚
â”‚                  â”‚         for fine-tuning                   â”‚ â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Service Comparison

| Component | Google Cloud | AWS |
|-----------|--------------|-----|
| **Event Streaming** | Pub/Sub | Kinesis Data Streams |
| **Stream Processing** | Dataflow | Kinesis Analytics |
| **Data Lake** | Cloud Storage | S3 |
| **Data Warehouse** | BigQuery | Redshift |
| **Feature Store** | Vertex AI Feature Store | SageMaker Feature Store |
| **Training** | Vertex AI Training | SageMaker Training |
| **Orchestration** | Vertex AI Pipelines | SageMaker Pipelines |

---

## 7. Cost Optimization for GenAI Systems

**In the big picture** (see [GenAI System: Big Picture](#genai-system-big-picture-frontend-to-backend)), this is **how we keep inference affordable**: cost scales with tokens (input + output) and model tier, so optimization is about **reducing spend per request**â€”shorter prompts, caching, model routing, quantization, and when relevant fine-tuning ROI. *Throughput* and *capacity* are in Â§8 Scalability; here we focus on *cost per request*.

**T-shaped summary:** Cost = f(tokens, model). Levers: prompt optimization, response/prompt caching, routing easy queries to smaller models, quantization, and continuous batching (better GPU use â†’ same throughput with fewer machines). Deep dive below.

---

### Token-Based Cost Model

**Cost Components:**
- **Input tokens**: Tokens in prompt (including context)
- **Output tokens**: Generated tokens (typically 2-4x more expensive)
- **Model tier**: Different models have different costs

**Aha:** GenAI cost scales with **length**, not just request count. A 10Ã— longer prompt or answer can mean ~10Ã— cost per call. So trimming context, caching prefixes, and routing easy queries to smaller models all directly lower spend.

**Example Calculation:**

```
Model: Gemini Pro
Input: $0.000125 per 1K tokens
Output: $0.0005 per 1K tokens

Request:
- Input: 1,000 tokens
- Output: 500 tokens

Cost = (1,000 / 1,000) Ã— $0.000125 + (500 / 1,000) Ã— $0.0005
     = $0.000125 + $0.00025
     = $0.000375 per request

At 1M requests/day: $375/day = $11,250/month
```

### Optimization Strategies

**1. Prompt Optimization**

| Technique | Savings | Trade-off |
|-----------|---------|-----------|
| Shorter prompts | 20-40% input tokens | May lose context |
| Fewer examples | 50-200 tokens/example | May reduce quality |
| Prompt compression | Variable | Compression cost vs savings |

**Few-shot sweet spot**: 2-3 examples usually sufficient. Research shows diminishing returns after 3 examplesâ€”the model has learned the pattern.

**2. Caching Strategy**

| Strategy | Hit Rate | Savings | Best For |
|----------|----------|---------|----------|
| Prompt caching | High for prefixes | 2-5x speedup | System prompts |
| Response caching | 10-30% | 100% for hits | FAQ systems |
| Semantic caching | 30-50% | Varies | Q&A systems |

**3. Model Selection (Tiered Strategy)**

| Model | Cost | Quality | Use For |
|-------|------|---------|---------|
| **Large (GPT-4, Gemini Ultra)** | $0.03-0.06/1K output | Best | Complex reasoning |
| **Medium (GPT-3.5, Gemini Pro)** | ~$0.002/1K output | Good | Most production tasks |
| **Small (Gemini Flash)** | ~$0.001/1K output | Basic | Simple, high-volume |

**Model Routing Strategies:**

| Strategy | How It Works | Savings |
|----------|--------------|---------|
| **Routing** | Classify query â†’ send to single optimal model | 40-60% |
| **Cascading** | Start small â†’ escalate to larger if low confidence | 50-80% |
| **Cascade Routing** | Combines both: route + escalation | Best cost/quality |

```
Query â†’ Classifier â†’ Simple? â†’ Small Model â†’ Done
                         â”‚
                         â””â”€â”€â–º Complex? â†’ Large Model â†’ Done

OR (Cascading):

Query â†’ Small Model â†’ Confident? â†’ Return
              â”‚
              â””â”€â”€â–º Low confidence â†’ Large Model â†’ Return
```

**Quality Estimation**: The key to routingâ€”use a small classifier or confidence scores to predict which model can handle the query.

**Aha:** Routing and cascading both assume "hard" and "easy" queries. If you can **predict** hardness (e.g. by query length, intent, or a tiny classifier), you send easy ones to small/cheap models and reserve the big model for the rest. The leverage comes from that prediction being cheap and reasonably accurate.

**4. Fine-tuning ROI**

- **Upfront cost**: $100-1000s
- **Break-even**: If fine-tuning costs $1000 and saves $0.001 per request, break-even at 1M requests
- **Benefits**: Better quality for domain, can use smaller base model

**5. Quantization**

Reducing numerical precision shrinks model size and speeds inference. **FP32** (32-bit float), **FP16** (16-bit), **INT8** (8-bit integer), **INT4** (4-bit) are common levels.

| Precision | Memory Reduction | Quality Loss |
|-----------|-----------------|--------------|
| FP32 â†’ FP16 | 2x | Minimal |
| FP16 â†’ INT8 | 4x | Some |
| INT8 â†’ INT4 | 8x | Significant |

**Why FP16 is safe**: Modern **GPUs** (graphics processing units) have Tensor Cores optimized for FP16. Quality loss is minimal (<1%) but memory/cost savings are significant.

**Aha:** Weights donâ€™t need 32-bit precision for good answers; most signal lives in a smaller range. Quantization **compresses** that range (FP32â†’FP16â†’INT8â†’INT4). You trade a little quality for large memory and speed gains. FP16 is the first step almost everyone takes because hardware is built for it and the drop is tiny.

**6. Continuous Batching**

- Static batching: 40â€“60% GPU utilization
- Continuous batching: 80â€“95% GPU utilization
- **Result**: 2â€“3Ã— higher throughput â†’ fewer machines for the same load (cost and scale). Throughput/parallelism patterns (model parallelism, pipeline parallelism) are in Â§8.

---

## 8. Scalability Patterns for GenAI

**In the big picture** (see [GenAI System: Big Picture](#genai-system-big-picture-frontend-to-backend)), this is **how we serve more load**: the LLM layer is GPU-heavy and stateful (KV cache), so scaling is about **throughput and capacity**â€”horizontal replication, model/pipeline parallelism, and caching that increases effective req/s. *Cost per request* is in Â§7; here we focus on *requests per second* and *utilization*.

**T-shaped summary:** Levers: stateless serving (more replicas), model parallelism (split layers across GPUs), pipeline parallelism (different layers on different GPUs), and caching (KV cache for prefixes, response cache for identical/similar queries). Deep dive below.

---

### Horizontal Scaling

**Challenge**: LLM inference is GPU-intensive and stateful (KV cache).

**Solutions:**

| Pattern | Description | Trade-off |
|---------|-------------|-----------|
| **Stateless Serving** | Load balancer â†’ Multiple LLM servers | Higher memory (each server has full model) |
| **Model Parallelism** | Split model across GPUs | Communication overhead |
| **Pipeline Parallelism** | Different GPUs handle different layers | Better utilization |

**Model Parallelism Visual:**

```
Input â†’ GPU 1 (Layers 1-10) â†’ GPU 2 (Layers 11-20) â†’ GPU 3 (Layers 21-30) â†’ Output
```

### Caching Strategies for Scale

*Cost* impact of caching is in Â§7; here we focus on **throughput** impact: same hardware serves more requests when prefixes or responses are reused.

| Strategy | Throughput / latency impact | Best For |
|----------|-----------------------------|----------|
| Prompt caching (KV cache) | 2â€“3Ã— effective throughput for repeated prefixes | System prompts, long context |
| Response caching | Near-instant for cache hits; frees GPU for other requests | Identical or near-identical queries |
| Semantic caching | Higher hit rate â†’ more requests served from cache | Similar queries (e.g. Q&A) |

---

## 9. Monitoring & Observability for GenAI

**In the big picture** (see [GenAI System: Big Picture](#genai-system-big-picture-frontend-to-backend)), this is **how we observe the system**: metrics, traces, and drift detection across the request path and the evaluation/training pipelines. Quality metrics and eval pipeline are in Â§5; here we focus on **what to track** and **which platform services** support it.

**T-shaped summary:** Track quality (task accuracy, safety), performance (latency, throughput), cost (tokens, model tier), reliability (errors, timeouts), and safety (toxicity, jailbreak). Use Cloud Monitoring / CloudWatch, logging, tracing (Trace / X-Ray), and model monitoring for drift. Deep dive below.

---

### Key Metrics to Track

| Category | Metrics |
|----------|---------|
| **Quality** | Task accuracy, ROUGE/BLEU, human evaluation |
| **Performance** | P50/P95/P99 latency, throughput, tokens/second |
| **Cost** | Cost per request, token usage, model tier breakdown |
| **Reliability** | Error rate, timeout rate, availability |
| **Safety** | Toxicity score, jailbreak attempts, bias detection |

### Platform Services

| Function | Google Cloud | AWS |
|----------|--------------|-----|
| **Metrics** | Cloud Monitoring, Vertex AI Monitoring | CloudWatch |
| **Logging** | Cloud Logging | CloudWatch Logs |
| **Tracing** | Cloud Trace | X-Ray |
| **Drift Detection** | Vertex AI Model Monitoring | SageMaker Model Monitor |

---

## 10. Security & Guardrails

**In the big picture** (see [GenAI System: Big Picture](#genai-system-big-picture-frontend-to-backend)), this is **how we protect the system**: inputs (prompt injection, jailbreak, PII), outputs (harmful content, PII leakage), and access (IAM, API keys). Guardrails sit *around* the request pathâ€”input checks before the LLM, output checks afterâ€”and work with HTTP-level protections (Cloud Armor, WAF) and data protection (DLP).

**T-shaped summary:** Threats: direct/indirect prompt injection, data leakage, jailbreaking, unauthorized access. Mitigations: input/output guardrails, spotlighting, least-privilege tools, Model Armor (or Bedrock Guardrails). Use defense-in-depth: gateway â†’ guardrails â†’ LLM â†’ guardrails â†’ response. Deep dive below.

---

### Key Security Concerns

**Aha:** LLMs take natural language as input, so **any** user text can be an attempt to override instructions ("Ignore previous instructionsâ€¦"). Guardrails and defense-in-depth exist because you can't whitelist "good" promptsâ€”you have to detect and constrain *malicious* or out-of-scope intent at the boundary.

| Threat | Risk | Mitigation |
|--------|------|------------|
| **Direct Prompt Injection** | User injects malicious instructions | Input validation, guardrails |
| **Indirect Prompt Injection** | Hidden instructions in external content | Content isolation, spotlighting |
| **Data Leakage** | Training data memorization, **PII** (personally identifiable information) in outputs | Output filtering, **DLP** (data loss prevention) |
| **Jailbreaking** | Bypassing safety controls | Multi-layer defense, red teaming |
| **Access Control** | Unauthorized model access | **IAM** (identity and access management), API keys, least privilege |

### Prompt Injection Defense-in-Depth

| Layer | Technique | Description |
|-------|-----------|-------------|
| **Input** | Spotlighting | Clearly delimit user input vs system prompt |
| **Input** | Input validation | Regex, blocklists, encoding detection |
| **Input** | Guardrails check | Detect injection attempts before LLM |
| **Processing** | Least privilege | Limit tools/data agent can access |
| **Output** | Guardrails check | Validate output aligns with user intent |
| **Output** | PII filtering | Detect/redact sensitive data |

### Guardrails Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   GUARDRAILS PIPELINE                           â”‚
â”‚                                                                 â”‚
â”‚   User Input                                                    â”‚
â”‚       â”‚                                                         â”‚
â”‚       â–¼                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚   â”‚ INPUT GUARDRAILâ”‚  â€¢ Prompt injection detection             â”‚
â”‚   â”‚                â”‚  â€¢ Jailbreak detection                    â”‚
â”‚   â”‚                â”‚  â€¢ PII detection                          â”‚
â”‚   â”‚                â”‚  â€¢ Content policy check                   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚           â”‚                                                     â”‚
â”‚     Block â”œâ”€â”€â–º Return error                                    â”‚
â”‚           â”‚                                                     â”‚
â”‚           â–¼                                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚   â”‚      LLM       â”‚                                           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚           â”‚                                                     â”‚
â”‚           â–¼                                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚   â”‚OUTPUT GUARDRAILâ”‚  â€¢ Hallucination check                    â”‚
â”‚   â”‚                â”‚  â€¢ Response relevancy                     â”‚
â”‚   â”‚                â”‚  â€¢ PII in output                          â”‚
â”‚   â”‚                â”‚  â€¢ Harmful content                        â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚           â”‚                                                     â”‚
â”‚           â–¼                                                     â”‚
â”‚   User Response                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Tool Call Validation** (for agents):
- **Pre-flight**: Validate tool call aligns with user's request before execution
- **Post-flight**: Validate returned data before showing to user

### Model Armor (Google Cloud)

Model Armor is Google Cloud's service for real-time input/output filtering on LLM traffic. It addresses threats that traditional **WAFs** (web application firewalls) can't catchâ€”specifically **prompt injection** and **sensitive data disclosure** at the semantic level.

**What Model Armor Catches vs Cloud Armor:**

| Threat | Cloud Armor | Model Armor |
|--------|-------------|-------------|
| SQL injection in HTTP | âœ… | âŒ (not its job) |
| DDoS / rate limiting | âœ… | âŒ |
| **Prompt injection** | âŒ | âœ… |
| **Jailbreak attempts** | âŒ | âœ… |
| **PII in LLM output** | âŒ | âœ… |

**Use both for production deploymentsâ€”they protect different attack surfaces.**

### Defense Layers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  SECURE AGENT ARCHITECTURE                      â”‚
â”‚                                                                 â”‚
â”‚   User Request                                                  â”‚
â”‚        â”‚                                                        â”‚
â”‚        â–¼                                                        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚
â”‚   â”‚ Cloud Armor   â”‚  HTTP-level: DDoS, rate limiting           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚
â”‚           â”‚                                                     â”‚
â”‚           â–¼                                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚
â”‚   â”‚  API Gateway  â”‚  Auth, authorization (IAM)                 â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚
â”‚           â”‚                                                     â”‚
â”‚           â–¼                                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚
â”‚   â”‚ Model Armor   â”‚  Input: prompt injection, PII              â”‚
â”‚   â”‚   (Input)     â”‚                                            â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚
â”‚           â”‚                                                     â”‚
â”‚           â–¼                                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚
â”‚   â”‚  LLM / Agent  â”‚                                            â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚
â”‚           â”‚                                                     â”‚
â”‚           â–¼                                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚
â”‚   â”‚ Model Armor   â”‚  Output: harmful content, PII              â”‚
â”‚   â”‚   (Output)    â”‚                                            â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚
â”‚           â”‚                                                     â”‚
â”‚           â–¼                                                     â”‚
â”‚   User Response                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Compliance Considerations

| Regulation | Key Requirements |
|------------|------------------|
| **GDPR** (General Data Protection Regulation) | Right to explanation, data deletion, privacy by design |
| **HIPAA** (Health Insurance Portability and Accountability Act) | Healthcare data protection, audit logging |
| **PCI-DSS** (Payment Card Industry Data Security Standard) | Payment data security, no storage of card numbers |

### Security Stack Summary

| Layer | Google Cloud | AWS |
|-------|--------------|-----|
| **LLM Security** | Model Armor | Bedrock Guardrails |
| **HTTP Security** | Cloud Armor | WAF (web application firewall) |
| **Data Protection** | Cloud DLP (data loss prevention) | Macie |
| **Secrets** | Secret Manager | Secrets Manager |
| **Network** | VPC (virtual private cloud) Service Controls | VPC |
| **Access** | IAM (identity and access management) | IAM |
| **Audit** | Cloud Audit Logs | CloudTrail |

---

## 11. Real-World Examples: Applying the Stack

This section comes **after** all core concepts (Â§1â€“Â§10) so you can apply them. Each example states the **problem**, the **concepts** from this guide that apply, and a **concrete solution** using specific stacks: **LangChain** / **LlamaIndex** (orchestration, RAG, agents), **Google (Vertex AI)** or **AWS (Bedrock)**, and **open source** (vLLM, RAGAS, Phoenix, etc.). Use these as blueprints for "how would I build this with real tools?"

---

### Example 1: Code Generation Assistant (like GitHub Copilot)

**Problem:** In-IDE completions that understand the codebase, respect privacy, and run with low latency.

**Concepts:** Â§1 (LLM serving / model routing), Â§2 (RAG for code context), Â§4 (single agent + tools), Â§7 (cost: smaller model for completions, routing by complexity).

**Concrete solution:**

- **Orchestration + RAG:** **LangChain** or **LlamaIndex** to build a "code context" pipeline: embed workspace chunks (or AST-based chunks), retrieve on cursor context, format as prefix for the model. Use **LlamaIndex** `CodeIndex` / doc split by language or **LangChain** `RecursiveCharacterTextSplitter` + vector store (e.g. Chroma, open source).
- **LLM:** **Vertex AI Codey** (Google) or **Amazon CodeWhisperer** / **Bedrock** (AWS) for code-native APIs; or **open source** (**CodeLlama**, **StarCoder**) behind **vLLM** for self-hosted, low-latency completion.
- **Evaluation:** **RAGAS** or **LangSmith** on a sample of (prompt, context, completion) for relevance and correctness; **Phoenix** for production traces and latency.
- **Guardrails:** Input/output length limits, optional PII/secret filters (e.g. **Guardrails AI**, **NeMo Guardrails**), or **Bedrock Guardrails** / **Model Armor** if on AWS/Google.

**Stack snapshot:** LangChain/LlamaIndex (RAG + routing) + Vertex Codey or Bedrock + vLLM (optional) + RAGAS/LangSmith/Phoenix (eval) + guardrails.

---

### Example 2: Customer Service Chatbot with RAG and Tools

**Problem:** Chat that answers from internal docs, checks orders/tickets via tools, and escalates to humans when needed.

**Concepts:** Â§2 (RAG: knowledge base), Â§4 (agent with tools, escalation as a "tool"), Â§5 (eval: faithfulness, relevancy), Â§10 (guardrails, PII).

**Concrete solution:**

- **Orchestration + agent:** **LangChain** `create_react_agent` or **LlamaIndex** `ReActAgent` with tools: RAG retriever (knowledge base), "check order" (API), "create ticket" (CRM API), "escalate" (handoff to human queue). Use **MCP** or custom tool schemas so the agent can call backend APIs.
- **RAG:** **Vertex AI RAG Engine** (Google) or **Bedrock Knowledge Bases** (AWS) for managed ingestion + retrieval; or **LangChain** + **Chroma** / **Pinecone** + **OpenAI** or **Cohere** embeddings (open / API). Apply chunking and reranking from Â§2.
- **LLM:** **Vertex AI** (Gemini) or **Bedrock** (Claude, Llama) for conversation and tool use.
- **Evaluation:** **RAGAS** (faithfulness, answer relevancy) on logged (query, context, response); **LangSmith** for dataset runs and human annotation queues.
- **Security:** **Bedrock Guardrails** or **Model Armor** for input/output; scope tools with IAM/least privilege; filter PII in tool *outputs* before they reach the model or user.

**Stack snapshot:** LangChain/LlamaIndex (agent + tools) + Vertex RAG Engine or Bedrock Knowledge Bases + Vertex/Bedrock LLM + RAGAS/LangSmith (eval) + Model Armor/Bedrock Guardrails.

---

### Example 3: Content Generation Platform (research â†’ draft â†’ grounding)

**Problem:** Multi-step content: research from web, generate draft, fact-check against sources, then SEO and multi-format output.

**Concepts:** Â§4 (sequential pipeline: research â†’ generation â†’ grounding â†’ SEO), Â§2 (RAG/grounding for fact-check), Â§5 (faithfulness eval), Â§7 (cost: model routing for easy vs hard steps).

**Concrete solution:**

- **Orchestration:** **LangChain** `SequentialChain` or a custom DAG: (1) research step = tool to **Google Search** or **Tavily** (or Vertex Search); (2) generation = LLM with research as context; (3) grounding = LLM or **Vertex AI grounding** / **Bedrock** retrieval + NLI-style check; (4) SEO = templates or a small LLM call. This is the "sequential pipeline" from Â§4 Additional Patterns.
- **LLM:** **Vertex AI** (Gemini) or **Bedrock** (Claude). Use **routing** (Â§7): e.g. Gemini Flash for research/summary, Gemini Pro for final draft.
- **Grounding:** **Vertex AI grounding with Google Search** or **Bedrock** retrieval + cite-check; or **open source**: RAG pipeline + **RAGAS** faithfulness on (claim, source) samples.
- **Evaluation:** **RAGAS** faithfulness and relevancy on (brief, sources, draft); **LangSmith** or **Braintrust** for A/B on prompts and model choices.

**Stack snapshot:** LangChain (sequential pipeline + tools) + Vertex/Bedrock LLMs + Vertex grounding or RAG + RAGAS (eval) + optional Giskard for regression tests.

---

### Cross-example takeaways

| Concern | Tools to reach for |
|--------|--------------------|
| **Orchestration (RAG, agents, pipelines)** | LangChain, LlamaIndex |
| **Managed RAG / embeddings** | Vertex RAG Engine, Bedrock Knowledge Bases |
| **LLM hosting** | Vertex AI (Codey, Gemini), Bedrock (Claude, CodeWhisperer, etc.), or vLLM for self-hosted |
| **Evaluation (reference-free)** | RAGAS (batch), LangSmith (datasets + humans), Phoenix (traces + evals) |
| **Guardrails** | Model Armor (Google), Bedrock Guardrails (AWS), Guardrails AI / NeMo (open source) |

---

## Resources

### Books

- **Building LLM Applications for Production** by Huyen, Chip
- **Designing Machine Learning Systems** by Chip Huyen
- **Designing Data-Intensive Applications** by Martin Kleppmann

### Online

- [vLLM Documentation](https://docs.vllm.ai/) - High-throughput LLM serving
- [RAGAS Documentation](https://docs.ragas.io/) - Reference-free RAG evaluation (faithfulness, relevancy, context metrics)
- [LangSmith Evaluation](https://docs.smith.langchain.com/evaluation) - Evaluators, datasets, human annotation
- [Arize Phoenix](https://phoenix.arize.com/) - LLM tracing and evals (hallucination, relevance, toxicity)
- [Giskard RAG Toolkit](https://docs.giskard.ai/en/stable/reference/rag-toolset/) - RAG test suite and testset generation
- [Braintrust Evaluate](https://www.braintrust.dev/docs/evaluation) - Custom scorers and experiments
- [Vectara FaithJudge](https://github.com/vectara/FaithJudge) - Faithfulness/hallucination benchmark and model
- [LangChain Documentation](https://docs.langchain.com/)
- [LlamaIndex Documentation](https://docs.llamaindex.ai/)
- [OpenAI Guardrails](https://openai.github.io/openai-guardrails-python/)
- [MCP (Model Context Protocol)](https://modelcontextprotocol.io/) - Standard for tools and context to LLMs
- [A2A (Agent-to-Agent Protocol)](https://google.github.io/A2A/) - Standard for agent-to-agent communication

### Google Cloud Documentation

- [Vertex AI Generative AI](https://cloud.google.com/vertex-ai/generative-ai/docs/overview)
- [Vertex AI Agent Builder](https://cloud.google.com/vertex-ai/docs/agent-builder/overview)
- [Vertex AI RAG Engine](https://cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/overview)
- [Model Armor](https://cloud.google.com/security/products/model-armor)

### AWS Documentation

- [Amazon Bedrock](https://docs.aws.amazon.com/bedrock/)
- [Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/)
- [Bedrock Agents](https://docs.aws.amazon.com/bedrock/latest/userguide/agents.html)
- [Bedrock Guardrails](https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails.html)

### Practice

- Build real GenAI applications
- Experiment with different model sizes and costs
- Practice with RAG systems and agents

---

## Quick Reference

### What FAANG Interviewers Evaluate

| Dimension | What They Test |
|-----------|----------------|
| **LLM Awareness** | Token limits, context windows, model types, pricing models |
| **Architectural Reasoning** | How retrieval, prompt logic, post-processing connect |
| **Cost-Latency Tradeoffs** | Balancing inference cost, response latency, quality |
| **Safety & Governance** | Reliable outputs, guardrails, compliance |
| **Observability** | Handling non-deterministic outputs, failure modes |

### Interview Framework (45-min structure)

**1. Clarify Requirements (5-10 min)**
- Token budget and latency targets
- Quality requirements (hallucination tolerance)
- Cost constraints (per-token, monthly budget)
- Safety requirements (compliance, content filtering)

**2. High-Level Architecture (10-15 min)**
- Draw components: API gateway â†’ orchestration â†’ LLM â†’ post-processing
- Show data flow and identify APIs
- Include: RAG, caching, model routing

**3. Deep Dive (15-20 min)**
- RAG design: chunking, embedding, retrieval, reranking
- Model selection and routing strategy
- Evaluation and observability approach
- Security layers (guardrails, Model Armor)

**4. Bottlenecks & Trade-offs (5-10 min)**
- KV cache memory management
- Quality vs cost (model size, routing)
- Latency vs throughput (batching)
- Single vs multi-agent complexity

### Key Trade-offs to Articulate

| Decision | Option A | Option B |
|----------|----------|----------|
| RAG vs Fine-tuning | Fresh data, per-query cost | Behavioral change, upfront cost |
| Large vs Small Model | Higher quality | Lower cost, faster |
| Dense vs Hybrid Search | Semantic matching | + Keyword precision |
| Single vs Multi-Agent | Simpler, faster | More capable, modular |
| Sync vs Async Eval | Immediate | Cost-effective |

---

*For foundational system design concepts, see [System Design Essentials](./system-design-essentials.md).*

*Last updated: January 2026*
